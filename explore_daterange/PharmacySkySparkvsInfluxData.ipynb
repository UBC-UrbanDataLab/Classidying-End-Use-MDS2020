{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports and setup the connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.2.3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import influxdb\n",
    "from datetime import timezone, datetime\n",
    "import pytz\n",
    "import matplotlib.pyplot as plt\n",
    "import certifi\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [15, 5]\n",
    "\n",
    "\n",
    "client = influxdb.InfluxDBClient(host='206.12.92.81',port=8086, \n",
    "                                  username='public', password='public',database='SKYSPARK')\n",
    "\n",
    "#Hopefully this is 5.2.3, will likely run into known bug with chunking if it is 5.3.0\n",
    "print(influxdb.__version__) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to locate valid and invalid records:\n",
    "\n",
    "Data in the SkySpark database has a unique identifying field called `id`. An entry might look like:\n",
    "\n",
    "@p:ubcv:r:236c8a88-c9742760 Pharmacy Heating Plant HX-2 P-HX2A HX2_PHX2A_VFD_PWR(kWh)\n",
    "\n",
    "The key thing here is the `@p:....` portion of the tag. This has not been stored in the influxDB.\n",
    "\n",
    "The influx DB uses `groupRef`, `equipRef`, `typeRef`, `navName`, `siteRef` and `unit`\\* to try to uniquely identify a sensor and link it back to SkySpark. This results in issues when one of those values is updated in skySpark (i.e. `equipRef` has 'Pharmacy' added to it). \n",
    "\n",
    "\\*`unit` isn't necessarily useful in uniquely identifying a tag, at least in the SkySpark database for Pharmacy there are no instances where `groupRef`, `equipRef`, `typeRef`, `navName` and `siteRef` are identical but `unit` differs.\n",
    "\n",
    "\n",
    "The end result is that there are many series in influxDB that can not be linked back to SkySpark so they will need to be deleted from influxDB and requeried for the update to the influxdb structure. Even those that can be linked may only have valid data starting at a specific date so older data will need to be queried and added.\n",
    "\n",
    "1. Records in SkySpark that can not be uniquely identified by the six fields listed above (there are 185 of these)   \n",
    "    - These are saved in Pharmacy-Conflicts-In-Metadata.csv   \n",
    "2. Records in SkySpark that can not be found in influxDB (there are 6006 of these)   \n",
    "    - These are combined with the records from part1 and saved in Pharmacy-Records-To-Requery.csv   \n",
    "3. Records in SkySpark that **CAN** be found in influxDB but only after a certain date (these don't need to be deleted from influxDB, only queried from database startDate until the specific first date for which points for that sensor already exist)\n",
    "    - These are saved as Pharmacy-Records-To-Keep.csv (but remember, observations from before the 'firstdate' value for these records will still need to be queried and added to influxDB!   \n",
    "\n",
    "\n",
    "Note that all of the `equipRef` tags in the SkySpark data have 'Pharmacy' at the start (after the `@p:...` bit that isn't included in influxDB). Only SOME of the data in influxDB has an `equipRef` that starts with 'Pharmacy'. The code takes care of adding this to create the csvs but it is something you should consider doing to the data in influxDB to try to get like sensors grouped together \n",
    "\n",
    "(i.e. a sensor may have 400 points with an equipRef that starts with 'Pharmacy' and then another 6000 points with an equipRef that doesn't start with Pharmacy. It is the same sensor. So it makes sense to write a script to go in and delete+update points to get them all together in one series with and equipRef that starts with 'Pharmacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Records in SkySpark that can not uniquely identified by tags available in influxDB\n",
    "\n",
    "These get saved into their own csv in case it is easier to access them that way but the data is also concatenated onto the final dataframe of SkySpark records that need to be requeried."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3020: DtypeWarning: Columns (2,54,81,118,133,140,149) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7938 sensors in the Pharmacy building's skyspark metadata\n",
      "185 conflicting entries in the dataframe of skyspark data (NOT including unit as part of unique 'key'.\n",
      "185 conflicting entries in the dataframe of skyspark data (including unit as part of unique 'key'). Saving as 'Pharmacy-Conflicts-In-Metadata.csv'\n"
     ]
    }
   ],
   "source": [
    "#Load export of \"metadata\" from skyspark for Pharmacy building (performed morning of 2020-06-05)\n",
    "#This could be a query to SkySpark or to the new metadata_points measurement in influxDB\n",
    "#   if you want to work through all the buildings\n",
    "\n",
    "metadata = pd.read_csv(\"skyspark-pharmacy-2020-06-05-query.csv\")\n",
    "\n",
    "#Update some tags to remove the @id from them to make them play nice with the data stored in the influxDB\n",
    "metadata['equipRefclean']=metadata['equipRef'].str.extract('[^ ]* (.*)', expand=True)\n",
    "metadata['groupRefclean']=metadata['groupRef'].str.extract('[^ ]* (.*)', expand=True)\n",
    "metadata['siteRefclean']=metadata['siteRef'].str.extract('[^ ]* (.*)', expand=True)\n",
    "\n",
    "#Create a uniqueID by concatenating the equipRef, groupRef, navName, siteRef, bmsName (aka typeRef), and unit\n",
    "metadata['uniqueId']=metadata['equipRefclean'].fillna('')+' '+metadata['groupRefclean'].fillna('')+' '+metadata['navName'].fillna('')+ \\\n",
    "' '+metadata['siteRefclean'].fillna('')+' '+metadata['bmsName'].fillna('')+metadata['unit'].fillna('')\n",
    "\n",
    "#Also create a uniqueId without a unit - this ends up being more useful for joining\n",
    "metadata['uniqueId_nounit']=metadata['equipRefclean'].fillna('')+' '+metadata['groupRefclean'].fillna('')+' '+metadata['navName'].fillna('')+ \\\n",
    "' '+metadata['siteRefclean'].fillna('')+' '+metadata['bmsName'].fillna('')+metadata['unit'].fillna('')\n",
    "\n",
    "#Confirm that the unique ID in each data frame is actually unique!\n",
    "print(\"There are\",len(metadata),\"sensors in the Pharmacy building's skyspark metadata\")\n",
    "\n",
    "print(sum(metadata.duplicated(subset=[\"uniqueId_nounit\"], keep=False)), \"conflicting entries in the dataframe of skyspark data (NOT including unit as part of unique 'key'.\")\n",
    "print(sum(metadata.duplicated(subset=[\"uniqueId\"], keep=False)), \"conflicting entries in the dataframe of skyspark data (including unit as part of unique 'key'). Saving as 'Pharmacy-Conflicts-In-Metadata.csv'\")\n",
    "\n",
    "\n",
    "(metadata[metadata.duplicated(subset=[\"uniqueId\"], keep=False)]).to_csv(\"Pharmacy-Conflicts-In-Metadata.csv\")\n",
    "\n",
    "\n",
    "#We'll use this dataframe later for joining in influx data\n",
    "metadata_no_conflicts = metadata.drop_duplicates(subset=['uniqueId_nounit'], keep=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Records in SkySpark that can not be found in influxDB (there are ~6000 of them)\n",
    "\n",
    "and\n",
    "\n",
    "### 3) Records in SkySpark that **CAN** be found in influxDB but only after a certain date\n",
    "\n",
    "This is a much lengthier process then part 1. First need to query the influxDB, add a uniqueId field, and do some joining with the metadata. In the process, we'll be answering part 3.\n",
    "\n",
    "End result will be a csv with all the SkySpark records that need to be requeried over the entire date range (including those from part 1) and a csv with all the SkySpark records that need to be requeried up until (and including) the specific date listed for that records.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 buildings found in the siteRef tag\n"
     ]
    }
   ],
   "source": [
    "#Get a list of all buildings stored in the siteRef tag\n",
    "#This code is not used but I'm leaving it here in case you want to implement it to loop through\n",
    "#All buildings instead of just Pharmacy\n",
    "\n",
    "results = client.query('SHOW TAG VALUES ON \"SKYSPARK\" WITH KEY = \"siteRef\";')\n",
    "site_list=[]\n",
    "for item in results[\"UBC_EWS\"]:\n",
    "    site_list.append(item[\"value\"])\n",
    "print(len(site_list),\"buildings found in the siteRef tag\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query influxDB to get the time stamp of the last value stored for each sensor\n",
    "\n",
    "(assuming that a combination of `groupRef`, `equipRef`, `typeRef`, `navName`, and `unit` uniquely identify a sensor - which as seen from part 1a is not true. This is the best we can do with the data though)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the database for all of the first time stamps for each sensor \n",
    "# (have to do first(value) to make influxDB happy even though I don't care about the value)\n",
    "\n",
    "#JUST LOOKING AT A SINGLE BUILDING so setting site=\"Pharmacy\"\n",
    "#But you can imagine we could do a for loop here and encapsulate all the following cells of code in it\n",
    "#something like: `for site in site_list:`\n",
    "site=\"Pharmacy\"\n",
    "\n",
    "query = '''select first(value), groupRef, equipRef, typeRef, navName, unit from UBC_EWS \n",
    "where siteRef=$siteRef group By groupRef, equipRef,typeRef,navName,unit'''\n",
    "\n",
    "where_params = {'siteRef': site}\n",
    "result = client.query(query = query, bind_params=where_params, chunked=True, chunk_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turn results of query into a dataframe\n",
    "#(I wasn't able to make the influxdb-python DataFrameClient object work for a query with grouping!)\n",
    "list_of_df=[]\n",
    "n=0\n",
    "for item in result.get_points(measurement=\"UBC_EWS\"):\n",
    "    list_of_df.append(pd.DataFrame.from_dict(item,orient=\"index\"))\n",
    "\n",
    "rawdf = pd.concat(list_of_df, axis=1)\n",
    "rawdf = rawdf.transpose(copy=True)\n",
    "rawdf.reset_index(drop=True, inplace=True)\n",
    "\n",
    "list_of_df=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get a date column from the timestamp 'time' column\n",
    "rawdf['time'] = pd.to_datetime(rawdf['time'])\n",
    "rawdf['firstdate'] = rawdf['time'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of supposedly unique Pharmacy sensors in influxDB 17584\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of supposedly unique Pharmacy sensors in influxDB\",len(rawdf))\n",
    "\n",
    "#This is kinda like a reset point so if things get messed up in the process below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next we will create a uniqueId in the influx data to be able to join it with the metadata. \n",
    "\n",
    "The tricky bit is that in influx, many of the records don't have equipRef tags that start with \"Pharmacy\" while basically every equipRef tag in the SkySpark data starts with \"Pharmacy\"..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7752 / 7753 SkySpark records have equipRef that starts with Pharmacy\n",
      "4303 / 17584 influx records have equipRef that starts with Pharmacy\n"
     ]
    }
   ],
   "source": [
    "#equipRef is an issue: In influxDB some records start with 'Pharmacy' (just like in SkySpark)\n",
    "# however the majority do not. This needs to be fixed to be able to link these records back to \n",
    "\n",
    "print(metadata_no_conflicts['equipRefclean'].str.startswith(\"Pharmacy \").sum(),\"/\",len(metadata_no_conflicts), \"SkySpark records have equipRef that starts with Pharmacy\")\n",
    "print(rawdf['equipRef'].str.startswith(\"Pharmacy \").sum(),\"/\",len(rawdf), \"influx records have equipRef that starts with Pharmacy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17584 / 17584 influx records now have equipRef_fix value that starts with Pharmacy\n"
     ]
    }
   ],
   "source": [
    "#Create uniqueID by concatenating specific fields in the data queried from influxDB\n",
    "\n",
    "#Make copy so that we don't have to run a bunch of cells every time we want to make a change to the code below\n",
    "influxdf = rawdf.copy() \n",
    "\n",
    "influxdf['equipRef_fix'] = influxdf['equipRef'].apply(lambda x: x if (x.startswith(site)) else site+\" \"+x)\n",
    "print(influxdf['equipRef_fix'].str.startswith(\"Pharmacy \").sum(),\"/\",len(influxdf), \"influx records now have equipRef_fix value that starts with Pharmacy\")\n",
    "\n",
    "influxdf['uniqueId_nounit']=influxdf['equipRef_fix'].fillna('')+' '+influxdf['groupRef'].fillna('')+' '+influxdf['navName'].fillna('')+ \\\n",
    "' '+site+' '+influxdf['typeRef'].fillna('')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've corrected all the sensors that didn't have 'Pharmacy' at the start of the equipRef, we need to basically regroup everything into series and end up with only the First Time stamp for each series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Influx series (sensors) before dropping newer instances of series: 17584\n",
      "Influx series (sensors) after retaining only the earliest instance of a series: 8660\n"
     ]
    }
   ],
   "source": [
    "influxdf_nodup = influxdf.sort_values(by='time', ascending=True).drop_duplicates(subset='uniqueId_nounit', keep=\"first\")\n",
    "print(\"Influx series (sensors) before dropping newer instances of series:\", len(influxdf))\n",
    "print(\"Influx series (sensors) after retaining only the earliest instance of a series:\", len(influxdf_nodup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally can join the influx data with the SkySpark data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of SkySpark records after dropping conflicts: 7753\n",
      "Number of unique influx series: 8660\n",
      "Number of influx records that can be linked back to skyspark records: 1747\n"
     ]
    }
   ],
   "source": [
    "\n",
    "joined_df = pd.merge(left=metadata_no_conflicts, right=influxdf_nodup, how='inner', on=['uniqueId_nounit'], suffixes=('_sspark','_influx') )\n",
    "print(\"Number of SkySpark records after dropping conflicts:\",len(metadata_no_conflicts))\n",
    "print(\"Number of unique influx series:\",len(influxdf_nodup))\n",
    "print(\"Number of influx records that can be linked back to skyspark records:\",len(joined_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now create some csvs of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Influx data that only needs to be requeried before 'firstdate' is saved as 'Pharmacy-Records-To-Keep.csv'\n",
      "Number of SkySpark records with no linkable instance in influx data (not including the 185 conflicts in SkySpark): 6006\n",
      "Number of SkySpark records with no linkable instance in influx data 6191\n",
      "Saving as 'Pharmacy-Records-To-Requery.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\api.py:107: RuntimeWarning: '<' not supported between instances of 'str' and 'int', sort order is undefined for incomparable objects\n",
      "  result = result.union(other)\n"
     ]
    }
   ],
   "source": [
    "#Select only the useful fields for identifying the records\n",
    "joined_df=joined_df[['uniqueId_nounit','id','firstdate','time','groupRef_influx',\n",
    "                     'equipRef_fix','equipRef_influx','navName_influx','siteRef','typeRef','unit_sspark','unit_influx','kind']]\n",
    "\n",
    "#Save the records in a .csv\n",
    "\n",
    "print(\"Influx data that only needs to be requeried before 'firstdate' is saved as 'Pharmacy-Records-To-Keep.csv'\")\n",
    "\n",
    "joined_df.to_csv(\"Pharmacy-Records-To-Keep.csv\")\n",
    "\n",
    "#Also make a csv of all the records that need to be re-queried in their entirety. \n",
    "# Using in indicator argument allows us to do a join and then only keep the records that only belong\n",
    "# to the left dataframe. Super handy!\n",
    "\n",
    "nokeep = pd.merge(left=metadata_no_conflicts, right=influxdf_nodup, on=['uniqueId_nounit'], how='left',indicator=True)\n",
    "nokeep = nokeep[nokeep['_merge'] == 'left_only'][['id','groupRefclean','equipRefclean','siteRef','navName_x','bmsName','unit_x','kind']]\n",
    "\n",
    "\n",
    "#Add the problematic records back in - the ones that can't be uniquely indexed by the main identifying fields used in influx\n",
    "conflicting_records = metadata[metadata.duplicated(subset=[\"uniqueId_nounit\"], keep=False)]['id']\n",
    "print(\"Number of SkySpark records with no linkable instance in influx data (not including the 185 conflicts in SkySpark):\",len(nokeep))\n",
    "print(\"Number of SkySpark records with no linkable instance in influx data\",len(nokeep)+len(conflicting_records))\n",
    "print(\"Saving as 'Pharmacy-Records-To-Requery.csv'\")\n",
    "pd.concat([nokeep,conflicting_records], axis=0).reset_index().to_csv(\"Pharmacy-Records-To-Requery.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
