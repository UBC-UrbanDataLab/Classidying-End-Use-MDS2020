{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0MRC0e0KhQ0S"
   },
   "source": [
    "# Multi-Model Classification Comparison Template\n",
    "This is a one file classificaiton model comparison template that allows quick comparison of the following models:\n",
    "* Decision Tree Classification (sds)\n",
    "* K-Nearest Neighbours (sds)\n",
    "* Kernel SVM (sds)\n",
    "* Logistic Regression (sds)\n",
    "* Naive Bayes (sds)\n",
    "* Random Forest Classification (sds)\n",
    "* Support Vector Machines (sds)\n",
    "* Extremely Randomized Trees (cl)\n",
    "* Bagging Classifier (cl)\n",
    "* AdaBoost (cl)\n",
    "* Gradient Boosting Classifier (cl)\n",
    "\n",
    "The available metrics that are used for comparison are listed below:\n",
    "* Accuracy (sds)\n",
    "* Precision (cl)\n",
    "* Recall/Sensitivity (cl)\n",
    "* F1 Score (cl)\n",
    "* Logloss (cl)\n",
    "* Specificity (not incorporated yet)\n",
    "\n",
    "This was setup to allow the user to pass in the path to their .csv file as a string stored in the `path` variable in the **Import the dataset** section below. The template has been setup such that any number of predictor variables can be used, however the response variable **MUST** be the last column in the .csv. \n",
    "\n",
    "This model template was adapted from the Super Data Science Machine Learning A-Z Udemy Course Model Selection templates. This template is a combination of all of the classification templates they had (denoted by sds above), and I have added more models and comparison metrics (denoted by cl above).\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'step4_data.csv'\n",
    "dataset = pd.read_csv(path)\n",
    "cols = dataset.columns.tolist()\n",
    "cols.remove('NRCan')\n",
    "cols.append('NRCan')\n",
    "dataset = dataset[cols]\n",
    "dataset = dataset.iloc[:,2:]\n",
    "dataset = dataset.fillna(0)\n",
    "dataset['NRCan'] = dataset['NRCan'].apply(lambda x: int(x[0]))\n",
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['decision_tree', 'knn', 'kernel_svm', 'logistic_regression', 'naive_bayse', 'random_forest', 'svm', 'extremely_random_trees', 'bagging', 'adaboost', 'gradientboost']\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'logloss']\n",
    "results = pd.DataFrame(index=models, columns=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Decision Tree Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bb6jCOCQiAmP"
   },
   "source": [
    "### Training the Decision Tree Classification model on the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e0pFVAmciHQs"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',\n",
       "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                       random_state=0, splitter='best')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h4Hwj34ziWQW"
   },
   "source": [
    "### Making the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D6bpZwUiiXic"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5  0  0  0]\n",
      " [ 0 10  0  0]\n",
      " [ 0  0 10  0]\n",
      " [ 0  0  0 40]]\n",
      "accuracy: 1.0\n",
      "precision: 1.0\n",
      "recall: 1.0\n",
      "f1: 1.0\n",
      "logloss: 3.0830039376129395e-15\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, log_loss\n",
    "y_pred = classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "# Caclulating and displaying comparison metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "logloss = log_loss(y_true=y_test, y_pred=classifier.predict_proba(X_test))\n",
    "print(\"accuracy: \"+str(accuracy))\n",
    "print(\"precision: \"+str(precision))\n",
    "print(\"recall: \"+str(recall))\n",
    "print(\"f1: \"+str(f1))\n",
    "print(\"logloss: \"+str(logloss))\n",
    "\n",
    "# Storing comparison metrics in dataframe\n",
    "results.loc['decision_tree', 'accuracy'] = accuracy\n",
    "results.loc['decision_tree', 'precision'] = precision\n",
    "results.loc['decision_tree', 'recall'] = recall\n",
    "results.loc['decision_tree', 'f1_score'] = f1\n",
    "results.loc['decision_tree', 'logloss'] = logloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## K-Nearest Neighbors (K-NN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the K-NN model on the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5  0  0  0]\n",
      " [ 0 10  0  0]\n",
      " [ 0  0 10  0]\n",
      " [ 0  0  0 40]]\n",
      "accuracy: 1.0\n",
      "precision: 1.0\n",
      "recall: 1.0\n",
      "f1: 1.0\n",
      "logloss: 0.019150689213021366\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, log_loss\n",
    "y_pred = classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "# Caclulating and displaying comparison metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "logloss = log_loss(y_true=y_test, y_pred=classifier.predict_proba(X_test))\n",
    "print(\"accuracy: \"+str(accuracy))\n",
    "print(\"precision: \"+str(precision))\n",
    "print(\"recall: \"+str(recall))\n",
    "print(\"f1: \"+str(f1))\n",
    "print(\"logloss: \"+str(logloss))\n",
    "\n",
    "# Storing comparison metrics in dataframe\n",
    "results.loc['knn', 'accuracy'] = accuracy\n",
    "results.loc['knn', 'precision'] = precision\n",
    "results.loc['knn', 'recall'] = recall\n",
    "results.loc['knn', 'f1_score'] = f1\n",
    "results.loc['knn', 'logloss'] = logloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Kernel SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Kernel SVM model on the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=True, random_state=0, shrinking=True, tol=0.001,\n",
       "    verbose=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'rbf', random_state = 0, probability=True)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5  0  0  0]\n",
      " [ 0 10  0  0]\n",
      " [ 0  0  9  1]\n",
      " [ 0  0  0 40]]\n",
      "accuracy: 0.9846153846153847\n",
      "precision: 0.9849906191369607\n",
      "recall: 0.9846153846153847\n",
      "f1: 0.9843054930774229\n",
      "logloss: 0.1283960423540199\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, log_loss\n",
    "y_pred = classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "# Caclulating and displaying comparison metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "logloss = log_loss(y_true=y_test, y_pred=classifier.predict_proba(X_test))\n",
    "print(\"accuracy: \"+str(accuracy))\n",
    "print(\"precision: \"+str(precision))\n",
    "print(\"recall: \"+str(recall))\n",
    "print(\"f1: \"+str(f1))\n",
    "print(\"logloss: \"+str(logloss))\n",
    "\n",
    "# Storing comparison metrics in dataframe\n",
    "results.loc['kernel_svm', 'accuracy'] = accuracy\n",
    "results.loc['kernel_svm', 'precision'] = precision\n",
    "results.loc['kernel_svm', 'recall'] = recall\n",
    "results.loc['kernel_svm', 'f1_score'] = f1\n",
    "results.loc['kernel_svm', 'logloss'] = logloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Logistic Regression model on the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=0, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state = 0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5  0  0  0]\n",
      " [ 0 10  0  0]\n",
      " [ 0  0  9  1]\n",
      " [ 0  0  0 40]]\n",
      "accuracy: 0.9846153846153847\n",
      "precision: 0.9849906191369607\n",
      "recall: 0.9846153846153847\n",
      "f1: 0.9843054930774229\n",
      "logloss: 0.12163248939752071\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, log_loss\n",
    "y_pred = classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "# Caclulating and displaying comparison metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "logloss = log_loss(y_true=y_test, y_pred=classifier.predict_proba(X_test))\n",
    "print(\"accuracy: \"+str(accuracy))\n",
    "print(\"precision: \"+str(precision))\n",
    "print(\"recall: \"+str(recall))\n",
    "print(\"f1: \"+str(f1))\n",
    "print(\"logloss: \"+str(logloss))\n",
    "\n",
    "# Storing comparison metrics in dataframe\n",
    "results.loc['logistic_regression', 'accuracy'] = accuracy\n",
    "results.loc['logistic_regression', 'precision'] = precision\n",
    "results.loc['logistic_regression', 'recall'] = recall\n",
    "results.loc['logistic_regression', 'f1_score'] = f1\n",
    "results.loc['logistic_regression', 'logloss'] = logloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Naive Bayes model on the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5  0  0  0]\n",
      " [ 0 10  0  0]\n",
      " [ 0  0  9  1]\n",
      " [ 0  0  0 40]]\n",
      "accuracy: 0.9846153846153847\n",
      "precision: 0.9849906191369607\n",
      "recall: 0.9846153846153847\n",
      "f1: 0.9843054930774229\n",
      "logloss: 0.5313699417109006\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, log_loss\n",
    "y_pred = classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "# Caclulating and displaying comparison metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "logloss = log_loss(y_true=y_test, y_pred=classifier.predict_proba(X_test))\n",
    "print(\"accuracy: \"+str(accuracy))\n",
    "print(\"precision: \"+str(precision))\n",
    "print(\"recall: \"+str(recall))\n",
    "print(\"f1: \"+str(f1))\n",
    "print(\"logloss: \"+str(logloss))\n",
    "\n",
    "# Storing comparison metrics in dataframe\n",
    "results.loc['naive_bayse', 'accuracy'] = accuracy\n",
    "results.loc['naive_bayse', 'precision'] = precision\n",
    "results.loc['naive_bayse', 'recall'] = recall\n",
    "results.loc['naive_bayse', 'f1_score'] = f1\n",
    "results.loc['naive_bayse', 'logloss'] = logloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Random Forest Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Random Forest Classification model on the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators = 100, criterion = 'gini', random_state = 0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5  0  0  0]\n",
      " [ 2  8  0  0]\n",
      " [ 0  0 10  0]\n",
      " [ 0  0  0 40]]\n",
      "accuracy: 0.9692307692307692\n",
      "precision: 0.978021978021978\n",
      "recall: 0.9692307692307692\n",
      "f1: 0.9700854700854701\n",
      "logloss: 0.0925444984460965\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, log_loss\n",
    "y_pred = classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "# Caclulating and displaying comparison metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "logloss = log_loss(y_true=y_test, y_pred=classifier.predict_proba(X_test))\n",
    "print(\"accuracy: \"+str(accuracy))\n",
    "print(\"precision: \"+str(precision))\n",
    "print(\"recall: \"+str(recall))\n",
    "print(\"f1: \"+str(f1))\n",
    "print(\"logloss: \"+str(logloss))\n",
    "\n",
    "# Storing comparison metrics in dataframe\n",
    "results.loc['random_forest', 'accuracy'] = accuracy\n",
    "results.loc['random_forest', 'precision'] = precision\n",
    "results.loc['random_forest', 'recall'] = recall\n",
    "results.loc['random_forest', 'f1_score'] = f1\n",
    "results.loc['random_forest', 'logloss'] = logloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the SVM model on the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
       "    max_iter=-1, probability=True, random_state=0, shrinking=True, tol=0.001,\n",
       "    verbose=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'linear', random_state = 0, probability=True)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5  0  0  0]\n",
      " [ 0 10  0  0]\n",
      " [ 0  0  9  1]\n",
      " [ 0  0  0 40]]\n",
      "accuracy: 0.9846153846153847\n",
      "precision: 0.9849906191369607\n",
      "recall: 0.9846153846153847\n",
      "f1: 0.9843054930774229\n",
      "logloss: 0.14892696341140402\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, log_loss\n",
    "y_pred = classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "# Caclulating and displaying comparison metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "logloss = log_loss(y_true=y_test, y_pred=classifier.predict_proba(X_test))\n",
    "print(\"accuracy: \"+str(accuracy))\n",
    "print(\"precision: \"+str(precision))\n",
    "print(\"recall: \"+str(recall))\n",
    "print(\"f1: \"+str(f1))\n",
    "print(\"logloss: \"+str(logloss))\n",
    "\n",
    "# Storing comparison metrics in dataframe\n",
    "results.loc['svm', 'accuracy'] = accuracy\n",
    "results.loc['svm', 'precision'] = precision\n",
    "results.loc['svm', 'recall'] = recall\n",
    "results.loc['svm', 'f1_score'] = f1\n",
    "results.loc['svm', 'logloss'] = logloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Extremely Randomized Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Extremely Randomized Tree model on the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
       "                     criterion='gini', max_depth=None, max_features='auto',\n",
       "                     max_leaf_nodes=None, max_samples=None,\n",
       "                     min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                     min_samples_leaf=1, min_samples_split=2,\n",
       "                     min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                     n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
       "                     warm_start=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "classifier = ExtraTreesClassifier(n_estimators = 100, criterion = 'gini', random_state = 0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5  0  0  0]\n",
      " [ 0 10  0  0]\n",
      " [ 0  0 10  0]\n",
      " [ 0  0  0 40]]\n",
      "accuracy: 1.0\n",
      "precision: 1.0\n",
      "recall: 1.0\n",
      "f1: 1.0\n",
      "logloss: 0.018632875252232426\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, log_loss\n",
    "y_pred = classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "# Caclulating and displaying comparison metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "logloss = log_loss(y_true=y_test, y_pred=classifier.predict_proba(X_test))\n",
    "print(\"accuracy: \"+str(accuracy))\n",
    "print(\"precision: \"+str(precision))\n",
    "print(\"recall: \"+str(recall))\n",
    "print(\"f1: \"+str(f1))\n",
    "print(\"logloss: \"+str(logloss))\n",
    "\n",
    "# Storing comparison metrics in dataframe\n",
    "results.loc['extremely_random_trees', 'accuracy'] = accuracy\n",
    "results.loc['extremely_random_trees', 'precision'] = precision\n",
    "results.loc['extremely_random_trees', 'recall'] = recall\n",
    "results.loc['extremely_random_trees', 'f1_score'] = f1\n",
    "results.loc['extremely_random_trees', 'logloss'] = logloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Bagging Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Bagging Classifier model on the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=None, bootstrap=True, bootstrap_features=False,\n",
       "                  max_features=1.0, max_samples=1.0, n_estimators=100,\n",
       "                  n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
       "                  warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "classifier = BaggingClassifier(n_estimators = 100, random_state = 0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5  0  0  0]\n",
      " [ 0 10  0  0]\n",
      " [ 0  0 10  0]\n",
      " [ 0  0  1 39]]\n",
      "accuracy: 0.9846153846153847\n",
      "precision: 0.986013986013986\n",
      "recall: 0.9846153846153847\n",
      "f1: 0.9848843139982382\n",
      "logloss: 0.052285585821714664\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, log_loss\n",
    "y_pred = classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "# Caclulating and displaying comparison metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "logloss = log_loss(y_true=y_test, y_pred=classifier.predict_proba(X_test))\n",
    "print(\"accuracy: \"+str(accuracy))\n",
    "print(\"precision: \"+str(precision))\n",
    "print(\"recall: \"+str(recall))\n",
    "print(\"f1: \"+str(f1))\n",
    "print(\"logloss: \"+str(logloss))\n",
    "\n",
    "# Storing comparison metrics in dataframe\n",
    "results.loc['bagging', 'accuracy'] = accuracy\n",
    "results.loc['bagging', 'precision'] = precision\n",
    "results.loc['bagging', 'recall'] = recall\n",
    "results.loc['bagging', 'f1_score'] = f1\n",
    "results.loc['bagging', 'logloss'] = logloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the AdaBoost Classifier model on the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                   n_estimators=100, random_state=0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "classifier = AdaBoostClassifier(n_estimators = 100, random_state = 0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4  1  0  0]\n",
      " [ 0 10  0  0]\n",
      " [ 0  0  9  1]\n",
      " [ 0  0  0 40]]\n",
      "accuracy: 0.9692307692307692\n",
      "precision: 0.9710046051509466\n",
      "recall: 0.9692307692307692\n",
      "f1: 0.968432477204407\n",
      "logloss: 0.5907685429166434\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, log_loss\n",
    "y_pred = classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "# Caclulating and displaying comparison metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "logloss = log_loss(y_true=y_test, y_pred=classifier.predict_proba(X_test))\n",
    "print(\"accuracy: \"+str(accuracy))\n",
    "print(\"precision: \"+str(precision))\n",
    "print(\"recall: \"+str(recall))\n",
    "print(\"f1: \"+str(f1))\n",
    "print(\"logloss: \"+str(logloss))\n",
    "\n",
    "# Storing comparison metrics in dataframe\n",
    "results.loc['adaboost', 'accuracy'] = accuracy\n",
    "results.loc['adaboost', 'precision'] = precision\n",
    "results.loc['adaboost', 'recall'] = recall\n",
    "results.loc['adaboost', 'f1_score'] = f1\n",
    "results.loc['adaboost', 'logloss'] = logloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Gradient Boosting Classifier model on the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
       "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "                           max_features=None, max_leaf_nodes=None,\n",
       "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                           min_samples_leaf=1, min_samples_split=2,\n",
       "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                           n_iter_no_change=None, presort='deprecated',\n",
       "                           random_state=0, subsample=1.0, tol=0.0001,\n",
       "                           validation_fraction=0.1, verbose=0,\n",
       "                           warm_start=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "classifier = GradientBoostingClassifier(n_estimators = 100, loss='deviance', criterion='friedman_mse', random_state = 0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5  0  0  0]\n",
      " [ 0 10  0  0]\n",
      " [ 0  0 10  0]\n",
      " [ 0  0  0 40]]\n",
      "accuracy: 1.0\n",
      "precision: 1.0\n",
      "recall: 1.0\n",
      "f1: 1.0\n",
      "logloss: 0.0003189946738986487\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, log_loss\n",
    "y_pred = classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "# Caclulating and displaying comparison metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "logloss = log_loss(y_true=y_test, y_pred=classifier.predict_proba(X_test))\n",
    "print(\"accuracy: \"+str(accuracy))\n",
    "print(\"precision: \"+str(precision))\n",
    "print(\"recall: \"+str(recall))\n",
    "print(\"f1: \"+str(f1))\n",
    "print(\"logloss: \"+str(logloss))\n",
    "\n",
    "# Storing comparison metrics in dataframe\n",
    "results.loc['gradientboost', 'accuracy'] = accuracy\n",
    "results.loc['gradientboost', 'precision'] = precision\n",
    "results.loc['gradientboost', 'recall'] = recall\n",
    "results.loc['gradientboost', 'f1_score'] = f1\n",
    "results.loc['gradientboost', 'logloss'] = logloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Summary of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>logloss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>decision_tree</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.083e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0191507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kernel_svm</th>\n",
       "      <td>0.984615</td>\n",
       "      <td>0.984991</td>\n",
       "      <td>0.984615</td>\n",
       "      <td>0.984305</td>\n",
       "      <td>0.128396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logistic_regression</th>\n",
       "      <td>0.984615</td>\n",
       "      <td>0.984991</td>\n",
       "      <td>0.984615</td>\n",
       "      <td>0.984305</td>\n",
       "      <td>0.121632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>naive_bayse</th>\n",
       "      <td>0.984615</td>\n",
       "      <td>0.984991</td>\n",
       "      <td>0.984615</td>\n",
       "      <td>0.984305</td>\n",
       "      <td>0.53137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random_forest</th>\n",
       "      <td>0.969231</td>\n",
       "      <td>0.978022</td>\n",
       "      <td>0.969231</td>\n",
       "      <td>0.970085</td>\n",
       "      <td>0.0925445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>0.984615</td>\n",
       "      <td>0.984991</td>\n",
       "      <td>0.984615</td>\n",
       "      <td>0.984305</td>\n",
       "      <td>0.148927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>extremely_random_trees</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0186329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bagging</th>\n",
       "      <td>0.984615</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.984615</td>\n",
       "      <td>0.984884</td>\n",
       "      <td>0.0522856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adaboost</th>\n",
       "      <td>0.969231</td>\n",
       "      <td>0.971005</td>\n",
       "      <td>0.969231</td>\n",
       "      <td>0.968432</td>\n",
       "      <td>0.590769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gradientboost</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000318995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        accuracy precision    recall  f1_score      logloss\n",
       "decision_tree                  1         1         1         1    3.083e-15\n",
       "knn                            1         1         1         1    0.0191507\n",
       "kernel_svm              0.984615  0.984991  0.984615  0.984305     0.128396\n",
       "logistic_regression     0.984615  0.984991  0.984615  0.984305     0.121632\n",
       "naive_bayse             0.984615  0.984991  0.984615  0.984305      0.53137\n",
       "random_forest           0.969231  0.978022  0.969231  0.970085    0.0925445\n",
       "svm                     0.984615  0.984991  0.984615  0.984305     0.148927\n",
       "extremely_random_trees         1         1         1         1    0.0186329\n",
       "bagging                 0.984615  0.986014  0.984615  0.984884    0.0522856\n",
       "adaboost                0.969231  0.971005  0.969231  0.968432     0.590769\n",
       "gradientboost                  1         1         1         1  0.000318995"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss\t     decision_tree\n",
      "accuracy     decision_tree\n",
      "precision    decision_tree\n",
      "recall       decision_tree\n",
      "f1_score     decision_tree\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "results = results.astype(float)\n",
    "print('logloss\\t     '+results.iloc[:,-1].idxmin())\n",
    "print(results.iloc[:,:-1].idxmax())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "decision_tree_classification.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
