{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0MRC0e0KhQ0S"
   },
   "source": [
    "# Multi-Model Classification Comparison Template\n",
    "This is a one file classificaiton model comparison template that allows quick comparison of the following models:\n",
    "* Decision Tree Classification (sds)\n",
    "* K-Nearest Neighbours (sds)\n",
    "* Kernel SVM (sds)\n",
    "* Logistic Regression (sds)\n",
    "* Naive Bayes (sds)\n",
    "* Random Forest Classification (sds)\n",
    "* Support Vector Machines (sds)\n",
    "* Extremely Randomized Trees (cl)\n",
    "* Bagging Classifier (cl)\n",
    "* AdaBoost (cl)\n",
    "* Gradient Boosting Classifier (cl)\n",
    "\n",
    "The available metrics that are used for comparison are listed below:\n",
    "* Accuracy (sds)\n",
    "* Precision (cl)\n",
    "* Recall/Sensitivity (cl)\n",
    "* F1 Score (cl)\n",
    "* Logloss (cl)\n",
    "* Specificity (not incorporated yet)\n",
    "\n",
    "This was setup to allow the user to pass in the path to their .csv file as a string stored in the `path` variable in the **Import the dataset** section below. The template has been setup such that any number of predictor variables can be used, however the response variable **MUST** be the last column in the .csv. \n",
    "\n",
    "This model template was adapted from the Super Data Science Machine Learning A-Z Udemy Course Model Selection templates. This template is a combination of all of the classification templates they had (denoted by sds above), and I have added more models and comparison metrics (denoted by cl above).\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the dataset"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "path = 'step4_data.csv'\n",
    "dataset = pd.read_csv(path)\n",
    "cols = dataset.columns.tolist()\n",
    "cols.remove('NRCan')\n",
    "cols.append('NRCan')\n",
    "dataset = dataset[cols]\n",
    "dataset = dataset.iloc[:,2:]\n",
    "dataset = dataset.fillna(0)\n",
    "dataset['NRCan'] = dataset['NRCan'].apply(lambda x: int(x[0]))\n",
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv('../../data_files/pre-processed_data/regression_outputs/single_20_regression_output.csv') # Reading in the training data file\n",
    "# Manipulating dataset to be in the appropriate format for creating seperate predictor and response datasets\n",
    "cols = training_data.columns.tolist()\n",
    "cols.remove('endUseLabel')\n",
    "cols.append('endUseLabel')\n",
    "training_data = training_data[cols]\n",
    "training_data = training_data.iloc[:,2:]\n",
    "training_data = training_data.fillna(0)\n",
    "# Extracting just the number from the label\n",
    "training_data['endUseLabel'] = training_data['endUseLabel'].apply(lambda x: int(str(x)[0:2]))\n",
    "training_data=training_data[(training_data['endUseLabel']!=99)]\n",
    "# Creating seperate predictor variable and response variable numpy arrays\n",
    "X = training_data.iloc[:, :-1].values\n",
    "y = training_data.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['decision_tree', 'knn', 'kernel_svm', 'logistic_regression', 'naive_bayse', 'random_forest', 'svm', 'extremely_random_trees', 'bagging', 'adaboost', 'gradientboost']\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'logloss']\n",
    "results = pd.DataFrame(index=models, columns=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Decision Tree Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bb6jCOCQiAmP"
   },
   "source": [
    "### Training the Decision Tree Classification model on the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e0pFVAmciHQs"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',\n",
       "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                       random_state=0, splitter='best')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h4Hwj34ziWQW"
   },
   "source": [
    "### Making the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D6bpZwUiiXic"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10  0  0  0  0  0]\n",
      " [ 0  5  0  0  0  0]\n",
      " [ 0  0  8  0  0  1]\n",
      " [ 0  0  0  2  0  0]\n",
      " [ 0  0  0  0  2  0]\n",
      " [ 0  0  0  1  0  6]]\n",
      "accuracy: 0.9428571428571428\n",
      "precision: 0.9523809523809522\n",
      "recall: 0.9428571428571428\n",
      "f1: 0.944873949579832\n",
      "logloss: 1.973644365423473\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, log_loss\n",
    "y_pred = classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "# Caclulating and displaying comparison metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "logloss = log_loss(y_true=y_test, y_pred=classifier.predict_proba(X_test))\n",
    "print(\"accuracy: \"+str(accuracy))\n",
    "print(\"precision: \"+str(precision))\n",
    "print(\"recall: \"+str(recall))\n",
    "print(\"f1: \"+str(f1))\n",
    "print(\"logloss: \"+str(logloss))\n",
    "\n",
    "# Storing comparison metrics in dataframe\n",
    "results.loc['decision_tree', 'accuracy'] = accuracy\n",
    "results.loc['decision_tree', 'precision'] = precision\n",
    "results.loc['decision_tree', 'recall'] = recall\n",
    "results.loc['decision_tree', 'f1_score'] = f1\n",
    "results.loc['decision_tree', 'logloss'] = logloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## K-Nearest Neighbors (K-NN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the K-NN model on the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10  0  0  0  0  0]\n",
      " [ 0  5  0  0  0  0]\n",
      " [ 1  0  7  0  0  1]\n",
      " [ 0  0  0  2  0  0]\n",
      " [ 0  0  0  0  2  0]\n",
      " [ 0  0  0  1  0  6]]\n",
      "accuracy: 0.9142857142857143\n",
      "precision: 0.9264069264069263\n",
      "recall: 0.9142857142857143\n",
      "f1: 0.9142517006802722\n",
      "logloss: 1.237491301716211\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, log_loss\n",
    "y_pred = classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "# Caclulating and displaying comparison metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "logloss = log_loss(y_true=y_test, y_pred=classifier.predict_proba(X_test))\n",
    "print(\"accuracy: \"+str(accuracy))\n",
    "print(\"precision: \"+str(precision))\n",
    "print(\"recall: \"+str(recall))\n",
    "print(\"f1: \"+str(f1))\n",
    "print(\"logloss: \"+str(logloss))\n",
    "\n",
    "# Storing comparison metrics in dataframe\n",
    "results.loc['knn', 'accuracy'] = accuracy\n",
    "results.loc['knn', 'precision'] = precision\n",
    "results.loc['knn', 'recall'] = recall\n",
    "results.loc['knn', 'f1_score'] = f1\n",
    "results.loc['knn', 'logloss'] = logloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Kernel SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Kernel SVM model on the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=True, random_state=0, shrinking=True, tol=0.001,\n",
       "    verbose=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'rbf', random_state = 0, probability=True)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10  0  0  0  0  0]\n",
      " [ 0  4  0  0  0  1]\n",
      " [ 0  0  9  0  0  0]\n",
      " [ 0  0  0  2  0  0]\n",
      " [ 0  0  0  2  0  0]\n",
      " [ 0  0  2  1  0  4]]\n",
      "accuracy: 0.8285714285714286\n",
      "precision: 0.8218181818181819\n",
      "recall: 0.8285714285714286\n",
      "f1: 0.8101133786848073\n",
      "logloss: 0.5347282236561574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/connor/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, log_loss\n",
    "y_pred = classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "# Caclulating and displaying comparison metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "logloss = log_loss(y_true=y_test, y_pred=classifier.predict_proba(X_test))\n",
    "print(\"accuracy: \"+str(accuracy))\n",
    "print(\"precision: \"+str(precision))\n",
    "print(\"recall: \"+str(recall))\n",
    "print(\"f1: \"+str(f1))\n",
    "print(\"logloss: \"+str(logloss))\n",
    "\n",
    "# Storing comparison metrics in dataframe\n",
    "results.loc['kernel_svm', 'accuracy'] = accuracy\n",
    "results.loc['kernel_svm', 'precision'] = precision\n",
    "results.loc['kernel_svm', 'recall'] = recall\n",
    "results.loc['kernel_svm', 'f1_score'] = f1\n",
    "results.loc['kernel_svm', 'logloss'] = logloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Logistic Regression model on the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=0, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state = 0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10  0  0  0  0  0]\n",
      " [ 0  4  0  0  0  1]\n",
      " [ 1  0  8  0  0  0]\n",
      " [ 0  0  0  2  0  0]\n",
      " [ 0  0  0  1  0  1]\n",
      " [ 0  0  2  1  0  4]]\n",
      "accuracy: 0.8\n",
      "precision: 0.7702164502164501\n",
      "recall: 0.8\n",
      "f1: 0.7768064850771618\n",
      "logloss: 0.49058690334057675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/connor/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, log_loss\n",
    "y_pred = classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "# Caclulating and displaying comparison metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "logloss = log_loss(y_true=y_test, y_pred=classifier.predict_proba(X_test))\n",
    "print(\"accuracy: \"+str(accuracy))\n",
    "print(\"precision: \"+str(precision))\n",
    "print(\"recall: \"+str(recall))\n",
    "print(\"f1: \"+str(f1))\n",
    "print(\"logloss: \"+str(logloss))\n",
    "\n",
    "# Storing comparison metrics in dataframe\n",
    "results.loc['logistic_regression', 'accuracy'] = accuracy\n",
    "results.loc['logistic_regression', 'precision'] = precision\n",
    "results.loc['logistic_regression', 'recall'] = recall\n",
    "results.loc['logistic_regression', 'f1_score'] = f1\n",
    "results.loc['logistic_regression', 'logloss'] = logloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Naive Bayes model on the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 4 0 0 0 3]\n",
      " [0 5 0 0 0 0]\n",
      " [0 0 8 0 0 1]\n",
      " [0 0 0 2 0 0]\n",
      " [0 0 0 0 2 0]\n",
      " [0 0 2 1 4 0]]\n",
      "accuracy: 0.5714285714285714\n",
      "precision: 0.6279365079365079\n",
      "recall: 0.5714285714285714\n",
      "f1: 0.5247360158638354\n",
      "logloss: 13.903797760599288\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, log_loss\n",
    "y_pred = classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "# Caclulating and displaying comparison metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "logloss = log_loss(y_true=y_test, y_pred=classifier.predict_proba(X_test))\n",
    "print(\"accuracy: \"+str(accuracy))\n",
    "print(\"precision: \"+str(precision))\n",
    "print(\"recall: \"+str(recall))\n",
    "print(\"f1: \"+str(f1))\n",
    "print(\"logloss: \"+str(logloss))\n",
    "\n",
    "# Storing comparison metrics in dataframe\n",
    "results.loc['naive_bayse', 'accuracy'] = accuracy\n",
    "results.loc['naive_bayse', 'precision'] = precision\n",
    "results.loc['naive_bayse', 'recall'] = recall\n",
    "results.loc['naive_bayse', 'f1_score'] = f1\n",
    "results.loc['naive_bayse', 'logloss'] = logloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Random Forest Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Random Forest Classification model on the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators = 100, criterion = 'gini', random_state = 0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10  0  0  0  0  0]\n",
      " [ 0  5  0  0  0  0]\n",
      " [ 0  0  8  0  0  1]\n",
      " [ 0  0  0  2  0  0]\n",
      " [ 0  0  0  0  1  1]\n",
      " [ 0  0  0  1  0  6]]\n",
      "accuracy: 0.9142857142857143\n",
      "precision: 0.9309523809523809\n",
      "recall: 0.9142857142857143\n",
      "f1: 0.9143977591036414\n",
      "logloss: 0.3103035665234523\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, log_loss\n",
    "y_pred = classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "# Caclulating and displaying comparison metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "logloss = log_loss(y_true=y_test, y_pred=classifier.predict_proba(X_test))\n",
    "print(\"accuracy: \"+str(accuracy))\n",
    "print(\"precision: \"+str(precision))\n",
    "print(\"recall: \"+str(recall))\n",
    "print(\"f1: \"+str(f1))\n",
    "print(\"logloss: \"+str(logloss))\n",
    "\n",
    "# Storing comparison metrics in dataframe\n",
    "results.loc['random_forest', 'accuracy'] = accuracy\n",
    "results.loc['random_forest', 'precision'] = precision\n",
    "results.loc['random_forest', 'recall'] = recall\n",
    "results.loc['random_forest', 'f1_score'] = f1\n",
    "results.loc['random_forest', 'logloss'] = logloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the SVM model on the Training set"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'linear', random_state = 0, probability=True)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the Confusion Matrix"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, log_loss\n",
    "y_pred = classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "# Caclulating and displaying comparison metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "logloss = log_loss(y_true=y_test, y_pred=classifier.predict_proba(X_test))\n",
    "print(\"accuracy: \"+str(accuracy))\n",
    "print(\"precision: \"+str(precision))\n",
    "print(\"recall: \"+str(recall))\n",
    "print(\"f1: \"+str(f1))\n",
    "print(\"logloss: \"+str(logloss))\n",
    "\n",
    "# Storing comparison metrics in dataframe\n",
    "results.loc['svm', 'accuracy'] = accuracy\n",
    "results.loc['svm', 'precision'] = precision\n",
    "results.loc['svm', 'recall'] = recall\n",
    "results.loc['svm', 'f1_score'] = f1\n",
    "results.loc['svm', 'logloss'] = logloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Extremely Randomized Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Extremely Randomized Tree model on the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
       "                     criterion='gini', max_depth=None, max_features='auto',\n",
       "                     max_leaf_nodes=None, max_samples=None,\n",
       "                     min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                     min_samples_leaf=1, min_samples_split=2,\n",
       "                     min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                     n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
       "                     warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "classifier = ExtraTreesClassifier(n_estimators = 100, criterion = 'gini', random_state = 0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10  0  0  0  0  0]\n",
      " [ 0  5  0  0  0  0]\n",
      " [ 0  0  8  0  0  1]\n",
      " [ 0  0  0  2  0  0]\n",
      " [ 0  0  0  0  2  0]\n",
      " [ 0  0  0  1  0  6]]\n",
      "accuracy: 0.9428571428571428\n",
      "precision: 0.9523809523809522\n",
      "recall: 0.9428571428571428\n",
      "f1: 0.944873949579832\n",
      "logloss: 1.0546157772314815\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, log_loss\n",
    "y_pred = classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "# Caclulating and displaying comparison metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "logloss = log_loss(y_true=y_test, y_pred=classifier.predict_proba(X_test))\n",
    "print(\"accuracy: \"+str(accuracy))\n",
    "print(\"precision: \"+str(precision))\n",
    "print(\"recall: \"+str(recall))\n",
    "print(\"f1: \"+str(f1))\n",
    "print(\"logloss: \"+str(logloss))\n",
    "\n",
    "# Storing comparison metrics in dataframe\n",
    "results.loc['extremely_random_trees', 'accuracy'] = accuracy\n",
    "results.loc['extremely_random_trees', 'precision'] = precision\n",
    "results.loc['extremely_random_trees', 'recall'] = recall\n",
    "results.loc['extremely_random_trees', 'f1_score'] = f1\n",
    "results.loc['extremely_random_trees', 'logloss'] = logloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Bagging Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Bagging Classifier model on the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=None, bootstrap=True, bootstrap_features=False,\n",
       "                  max_features=1.0, max_samples=1.0, n_estimators=100,\n",
       "                  n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
       "                  warm_start=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "classifier = BaggingClassifier(n_estimators = 100, random_state = 0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10  0  0  0  0  0]\n",
      " [ 0  5  0  0  0  0]\n",
      " [ 0  0  8  0  0  1]\n",
      " [ 0  0  0  2  0  0]\n",
      " [ 0  0  0  0  2  0]\n",
      " [ 0  0  0  1  0  6]]\n",
      "accuracy: 0.9428571428571428\n",
      "precision: 0.9523809523809522\n",
      "recall: 0.9428571428571428\n",
      "f1: 0.944873949579832\n",
      "logloss: 0.19613896262455327\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, log_loss\n",
    "y_pred = classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "# Caclulating and displaying comparison metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "logloss = log_loss(y_true=y_test, y_pred=classifier.predict_proba(X_test))\n",
    "print(\"accuracy: \"+str(accuracy))\n",
    "print(\"precision: \"+str(precision))\n",
    "print(\"recall: \"+str(recall))\n",
    "print(\"f1: \"+str(f1))\n",
    "print(\"logloss: \"+str(logloss))\n",
    "\n",
    "# Storing comparison metrics in dataframe\n",
    "results.loc['bagging', 'accuracy'] = accuracy\n",
    "results.loc['bagging', 'precision'] = precision\n",
    "results.loc['bagging', 'recall'] = recall\n",
    "results.loc['bagging', 'f1_score'] = f1\n",
    "results.loc['bagging', 'logloss'] = logloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the AdaBoost Classifier model on the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                   n_estimators=100, random_state=0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "classifier = AdaBoostClassifier(n_estimators = 100, random_state = 0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7 0 0 0 0 3]\n",
      " [0 4 0 0 0 1]\n",
      " [0 0 0 0 0 9]\n",
      " [0 0 0 2 0 0]\n",
      " [0 0 0 0 0 2]\n",
      " [0 0 0 1 0 6]]\n",
      "accuracy: 0.5428571428571428\n",
      "precision: 0.5238095238095237\n",
      "recall: 0.5428571428571428\n",
      "f1: 0.4937068160597572\n",
      "logloss: 1.1455907793657576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/connor/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, log_loss\n",
    "y_pred = classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "# Caclulating and displaying comparison metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "logloss = log_loss(y_true=y_test, y_pred=classifier.predict_proba(X_test))\n",
    "print(\"accuracy: \"+str(accuracy))\n",
    "print(\"precision: \"+str(precision))\n",
    "print(\"recall: \"+str(recall))\n",
    "print(\"f1: \"+str(f1))\n",
    "print(\"logloss: \"+str(logloss))\n",
    "\n",
    "# Storing comparison metrics in dataframe\n",
    "results.loc['adaboost', 'accuracy'] = accuracy\n",
    "results.loc['adaboost', 'precision'] = precision\n",
    "results.loc['adaboost', 'recall'] = recall\n",
    "results.loc['adaboost', 'f1_score'] = f1\n",
    "results.loc['adaboost', 'logloss'] = logloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Gradient Boosting Classifier model on the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
       "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "                           max_features=None, max_leaf_nodes=None,\n",
       "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                           min_samples_leaf=1, min_samples_split=2,\n",
       "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                           n_iter_no_change=None, presort='deprecated',\n",
       "                           random_state=0, subsample=1.0, tol=0.0001,\n",
       "                           validation_fraction=0.1, verbose=0,\n",
       "                           warm_start=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "classifier = GradientBoostingClassifier(n_estimators = 100, loss='deviance', criterion='friedman_mse', random_state = 0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10  0  0  0  0  0]\n",
      " [ 0  5  0  0  0  0]\n",
      " [ 0  0  8  0  0  1]\n",
      " [ 0  0  0  2  0  0]\n",
      " [ 0  0  0  0  2  0]\n",
      " [ 0  0  0  1  0  6]]\n",
      "accuracy: 0.9428571428571428\n",
      "precision: 0.9523809523809522\n",
      "recall: 0.9428571428571428\n",
      "f1: 0.944873949579832\n",
      "logloss: 0.21361263465331412\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, log_loss\n",
    "y_pred = classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "# Caclulating and displaying comparison metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "logloss = log_loss(y_true=y_test, y_pred=classifier.predict_proba(X_test))\n",
    "print(\"accuracy: \"+str(accuracy))\n",
    "print(\"precision: \"+str(precision))\n",
    "print(\"recall: \"+str(recall))\n",
    "print(\"f1: \"+str(f1))\n",
    "print(\"logloss: \"+str(logloss))\n",
    "\n",
    "# Storing comparison metrics in dataframe\n",
    "results.loc['gradientboost', 'accuracy'] = accuracy\n",
    "results.loc['gradientboost', 'precision'] = precision\n",
    "results.loc['gradientboost', 'recall'] = recall\n",
    "results.loc['gradientboost', 'f1_score'] = f1\n",
    "results.loc['gradientboost', 'logloss'] = logloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Summary of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>logloss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>decision_tree</th>\n",
       "      <td>0.942857</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.942857</td>\n",
       "      <td>0.944874</td>\n",
       "      <td>1.97364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>0.914286</td>\n",
       "      <td>0.926407</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>0.914252</td>\n",
       "      <td>1.23749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kernel_svm</th>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.821818</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.810113</td>\n",
       "      <td>0.534728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logistic_regression</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.770216</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.776806</td>\n",
       "      <td>0.490587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>naive_bayse</th>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.627937</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.524736</td>\n",
       "      <td>13.9038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random_forest</th>\n",
       "      <td>0.914286</td>\n",
       "      <td>0.930952</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>0.914398</td>\n",
       "      <td>0.310304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>extremely_random_trees</th>\n",
       "      <td>0.942857</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.942857</td>\n",
       "      <td>0.944874</td>\n",
       "      <td>1.05462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bagging</th>\n",
       "      <td>0.942857</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.942857</td>\n",
       "      <td>0.944874</td>\n",
       "      <td>0.196139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adaboost</th>\n",
       "      <td>0.542857</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.542857</td>\n",
       "      <td>0.493707</td>\n",
       "      <td>1.14559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gradientboost</th>\n",
       "      <td>0.942857</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.942857</td>\n",
       "      <td>0.944874</td>\n",
       "      <td>0.213613</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        accuracy precision    recall  f1_score   logloss\n",
       "decision_tree           0.942857  0.952381  0.942857  0.944874   1.97364\n",
       "knn                     0.914286  0.926407  0.914286  0.914252   1.23749\n",
       "kernel_svm              0.828571  0.821818  0.828571  0.810113  0.534728\n",
       "logistic_regression          0.8  0.770216       0.8  0.776806  0.490587\n",
       "naive_bayse             0.571429  0.627937  0.571429  0.524736   13.9038\n",
       "random_forest           0.914286  0.930952  0.914286  0.914398  0.310304\n",
       "svm                          NaN       NaN       NaN       NaN       NaN\n",
       "extremely_random_trees  0.942857  0.952381  0.942857  0.944874   1.05462\n",
       "bagging                 0.942857  0.952381  0.942857  0.944874  0.196139\n",
       "adaboost                0.542857   0.52381  0.542857  0.493707   1.14559\n",
       "gradientboost           0.942857  0.952381  0.942857  0.944874  0.213613"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss\t     bagging\n",
      "accuracy     decision_tree\n",
      "precision    decision_tree\n",
      "recall       decision_tree\n",
      "f1_score     decision_tree\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "results = results.astype(float)\n",
    "print('logloss\\t     '+results.iloc[:,-1].idxmin())\n",
    "print(results.iloc[:,:-1].idxmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results.to_csv('vbgm_supervised_comparisons.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Average 15: Best: bagging: accuracy=0.914286, logloss=0.268255\n",
    "* Complete 16: Best: random_forest: accuracy=0.942857, logloss=0.37855\n",
    "* K-means 15: Best: extremely_random_trees: accuracy=0.914286, logloss=1.22825\n",
    "* Single 20: Best: bagging: accuracy=0.942857, logloss=0.196139\n",
    "* Single 25: Best: gradientboost: accuracy=0.942857, logloss=0.197863\n",
    "* VBGM 19: Best: gradientboost: accuracy=0.942857, logloss=0.458908\n",
    "* Ward: Best: bagging: accuracy=0.914286, logloss=0.351778"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "decision_tree_classification.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
