{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vfhOoCOpbmdy"
   },
   "source": [
    "***\n",
    "<h1 style=\"text-align:center;\"><img src=\"images/udl_logo.jpg\" alt=\"UDL Logo\" style=\"height:50px; vertical-align:middle;\"/>    UBC Master of Data Science 2020 - UDL Capstone Project    <img src=\"images/ubc_logo.png\" alt=\"UBC Logo\" style=\"height:50px; vertical-align:middle;\"/></h1>\n",
    "\n",
    "***\n",
    "The purpose of this document is to provide descriptions of what each code does for the primary steps of the UBC Master of Data Science 2020 - Urban Data Lab (UDL) Capstone Project (the Project). The goal of the Project was to develop code to predict the end-uses of the Energy Consumption (EC) sensors within the buildings on the UBC Vancouver campus.\n",
    "\n",
    "This document contains the following sections:\n",
    "* Step 0 - Library Imports and Initializations\n",
    "* Step 1 - Cluster NC data\n",
    "* Step 2 - Model EC/NC relationship\n",
    "* Step 3 - Prep EC data for classification model\n",
    "* Step 4 - Classification model for EC data\n",
    "* Step 5 - Write results to InfluxDB\n",
    "\n",
    "\n",
    "The following flowchart shows how the data flows through the code\n",
    "\n",
    "![Project Concept Flowchart](images/project_concept_flowchart.png)\n",
    "\n",
    "This file was developed specifically for the Pharmacy building, however it was designed with scalability in mind such that it could be applied to other buildings. The file was also developed to query data from the `UBC_EWS` measurement. However, functionality has been included such that the measurement from which to query data can be user-defined. This is to facilitate applying this model to new measurments in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7jsBfE-Hwg0N"
   },
   "source": [
    "***\n",
    "## Step 0 - Library Imports and Initializations\n",
    "The purpose of Step 0 is to import the required libraries and initialize the constants that will be used throughout the document.\n",
    "### Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZaQ2Fk_6v6MG"
   },
   "outputs": [],
   "source": [
    "### ~ Library Imports ~ ###\n",
    "# General Imports\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# Data Formatting and Manipulation Imports\n",
    "import pandas as pd\n",
    "# Clustering Step Imports\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "# Regression Step Imports\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# Supervised Classification Step Imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, log_loss\n",
    "from sklearn.ensemble import BaggingClassifier # Selected for Pharmacy (others listed below are alternatives integrated in the last step to test which is optimal for the current data)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# InfluxDB import\n",
    "import influxdb\n",
    "# Modules Developed for this Project Imports\n",
    "import data_preparation\n",
    "import aggregation\n",
    "import clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cAEEC6Bwwjw1"
   },
   "source": [
    "### Constant Definitions\n",
    "The code employs several user-defined constants that can change how the code behaves. These constants are all declared in the following code chunk for the users convenience.\n",
    "\n",
    "The various constants and their purposes are listed below:\n",
    "* Constants associated with data collection:\n",
    "  * `QUERY_SITE`: The site from which to query sensor data\n",
    "  * `MEASUREMENT`: The InfluxDB measurement from which to query\n",
    "  * `METADATA_CSV_PATH`: The path to the metadata csv\n",
    "  * `TRAINING_SET_PATH`: The path to the training data\n",
    "  * `SENSOR_ID_TAGS`: The numerical index(s) for which column(s) make up the uniqe sensor ids\n",
    "  * `QUERY_FROM_DB`: A boolean defining whether to query form the database or pull data from previously queried csv files\n",
    "  * `QUERY_CSV_PATH` and `QUERY_WEATHER_CSV_PATH`: The paths to the csv files from which to read the sensor data (if pulling data from csv files)\n",
    "  * `START_DATE` and `END_DATE`: The start and end dates of the date range to query (if querying from database)\n",
    "* Constants associated with saving outputs:\n",
    "  * `SAVE_STEP_OUTPUTS`: A boolean defining whether or not to save the outputs for each step\n",
    "  * `STEP1_SAVE_PATH`: The path to save the outputs from Step 1 (if saving step outputs)\n",
    "  * `STEP2_SAVE_PATH_FINAL_DF` and `STEP2_SAVE_PATH_EC_DATA2`: The paths to save the outputs from Step 2 (if saving step outputs)\n",
    "  * `STEP3_SAVE_PATH`: The path to save the outputs from Step 3 each step (if saving step outputs)\n",
    "  * `PREDICTED_SAVE_PATH`: The path to save the final end-use labels\n",
    "* Constant associated with displaying model performance/outputs:\n",
    "  * `DISPLAY_PREDICTION_METRICS`: A boolean defining whether or not to display the prediction metrics of the current model\n",
    "  * `PLOT_CLUSTERS`: A boolean defining whether or not to display the plotted clusters\n",
    "* Constants associated with writing the predicted end-uses to InfluxDB:\n",
    "  * `MODEL_OUTPUT_FILE_LOCATION`: The path to the file containing data to write to InfluxDB\n",
    "  * `HOST_ADDRESS`: The IP address for where the InfluxDB is hosted\n",
    "  * `HOST_PORT`: The port for where the InfluxDB is hosted\n",
    "  * `DATABASE_NAME`: The name of the database to write to\n",
    "  * `EU_MEASUREMENT`: The name of the measurement to write to\n",
    "\n",
    "The easiest way to change any of the items listed above is to update them in the following code chunk. This includes running the code for different buildings or pulling data from different measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9nFRXbO0v8OF"
   },
   "outputs": [],
   "source": [
    "##### ~~~ Constants Related to Data Collection ~~~ #####\n",
    "# String defining which site to run the model for\n",
    "QUERY_SITE = 'Pharmacy'\n",
    "# String defining which measurement from the database to query\n",
    "MEASUREMENT='UBC_EWS'\n",
    "# String defining the path to the metadata csv for the given building\n",
    "METADATA_CSV_PATH = '../data/PharmacyQuery.csv'\n",
    "# String defining the path to the training dataset\n",
    "TRAINING_SET_PATH = '../data/pharmacy_train_test_data.csv'\n",
    "# List of indices that can be combined to uniquely identify a sensor (used to group on each sensors)\n",
    "SENSOR_ID_TAGS = [1,2,3,4,5,6] # order is [\"groupRef\",\"equipRef\",\"navName\",\"siteRef\",\"typeRef\",\"unit\"]\n",
    "                                # The planned update to the InfluxDB may change SENSOR_ID_TAGS to only [1] as in [\"uniqueID\"]\n",
    "# Boolean defining if the model should query from the database or pull from csv's (from database if True, else False)\n",
    "QUERY_FROM_DB = False\n",
    "# Strings defining the start and end date fo the date range to query (if QUERY_FROM_DB==True)\n",
    "START_DATE = '2020-03-16'\n",
    "END_DATE = '2020-03-17'\n",
    "# Strings containing the paths to the folders that contains the csv's to pull data from if QUERY_FROM_DB==False\n",
    "# All file names within the folders must be formatted as \"YYYY-MM-DD.csv\"\n",
    "QUERY_CSV_PATH = '../data/sensor_data/'\n",
    "QUERY_WEATHER_CSV_PATH = '../data/weather_data/'\n",
    "\n",
    "##### ~~~ Constants Related to Data Collection ~~~ #####\n",
    "# Boolean defining if the output dataframes from each step should be saved (save if True, else False)\n",
    "SAVE_STEP_OUTPUTS = True\n",
    "# Strings defining the location and file name for saving the Step 1, 2, and 3 output dataframes as csv\n",
    "STEP1_SAVE_PATH = '../data/csv_outputs/step1_clustering_phase_output.csv'\n",
    "STEP2_SAVE_PATH_FINAL_DF = '../data/csv_outputs/step2_ridge_regression_output.csv'\n",
    "STEP2_SAVE_PATH_EC_DATA2 = '../data/csv_outputs/step2_aggregated_ec_data_output.csv'\n",
    "STEP3_SAVE_PATH = '../data/csv_outputs/step3_data_prep_for_classification_output.csv'\n",
    "# String defining the location and file name for saving the predicted end uses dataframe as csv\n",
    "PREDICTED_SAVE_PATH = '../data/csv_outputs/predicted_end_use_labels.csv'\n",
    "\n",
    "##### ~~~ Constants Related to Data Collection ~~~ #####\n",
    "# Boolean defining if the prediction metrics should be shown (show if True, else False)\n",
    "DISPLAY_PREDICTION_METRICS = True\n",
    "# Boolean defining if the clusters should be plotted\n",
    "PLOT_CLUSTERS = False # Default is `False` because the process is slow and isn't required for base functionality\n",
    "\n",
    "##### ~~~ Constants Related to Data Collection ~~~ #####\n",
    "# The path to the data to write to InfluxDB\n",
    "MODEL_OUTPUT_FILE_LOCATION = PREDICTED_SAVE_PATH # Default is the file being saved by the program\n",
    "# The IP address for where the InfluxDB is hosted\n",
    "HOST_ADDRESS = '206.12.92.81'\n",
    "# The port for where the InfluxDB is hosted\n",
    "HOST_PORT = '8086'\n",
    "# The name of the database to write to\n",
    "DATABASE_NAME = 'SKYSPARK'\n",
    "# The name of the measurement to write to\n",
    "EU_MEASUREMENT = 'END_USE'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "egTFsf0RDdDS"
   },
   "source": [
    "### Generating DATELIST and client\n",
    "In this section the following are generated:\n",
    "* The `DATELIST` for the desired date range to collect data from corresponding with either:\n",
    "  * The `START_DATE` and `END_DATE` defined above (if `QUERY_FROM_DB`==True)\n",
    "  * The dates of the csv files available in `QUERY_CSV_PATH` (if `QUERY_FROM_DB`==False)\n",
    "* The `client` of the InfluxDB to query from (if `QUERY_FROM_DB`==True)\n",
    "\n",
    "If there is a need to update which days are queried, for example randomly sampling days between a date range rather than collecting every date, updating how `DATELIST` is generated in this chunk would be the easiest method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AB9_PJb-Dbwk"
   },
   "outputs": [],
   "source": [
    "if QUERY_FROM_DB:\n",
    "    # Generating a list of the dates within the daterange to query (if QUERY_FROM_DB==True)\n",
    "    DELTA = (datetime.strptime(END_DATE, \"%Y-%m-%d\")-datetime.strptime(START_DATE, \"%Y-%m-%d\")).days\n",
    "    DATELIST =  [(datetime.strptime(END_DATE, \"%Y-%m-%d\") - timedelta(days=x)).strftime('%Y-%m-%d') for x in range(0,delta+1)]\n",
    "    DATELIST.sort(key=lambda date: datetime.strptime(date, \"%Y-%m-%d\"))\n",
    "else:\n",
    "    # Getting the list of files stored in the path provided in QUERY_CSV_PATH (if QUERY_FROM_DB==False)\n",
    "    # All files names must be formatted as \"YYYY-MM-DD.csv\"\n",
    "    DATELIST = [f.split(\".\")[0] for f in listdir(QUERY_CSV_PATH) if isfile(join(QUERY_CSV_PATH, f))]\n",
    "\n",
    "# Connecting to influxDB if QUERY_FROM_DB==True\n",
    "if QUERY_FROM_DB:\n",
    "    client = data_preparation.connect_to_db()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EDMHVJmvbsr9"
   },
   "source": [
    "***\n",
    "## Step 1 - Cluster NC data\n",
    "The purpose of step 1 is to cluster the non-energy consumption (NC) sensors into groups of sensors that have similar responses, and then aggregate the sensor values for each cluster for each hour of each day in the desired date range. The following aggregations were calculated on the values measured by the sensors within each cluster:\n",
    "* Mean\n",
    "* Minimum\n",
    "* Maximum\n",
    "* Standard Deviation\n",
    "* Average Sensor Update Rate\n",
    "\n",
    "The activities required to attain the goal of Step 1 are the following:\n",
    "1. For each day in the desired date range:\n",
    "\n",
    "    1. Collect the non-energy consumption sensor data for the desired building and store in `temp_df`\n",
    "\n",
    "    2. Aggregate the data for each non-energy consumption sensor to get the previously listed aggregations over the entire time range (meaning only 5 measurements per sensor) and store in `temp_df`\n",
    "\n",
    "    3. Combine the current day's aggregations with the aggregations of all the previous days using weighted averages and store in `nc_data`\n",
    "2. Encode the unit of measure and scale the aggregated values for the sensors and store outputs in `nc_data`\n",
    "3. Calculate the Gower's distance between sensors and store the results in `gow_dist`\n",
    "4. Cluster the sensors using the Gower's distance and store the results in `clusters`\n",
    "5. For each day in the desired date range:\n",
    "    1. Collect the non-energy consumption sensor data for the desired building and store the results in `temp_df`\n",
    "    2. Assign each sensor its appropriate cluster number as identified in activity 4 by joining the clusters to `temp_df`\n",
    "    3. Aggregate the data for each cluster by hour of the day and date to get the previously listed aggregations (so 1 row per hour per day, and 5 columns per cluster) and store the results in `temp_df_aggs`\n",
    "    4. Combine the current day's aggregations with the aggregations of all the previous days using weighted averages and store the results in `nc_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "csE_cCjyZDkO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### ~~~~~ Started - Step 1: Clustering Phase ~~~~~ #######\n",
      "\t##### ~~~ Started - Step 1 a): Aggregation Phase 1 ~~~ #####\n",
      "\t\t1: 2020-03-16\n",
      "\t\t2: 2020-05-01\n",
      "\t\t### ~ Started - Step 1 a): Agg Phase 1: Calculating Update Rates ~ ###\n",
      "\t##### ~~~ Step 1 b): Started - Clustering Phase ~~~ #####\n",
      "\t\t### ~ Started - Step 1 c): Clust Phase 1: Calculating Gower's Distance ~ ###\n",
      "\t\t### ~ Started - Step 1 c): Clust Phase 2: Calculating Clusters ~ ###\n",
      "\t##### ~~~ Started - Step 1 d): Aggregation Phase 2 ~~~ #####\n",
      "\t\t1: 2020-03-16\n",
      "\t\t2: 2020-05-01\n",
      "\t\t### ~ Started - Step 1 d): Agg Phase 2: Calculating Update Rates ~ ###\n",
      "####### ~~~~~ Complete - Step 1: NC Aggregation and Clustering Phase ~~~~~ #######\n"
     ]
    }
   ],
   "source": [
    "print(\"####### ~~~~~ Started - Step 1: Clustering Phase ~~~~~ #######\") # For tracking program progress\n",
    "###   a) load+aggregate NC data including weather, grouping by sensor ID fields\n",
    "print(\"\\t##### ~~~ Started - Step 1 a): Aggregation Phase 1 ~~~ #####\") # For tracking program progress\n",
    "last_idx_as_cols = False\n",
    "is_first_iter = True\n",
    "cnt=1\n",
    "for day in DATELIST:\n",
    "    print(\"\\t\\t\"+str(cnt)+\": \"+str(day)) # For tracking program progress\n",
    "    # Querying and preping data for aggregations\n",
    "    if QUERY_FROM_DB:\n",
    "        temp_df = data_preparation.query_db_nc(client, day, measurement=MEASUREMENT, num_days=1, site=QUERY_SITE)\n",
    "        if temp_df is not None:\n",
    "            # Making the datetime index into a column so that date and hour can be extracted later\n",
    "            temp_df.reset_index(level=0, inplace=True)\n",
    "    else:\n",
    "        temp_df = data_preparation.query_csv(client=QUERY_CSV_PATH, date=day, site=None)\n",
    "        weather_df = data_preparation.query_weather_csv(client=QUERY_WEATHER_CSV_PATH, date=day, site=None)\n",
    "        if weather_df is None:\n",
    "            pass\n",
    "        else:\n",
    "            temp_df = pd.concat([temp_df, weather_df])\n",
    "            temp_df = temp_df.fillna('empty') # Aggregation doesn't work with nan's, using empty as an obvious flag for value being nan\n",
    "    if temp_df is None:\n",
    "        continue\n",
    "    # Formatting the dataframe columns for date, month, and hour extraction\n",
    "    col_names = ['datetime']\n",
    "    col_names.extend(temp_df.columns[1:])\n",
    "    temp_df.columns = col_names\n",
    "    # Getting date, month, and hour\n",
    "    temp_df = aggregation.split_datetime(temp_df)\n",
    "    if is_first_iter:\n",
    "        # Creating a low memory dataframe for the append_agg function before the structure is changed by agg_all\n",
    "        struct_df = temp_df.head(1)\n",
    "        # Aggregating the first date's data\n",
    "        nc_data = aggregation.agg_all(temp_df, how=\"all\", col_idx=SENSOR_ID_TAGS, last_idx_to_col=last_idx_as_cols)\n",
    "        is_first_iter = False\n",
    "    else:\n",
    "        # Aggregating the current date's data and aggregate it with the current running total\n",
    "        temp_df = aggregation.agg_all(temp_df, how=\"all\", col_idx=SENSOR_ID_TAGS, last_idx_to_col=last_idx_as_cols)\n",
    "        nc_data = aggregation.append_agg(df1=temp_df, df2=nc_data, struct_df=struct_df, col_idx=SENSOR_ID_TAGS, last_idx_to_col=last_idx_as_cols)\n",
    "    cnt += 1\n",
    "    \n",
    "print(\"\\t\\t### ~ Started - Step 1 a): Agg Phase 1: Calculating Update Rates ~ ###\") # For tracking program progress\n",
    "# Freeing up some memory\n",
    "temp_df = None\n",
    "weather_df = None\n",
    "struct_df = None\n",
    "# Calculating the update rate\n",
    "nc_data[\"update_rate\"] = nc_data[\"count\"] / cnt\n",
    "nc_data.drop(\"count\", inplace=True, axis=1)\n",
    "\n",
    "###   b) Encode and scale NC data\n",
    "print(\"\\t##### ~~~ Step 1 b): Started - Clustering Phase ~~~ #####\") # For tracking program progress\n",
    "\n",
    "# Getting Indexes of the continuous columns\n",
    "cont_cols = [i for i in range(len(SENSOR_ID_TAGS),len(nc_data.columns))]\n",
    "\n",
    "# Scale Continuous\n",
    "scaled_data = data_preparation.scale_continuous(nc_data, cont_cols)\n",
    "nc_data = pd.concat([nc_data.iloc[:, 0:len(SENSOR_ID_TAGS)], pd.DataFrame(scaled_data, index=nc_data.index, columns=nc_data.columns[cont_cols].tolist())], axis=1)\n",
    "scaled_data = None\n",
    "\n",
    "# Encoding units\n",
    "nc_data = data_preparation.encode_units(nc_data)\n",
    "\n",
    "###   c) cluster NC data to get df of sensor id fields + cluster group number\n",
    "# Calculating Gower's Distance, and clusters\n",
    "print(\"\\t\\t### ~ Started - Step 1 c): Clust Phase 1: Calculating Gower's Distance ~ ###\") # For tracking program progress\n",
    "gow_dist = clustering.calc_gowers(nc_data, cont_cols)\n",
    "print(\"\\t\\t### ~ Started - Step 1 c): Clust Phase 2: Calculating Clusters ~ ###\") # For tracking program progress\n",
    "clusters = AgglomerativeClustering(linkage = 'single', affinity='precomputed', n_clusters=20).fit_predict(gow_dist)\n",
    "\n",
    "# Generating a list of the columns to keep when making the dataframe relating sensors to clusters(the unique identifiers for an NC sensor and cluster)\n",
    "unique_cols_idx = [i for i in range(len(SENSOR_ID_TAGS))]\n",
    "unique_cols = nc_data.columns[unique_cols_idx].values.tolist()\n",
    "unique_cols.append(\"cluster\")\n",
    "# Creating a dataframe that identifies which unique sensors belong to which cluster\n",
    "drop_cols = list(set(nc_data.columns.tolist())-set(unique_cols))\n",
    "cluster_groups = pd.concat([nc_data, pd.DataFrame(clusters, columns=[\"cluster\"])], axis=1)\n",
    "cluster_groups = cluster_groups.drop(drop_cols, axis=1)\n",
    "\n",
    "###   d) Reload NC data + join cluster group num + aggregate, this time grouping by date, time, and clust_group_num\n",
    "print(\"\\t##### ~~~ Started - Step 1 d): Aggregation Phase 2 ~~~ #####\") # For tracking program progress\n",
    "last_idx_as_cols = True\n",
    "is_first_iter = True\n",
    "cnt=1\n",
    "for day in DATELIST:\n",
    "    print(\"\\t\\t\"+str(cnt)+\": \"+str(day)) # For tracking program progress\n",
    "    # Querying and preping data for aggregations\n",
    "    if QUERY_FROM_DB:\n",
    "        temp_df = data_preparation.query_db_nc(client, day, measurement=MEASUREMENT, num_days=1, site=QUERY_SITE)\n",
    "        if temp_df is not None:\n",
    "            # Making the datetime index into a column so that date and hour can be extracted later\n",
    "            temp_df.reset_index(level=0, inplace=True)\n",
    "    else:\n",
    "        temp_df = data_preparation.query_csv(client=QUERY_CSV_PATH, date=day, site=None)\n",
    "        weather_df = data_preparation.query_weather_csv(client=QUERY_WEATHER_CSV_PATH, date=day, site=None)\n",
    "        if weather_df is None:\n",
    "            pass\n",
    "        else:\n",
    "            temp_df = pd.concat([temp_df, weather_df])\n",
    "            temp_df = temp_df.fillna('empty') # Aggregation doesn't work with nan's, using empty as an obvious flag for value being nan\n",
    "    if temp_df is None:\n",
    "        continue\n",
    "      # Formatting the dataframe columns for date, month, and hour extraction\n",
    "    col_names = ['datetime']\n",
    "    col_names.extend(temp_df.columns[1:])\n",
    "    temp_df.columns = col_names\n",
    "    # Getting date, month, and hour (must have at least hour for aggregations)\n",
    "    temp_df = aggregation.split_datetime(temp_df)\n",
    "    # Adding cluster groupings to the data for aggregation purposes\n",
    "    temp_df = temp_df.merge(cluster_groups, how='left', on=cluster_groups.columns[:-1].tolist())\n",
    "    if is_first_iter:\n",
    "        # Calculating the count of sensor updates per hour per day per cluster for the first date\n",
    "        update_rates = temp_df.groupby(['hour','date','cluster']).agg({'value':'count'},axis=1)\n",
    "        # Creating a low memory dataframe for the append_agg function before the structure is changed by agg_all (different structure required than previous)\n",
    "        struct_df = temp_df.head(1)\n",
    "        # Identifying the indexes of the items being aggregated on (hour, date, and cluster)\n",
    "        cluster_id_tags = [temp_df.columns.tolist().index(\"hour\"), temp_df.columns.tolist().index(\"date\"), temp_df.columns.tolist().index(\"cluster\")]\n",
    "        # Aggregating the first date's data (Aggregations must be seperate b/c data gets too big during calculations otherwise)\n",
    "        temp_df_aggs = aggregation.agg_all(temp_df, how=\"mean\", col_idx=cluster_id_tags, last_idx_to_col=last_idx_as_cols)\n",
    "        if 'count_x' in temp_df_aggs.columns.tolist():\n",
    "            # Each aggregation type outputs a count, ensuring joins only result in 1 count (applies for all similar if statements)\n",
    "            temp_df_aggs = temp_df_aggs.drop('count_y', axis=1)\n",
    "            temp_df_aggs = temp_df_aggs.rename(columns={'count_x':'count'})\n",
    "        temp_df_aggs = temp_df_aggs.merge(aggregation.agg_all(temp_df, how=\"std\", col_idx=cluster_id_tags, last_idx_to_col=last_idx_as_cols), how='left', on=temp_df_aggs.columns.tolist()[:2])\n",
    "        if 'count_x' in temp_df_aggs.columns.tolist():\n",
    "            temp_df_aggs = temp_df_aggs.drop('count_y', axis=1)\n",
    "            temp_df_aggs = temp_df_aggs.rename(columns={'count_x':'count'})\n",
    "        temp_df_aggs = temp_df_aggs.merge(aggregation.agg_all(temp_df, how=\"max\", col_idx=cluster_id_tags, last_idx_to_col=last_idx_as_cols), how='left', on=temp_df_aggs.columns.tolist()[:2])\n",
    "        if 'count_x' in temp_df_aggs.columns.tolist():\n",
    "            temp_df_aggs = temp_df_aggs.drop('count_y', axis=1)\n",
    "            temp_df_aggs = temp_df_aggs.rename(columns={'count_x':'count'})\n",
    "        nc_data = temp_df_aggs.merge(aggregation.agg_all(temp_df, how=\"min\", col_idx=cluster_id_tags, last_idx_to_col=last_idx_as_cols), how='left', on=temp_df_aggs.columns.tolist()[:2])\n",
    "        if 'count_x' in nc_data.columns.tolist():\n",
    "            nc_data = nc_data.drop('count_y', axis=1)\n",
    "            nc_data = nc_data.rename(columns={'count_x':'count'})\n",
    "        is_first_iter = False\n",
    "    else:\n",
    "        # Calculating the count of sensor updates per hour per day per cluster for the current date\n",
    "        update_rates = pd.concat([update_rates, temp_df.groupby(['hour','date','cluster']).agg({'value':'count'},axis=1)])\n",
    "        # Aggregating the current date's data and aggregate it with the current running total\n",
    "        temp_df_aggs = aggregation.agg_all(temp_df, how=\"mean\", col_idx=cluster_id_tags, last_idx_to_col=last_idx_as_cols)\n",
    "        if 'count_x' in temp_df_aggs.columns.tolist():\n",
    "            temp_df_aggs = temp_df_aggs.drop('count_y', axis=1)\n",
    "            temp_df_aggs = temp_df_aggs.rename(columns={'count_x':'count'})\n",
    "        temp_df_aggs = temp_df_aggs.merge(aggregation.agg_all(temp_df, how=\"std\", col_idx=cluster_id_tags, last_idx_to_col=last_idx_as_cols), how='left', on=temp_df_aggs.columns.tolist()[:2])\n",
    "        if 'count_x' in temp_df_aggs.columns.tolist():\n",
    "            temp_df_aggs = temp_df_aggs.drop('count_y', axis=1)\n",
    "            temp_df_aggs = temp_df_aggs.rename(columns={'count_x':'count'})\n",
    "        temp_df_aggs = temp_df_aggs.merge(aggregation.agg_all(temp_df, how=\"max\", col_idx=cluster_id_tags, last_idx_to_col=last_idx_as_cols), how='left', on=temp_df_aggs.columns.tolist()[:2])\n",
    "        if 'count_x' in temp_df_aggs.columns.tolist():\n",
    "            temp_df_aggs = temp_df_aggs.drop('count_y', axis=1)\n",
    "            temp_df_aggs = temp_df_aggs.rename(columns={'count_x':'count'})\n",
    "        temp_df_aggs = temp_df_aggs.merge(aggregation.agg_all(temp_df, how=\"min\", col_idx=cluster_id_tags, last_idx_to_col=last_idx_as_cols), how='left', on=temp_df_aggs.columns.tolist()[:2])\n",
    "        if 'count_x' in temp_df_aggs.columns.tolist():\n",
    "            temp_df_aggs = temp_df_aggs.drop('count_y', axis=1)\n",
    "            temp_df_aggs = temp_df_aggs.rename(columns={'count_x':'count'})\n",
    "        nc_data = aggregation.append_agg(df1=temp_df_aggs, df2=nc_data, struct_df=struct_df, col_idx=cluster_id_tags, last_idx_to_col=last_idx_as_cols)\n",
    "    cnt += 1\n",
    "    \n",
    "print(\"\\t\\t### ~ Started - Step 1 d): Agg Phase 2: Calculating Update Rates ~ ###\") # For tracking program progress\n",
    "# Re-format update rates so that clusters are columns\n",
    "update_rates = update_rates.unstack()\n",
    "update_rates.columns = update_rates.columns.droplevel(level=0)\n",
    "update_rates = update_rates.fillna(0)\n",
    "\n",
    "# Calculate the number of sensors per cluster\n",
    "sensor_count_per_cluster = cluster_groups.groupby('cluster').agg({cluster_groups.columns.tolist()[0]:'count'})\n",
    "sensor_count_per_cluster.columns = ['count']\n",
    "\n",
    "# Calculate the average sensor update rate per hour per cluster\n",
    "for cluster in cluster_groups['cluster'].unique():\n",
    "    update_rates.loc[:,cluster] = update_rates.loc[:,cluster]/sensor_count_per_cluster.loc[cluster][0]\n",
    "\n",
    "# Rename the update rate columns and join them to the nc_data\n",
    "update_rates = update_rates.add_prefix('urate_')\n",
    "nc_data = nc_data.join(update_rates, how='left', on=['hour', 'date'])\n",
    "nc_data = nc_data.drop('count', axis=1)\n",
    "if SAVE_STEP_OUTPUTS:\n",
    "    nc_data.to_csv(STEP1_SAVE_PATH)\n",
    "\n",
    "# Freeing up some space\n",
    "temp_df_aggs = None\n",
    "struct_df = None\n",
    "temp_df = None\n",
    "update_rates = None\n",
    "\n",
    "print(\"####### ~~~~~ Complete - Step 1: NC Aggregation and Clustering Phase ~~~~~ #######\") # For tracking program progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5BIfp4Z4EFAQ"
   },
   "source": [
    "### Loading Step 1 Output from csv\n",
    "The following code chunk allows the user to load the Step 1 output from a csv of a previously saved output from Step 1. \n",
    "\n",
    "**This chunk does not need to be run if the full Step 1 code has already been run.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mzl9oI11EEsD"
   },
   "outputs": [],
   "source": [
    "nc_data = pd.read_csv(STEP1_SAVE_PATH, index_col=0)\n",
    "nc_data = nc_data.astype({\"date\": object, \"hour\": str})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3-scFXIzkMpQ"
   },
   "source": [
    "### Ploting Clusters\n",
    "The following code chunk is for plotting the cluster groups, the code is wrapped in an if-statement that is only executed if `PLOT_CLUSTERS` is `False`. The reason is that this chunk runs a multidimensional Scaling (MDS) calculation in order to plot the data. The if-statment prevents the user from accidentally running this chunk.\n",
    "\n",
    "**WARNING: The code chunk for plotting the clusters requires multidimensional scaling to be calculated on the Gower's distance. This calculation can take a long time, only run this code if you want to plot the clusters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bzC2JRB9kLCv"
   },
   "outputs": [],
   "source": [
    "if PLOT_CLUSTERS: \n",
    "    # MDS calculation\n",
    "    mds = clustering.multidim_scale(gow_dist, num_dim=2)\n",
    "\n",
    "    # Preping the dataframe for plotting\n",
    "    data_2d_df = pd.DataFrame(data=mds, columns = ['x','y'])\n",
    "    data_2d_df['cluster'] = cluster_groups['cluster']\n",
    "    mds = None\n",
    "  \n",
    "    # Plotting the clusters\n",
    "    plt.scatter(data_2d_df['x']*1000,data_2d_df['y']*1000, c=data_2d_df['cluster'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0f5x5-PEgYTC"
   },
   "source": [
    "***\n",
    "## Step 2 - Model EC/NC relationship\n",
    "The purpose of Step 2 is for feature engineering where the coefficients from the regression model are fed into the classification model. Each EC sensor's values (response variable) are fitted with NC cluster values (predictor variables) to represent part of the relationship between the two groups of sensors.\n",
    "\n",
    "Additionally, Step 2 creates a dataset named `ec_data2` that is used for Step 3, because it saves querying time by querying for EC data only once.\n",
    "\n",
    "The activities required to achieve Step 2 are the following:   \n",
    "1. Query EC data \n",
    "2. The data is stored in a temporary `temp_df2` dataframe. Next, split datetime to date and hour and two dataframes (`ec_data1`, `ec_data2`) are incrementally created from `temp_df2`\n",
    "3. Both `ec_data1` and `ec_data2` dataframes are aggregated where `ec_data1` is aggregated by date and hour and  `ec_data2` is aggregated by unique sensor ID\n",
    "4. Scale continuous data in `ec_data1` and Step 1's `nc_data` \n",
    "5. For each unique sensor ID\n",
    "  * Merge `ec_data1` with `nc_data` into a new dataframe \n",
    "  * Set `X` to `nc_data` values and `Y` to `ec_data1` mean value\n",
    "  * Use cross validation to find optimal alpha value. Feed `X` and `Y` arrays into ridge regression model with optimal alpha value as a parameter\n",
    "  * Store each unique sensor ID's model coeffcients into a list named `coefficients_list`\n",
    "5. Append model coefficients for every unique sensor ID into a single dataframe named `final_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l9lTG3vcgV3L"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### ~~~~~ Starting - Step 2: Model EC/NC Relationship ~~~~~ #######\n",
      "\t##### ~~~ Started - Step 2 a): Aggregation ~~~ #####\n",
      "\t\t1: 2020-03-16\n",
      "\t\t2: 2020-05-01\n",
      "\t##### ~~~ Started - Step 2 b): Regression ~~~ #####\n",
      "avg_mse: 0.00027713443090975797\n",
      "####### ~~~~~ Complete - Step 2: Model EC/NC Relationship ~~~~~ #######\n"
     ]
    }
   ],
   "source": [
    "# Freeing up some memory (done here b/c Gower's distance is required to calculate MDS for plotting)\n",
    "gow_dist = None\n",
    "\n",
    "print(\"####### ~~~~~ Starting - Step 2: Model EC/NC Relationship ~~~~~ #######\") # For tracking program progress\n",
    "###   a) Aggregate EC sensors to be response variables for the regression model\n",
    "print(\"\\t##### ~~~ Started - Step 2 a): Aggregation ~~~ #####\") # For tracking program progress\n",
    "last_idx_as_cols = False\n",
    "is_first_iter = True\n",
    "cnt=1\n",
    "for day in DATELIST:\n",
    "    print(\"\\t\\t\"+str(cnt)+\": \"+str(day)) # For tracking program progress\n",
    "    # Querying and preping data for aggregations\n",
    "    if QUERY_FROM_DB:\n",
    "        temp_df2 = data_preparation.query_db_ec(client, day, measurement=MEASUREMENT, num_days=1, site=QUERY_SITE)\n",
    "        if temp_df2 is not None:\n",
    "            # Making the datetime index into a column so that date and hour can be extracted later\n",
    "            temp_df2.reset_index(level=0, inplace=True)\n",
    "    else:\n",
    "        temp_df2 = data_preparation.query_csv(client=QUERY_CSV_PATH, date=day, site=None)\n",
    "        # Filter for EC data, this step will be done in the query\n",
    "        if temp_df2 is not None:\n",
    "            temp_df2=temp_df2[(temp_df2['unit']=='kWh') | (temp_df2['unit']=='m³')]\n",
    "    if temp_df2 is None:\n",
    "        continue\n",
    "    col_names = ['datetime']\n",
    "    col_names.extend(temp_df2.columns[1:])\n",
    "    temp_df2.columns = col_names\n",
    "    temp_df2 = aggregation.split_datetime(temp_df2)\n",
    "    # Creating uniqueId\n",
    "    temp_df2=data_preparation.create_unique_id(temp_df2)\n",
    "    # Filtering dataframe for only relevant fields\n",
    "    temp_df2=temp_df2[['uniqueId', 'date', 'hour', 'unit', 'value']]\n",
    "    if is_first_iter:\n",
    "        # Creating a low memory dataframe for the append_agg function before the structure is changed by agg_all\n",
    "        struct_df2 = temp_df2.head(1)\n",
    "        # Aggregating the first date's data\n",
    "        ec_data1=aggregation.agg_numeric_by_col(temp_df2, col_idx=[0,1,2,3], how='mean')\n",
    "        # Also create second DF by aggregating further just using sensor ID fields (end result=1row per sensor)\n",
    "        ec_data2=aggregation.agg_numeric_by_col(temp_df2, col_idx=[0,3], how='all')\n",
    "        is_first_iter = False\n",
    "    else:\n",
    "        # Aggregating the current date's data and aggregate it with the current running total\n",
    "        temp_df2a=aggregation.agg_numeric_by_col(temp_df2, col_idx=[0,1,2,3], how='mean')\n",
    "        temp_df2b=aggregation.agg_numeric_by_col(temp_df2, col_idx=[0,3], how='all')\n",
    "        ec_data1=aggregation.append_agg(df1=temp_df2a, df2=ec_data1, struct_df=struct_df2, col_idx=[0,1,2,3])\n",
    "        ec_data2=aggregation.append_agg(df1=temp_df2b, df2=ec_data2, struct_df=struct_df2, col_idx=[0,3])\n",
    "    cnt += 1\n",
    "# Freeing up some memory\n",
    "temp_df2 = None\n",
    "temp_df2a = None\n",
    "temp_df2b = None\n",
    "# Calculating the update rate\n",
    "ec_data2[\"update_rate\"] = ec_data2[\"count\"] / (cnt*24)\n",
    "ec_data2.drop(\"count\", inplace=True, axis=1)\n",
    "\n",
    "# Resetting index columns\n",
    "ec_data1=ec_data1.reset_index()\n",
    "ec_data2=ec_data2.reset_index()\n",
    "\n",
    "# Renaming column\n",
    "ec_data1=ec_data1.rename(columns={\"mean\":\"EC_mean_value\"})\n",
    "\n",
    "# Dataframe with unique sensor ids\n",
    "uniqueSensors=ec_data2['uniqueId'].unique()\n",
    "\n",
    "# Scaling EC data\n",
    "ec_data1['EC_mean_value']=data_preparation.scale_continuous(ec_data1, indexes=[4])\n",
    "\n",
    "\n",
    "# Scaling Cluster data\n",
    "for i in range(6,len(nc_data.columns)):\n",
    "    nc_data.iloc[:,i]=data_preparation.scale_continuous(nc_data, indexes=[i])\n",
    "\n",
    "###   b) For each unique EC sensorID (i.e. row in 2b_EC_data_df), create Ridge Regression model using\n",
    "#       2a_EC_data_df and step1_output_NC_data_df. Model is basically: Y=EC response and Xn=NC data\n",
    "print(\"\\t##### ~~~ Started - Step 2 b): Regression ~~~ #####\") # For tracking program progress\n",
    "\n",
    "# Will store each ridge output into a list and append all the dataframes\n",
    "coefficients_list=[]\n",
    "\n",
    "# total sum of mse from each ridge regression model (accumulative)\n",
    "score=0\n",
    "\n",
    "# Creating individual data frames for each sensor and implementing Ridge Regression\n",
    "for sensor in uniqueSensors:\n",
    "    # Create data frame for only that relevant sensor\n",
    "    new_df=ec_data1[ec_data1['uniqueId']==sensor]\n",
    "    # Changing EC data types for merging later\n",
    "    nc_data = nc_data.astype({\"date\": str})\n",
    "    new_df = new_df.astype({\"date\": object, \"hour\": object})\n",
    "    new_df.loc[:,'date']=new_df['date'].apply(lambda x: str(x)[0:10])\n",
    "\n",
    "    # Merge specific sensor to cluster data\n",
    "    new_merged=pd.merge(nc_data, new_df, how='inner', left_on=['date','hour'], right_on=['date','hour'])\n",
    "    # Replacing NaN's with 0 (Ridge Regression doesn't allow NANs)\n",
    "    new_merged=new_merged.fillna(0)\n",
    "\n",
    "    # All NC predictor variables\n",
    "    X=new_merged.iloc[:,2:(len(new_merged.columns)-3)]\n",
    "\n",
    "    # Mean value of EC data\n",
    "    Y=new_merged['EC_mean_value']\n",
    "    Y=Y.to_numpy().reshape(len(Y),1)\n",
    "\n",
    "    # Ridge CV to find optimal alpha value\n",
    "    alphas=[0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1e3, 1e4, 1e5, 1e6, 1e7, 1e8]\n",
    "    reg=RidgeCV(alphas=alphas, store_cv_values=True)\n",
    "    reg.fit(X, Y)\n",
    "    alpha_best=reg.alpha_\n",
    "\n",
    "    # Ridge model using optimal alpha value found in step above\n",
    "    ridge_test=Ridge(alpha=alpha_best, tol=.01, max_iter=10e7,normalize=True)\n",
    "    ridge_test.fit(X,Y)\n",
    "    coef=ridge_test.coef_\n",
    "    mse=mean_squared_error(y_true=Y, y_pred=ridge_test.predict(X))\n",
    "    score=score+mse\n",
    "\n",
    "    # Store coefficients into a dataframe\n",
    "    new=pd.DataFrame(data=coef.reshape(1,((len(new_merged.columns)-3)-2)))\n",
    "\n",
    "    # Add uniqueId to the dataframe\n",
    "    new['uniqueId']=sensor\n",
    "\n",
    "    # Store each sensorID's ridge coefficients into a list\n",
    "    coefficients_list.append(new)\n",
    "\n",
    "# Append all ridge coefficients for all sensors into a single dataframe\n",
    "for df in uniqueSensors:\n",
    "    final_df = pd.concat(coefficients_list)\n",
    "\n",
    "# calculate the avarge mse across all ridge regression models\n",
    "avg_mse=score/len(uniqueSensors)\n",
    "print(\"avg_mse:\",avg_mse)\n",
    "\n",
    "if SAVE_STEP_OUTPUTS:\n",
    "    final_df.to_csv(STEP2_SAVE_PATH_FINAL_DF)\n",
    "    ec_data2.to_csv(STEP2_SAVE_PATH_EC_DATA2)\n",
    "print(\"####### ~~~~~ Complete - Step 2: Model EC/NC Relationship ~~~~~ #######\") # For tracking program progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fvG-GfiSEwyZ"
   },
   "source": [
    "### Loading Step 2 Output from csv\n",
    "The following code chunk allows the user to load the Step 2 output from a csv of a previously saved output from Step 2. \n",
    "\n",
    "**This chunk does not need to be run if the full Step 2 code has already been run.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Wb1xvuBEwyc"
   },
   "outputs": [],
   "source": [
    "final_df = pd.read_csv(STEP2_SAVE_PATH_FINAL_DF, index_col=0)\n",
    "ec_data2 = pd.read_csv(STEP2_SAVE_PATH_EC_DATA2, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7LsEiE2kgiIS"
   },
   "source": [
    "***\n",
    "## Step 3 - Prep EC data for classification model\n",
    "\n",
    "This purpose of Step 3 is to prepare EC data to feed into the classification model by appending various features, passing it through a feature selection process, encoding the categorical features, scaling the numeric features, and combining the columns of coefficients from the EC/NC Relationship model results.\n",
    "\n",
    "The activities required to achieve Step 3 are the following: \n",
    "1. Load metadata csv and store as `metadata` dataframe. Drop duplicates based on most recent lastSynced date\n",
    "2. Clean `metadata` by dropping unncessary fields and changing boolean check marks to more easily identifiable levels\n",
    "3. Create a `left_merged` dataframe by doing a left join using `ec_data2` and `metadata` on uniqueId\n",
    "4. Load training/test set csv and store as `end_use_labels` dataframe\n",
    "5. Clean `end_use_labels` by correcting unit of measurements, changing boolean check marks to more easily identifiable levels, selecting relevant fields, and dropping duplicate rows\n",
    "6. Create a `merged_outer` dataframe by doing an outer join using `merged_left` and `end_use_labels` on uniqueId\n",
    "7. Transform equipRef and navName categorical levels into smaller groups for `merged_outer`\n",
    "8. After feature selection (please refer to feature_selection.ipynb on Git), encode selected categorical features and scale selected numeric features in `merged_outer`\n",
    "9. Create a `step3_data` dataframe by doing an outer join using `merged_outer` and Step 2's `final_data` on uniqueId\n",
    "10. Drop unncessary fields from `step3_data`\n",
    "11. Populate endUseLabel that are null with \"99_UNKNOWN\" in `step3_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IN7oNIgxgg6_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### ~~~~~ Starting - Step 3: Prep EC Data for Classification Model ~~~~~ #######\n",
      "####### ~~~~~ Complete - Step 3: Prep EC Data for Classification Model ~~~~~ #######\n"
     ]
    }
   ],
   "source": [
    "print(\"####### ~~~~~ Starting - Step 3: Prep EC Data for Classification Model ~~~~~ #######\") # For tracking program progress\n",
    "###   a) Load metadata and join with 2b_EC_data_df\n",
    "metadata=pd.read_csv(METADATA_CSV_PATH, dtype=object)\n",
    "# Make uniqueIDs\n",
    "metadata=data_preparation.create_unique_id(metadata, metadata=True)\n",
    "# Drop duplicates\n",
    "metadata=metadata.sort_values('lastSynced').drop_duplicates('uniqueId',keep='last')\n",
    "# Choose relevant fields\n",
    "metadata=metadata[['uniqueId','kind', 'energy','power', 'sensor', 'unit', 'water']] \n",
    "# Changing boolean to easily identify during encoding process\n",
    "metadata['energy']=metadata['energy'].apply(lambda x: 'yes_energy' if x=='✓' else 'no_energy')\n",
    "metadata['power']=metadata['power'].apply(lambda x: 'yes_power' if x=='✓' else 'no_power')\n",
    "metadata['sensor']=metadata['sensor'].apply(lambda x: 'yes_sensor' if x=='✓' else 'no_sensor')\n",
    "metadata['water']=metadata['water'].apply(lambda x: 'yes_water' if x=='✓' else 'no_water')\n",
    "metadata['unit']=metadata['unit'].apply(lambda x: 'omit' if x=='_' else x)\n",
    "# left join metadata and 2b_EC_data_df\n",
    "merged_left=pd.merge(ec_data2, metadata, how='left', left_on='uniqueId', right_on='uniqueId')\n",
    "\n",
    "###   b) Apply feature selection function(s) to the joined EC+metadata\n",
    "# load end_use classifications training data\n",
    "end_use_labels=pd.read_csv(TRAINING_SET_PATH)\n",
    "# make uniqueId\n",
    "end_use_labels['siteRef']=QUERY_SITE\n",
    "end_use_labels=data_preparation.create_unique_id(end_use_labels)\n",
    "\n",
    "# rename columns to fix unit of measurements\n",
    "end_use_labels.rename(columns={'UBC_EWS.firstValue':'value'}, inplace=True)\n",
    "# run correct_df_units function\n",
    "end_use_labels=data_preparation.correct_df_units(end_use_labels)\n",
    "\n",
    "# TRAINING DATA CLEANING\n",
    "# Change ? to 0 since uom fixed\n",
    "end_use_labels=end_use_labels.assign(isGas=end_use_labels.isGas.apply(lambda x: '0' if x=='?' else x))\n",
    "# changing boolean for more descriptive encoding\n",
    "end_use_labels=end_use_labels.assign(isGas=end_use_labels.isGas.apply(lambda x: 'no_gas' if x=='0' else 'yes_gas'))\n",
    "\n",
    "# selecting relevant training data fields\n",
    "end_use_labels=end_use_labels[['uniqueId', 'isGas', 'equipRef', 'groupRef', 'navName', 'endUseLabel']]\n",
    "end_use_labels=end_use_labels.drop_duplicates()\n",
    "merged_outer=pd.merge(left=merged_left, right=end_use_labels, how='left', left_on='uniqueId', right_on='uniqueId')\n",
    "# make equipRef and navName into smaller categories for feature engineering\n",
    "merged_outer=merged_outer.assign(equipRef=merged_outer.equipRef.apply(lambda x: data_preparation.equip_label(str(x))))\n",
    "merged_outer=merged_outer.assign(navName=merged_outer.navName.apply(lambda x: data_preparation.nav_label(str(x))))\n",
    "\n",
    "###   c) Encode and scale the EC+metadata\n",
    "# encoding after feature selection\n",
    "merged_outer=merged_outer.assign(energy_no_energy=merged_outer.energy.apply(lambda x: 1 if x=='no_energy' else 0))\n",
    "merged_outer=merged_outer.assign(energy_yes_energy=merged_outer.energy.apply(lambda x: 1 if x=='yes_energy' else 0))\n",
    "merged_outer=merged_outer.assign(sensor_no_sensor=merged_outer.sensor.apply(lambda x: 1 if x=='no_sensor' else 0))\n",
    "merged_outer=merged_outer.assign(sensor_yes_sensor=merged_outer.sensor.apply(lambda x: 1 if x=='yes_sensor' else 0))\n",
    "merged_outer=merged_outer.assign(equipRef_Air_Equip=merged_outer.equipRef.apply(lambda x: 1 if x=='Air_Equip' else 0))\n",
    "merged_outer=merged_outer.assign(equipRef_Cooling=merged_outer.equipRef.apply(lambda x: 1 if x=='Cooling' else 0))\n",
    "merged_outer=merged_outer.assign(equipRef_Heating=merged_outer.equipRef.apply(lambda x: 1 if x=='Heating' else 0))\n",
    "merged_outer=merged_outer.assign(equipRef_LEED=merged_outer.equipRef.apply(lambda x: 1 if x=='LEED' else 0))\n",
    "merged_outer=merged_outer.assign(navName_Energy=merged_outer.navName.apply(lambda x: 1 if x=='Energy' else 0))\n",
    "\n",
    "# scaling after feature selection\n",
    "for i in range(1,6):\n",
    "    merged_outer.iloc[:,i]=data_preparation.scale_continuous(merged_outer, indexes=[i])\n",
    "\n",
    "###   d) Join the model coeffecients from step2 output to the EC+metadata\n",
    "step3_data = pd.merge(merged_outer, final_df, left_on='uniqueId', right_on='uniqueId', how='outer')\n",
    "# dropping unnessary columns to feed into classification\n",
    "step3_data = step3_data.drop(['kind', 'energy', 'power', 'sensor', 'water', 'isGas', 'equipRef', 'groupRef', 'navName', 'unit'], axis=1)\n",
    "# Populating endUseLabel that are null with 99_UNKNOWN so that they can be predicted\n",
    "step3_data.loc[:, 'endUseLabel'] = step3_data.loc[:, 'endUseLabel'].fillna('99_UNKNOWN')\n",
    "if SAVE_STEP_OUTPUTS:\n",
    "    step3_data.to_csv(STEP3_SAVE_PATH)\n",
    "print(\"####### ~~~~~ Complete - Step 3: Prep EC Data for Classification Model ~~~~~ #######\") # For tracking program progress\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ATL9BGJVFnpJ"
   },
   "source": [
    "### Loading Step 3 Output from csv\n",
    "The following code chunk allows the user to load the Step 3 output from a csv of a previously saved output from Step 3. \n",
    "\n",
    "**This chunk does not need to be run if the full Step 3 code has already been run.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AmZMc6quFnpM"
   },
   "outputs": [],
   "source": [
    "step3_data = pd.read_csv(STEP3_SAVE_PATH, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2y7Sozlogqtd"
   },
   "source": [
    "***\n",
    "## Step 4 - Classification model for EC data\n",
    "The purpose of Step 4 is to take the data from the previous steps and predict the end-uses of each of the unknown energy-consumption sensors. The final output of this step is a dataframe of sensor unique ID's and the corresponding sensor end-uses. This dataframe is saved to a csv, which can then be fed into Step 5.\n",
    "\n",
    "The activities required to accomplish the goals of this step are the following:\n",
    "1. Manipulate the dataset such that the known end-use labels from the training set are the last column in the dataframe and store updates back in `training_data`\n",
    "  * NOTE: This allows for any number of predictor variables to be used, this flexibility is required to maintain the ability to customize the number of clusters generated in Step 1\n",
    "2. Extract only the numbers from the end-use labels since the classifier works with integers not strings and store updates back in `training_data['endUseLabel']`\n",
    "3. Prep the data for prediction and classification by creating separate datasets for training and predicting and store them in `training_data` and `predicting_data` respectively\n",
    "4. Create and train the `classifier`\n",
    "5. Predict the end-uses of the unlabeled sensors and store the results in `y_pred`\n",
    "6. Create and populate the output dataframe, `sensor_labels`, and save it as a csv\n",
    "7. If `DISPLAY_PREDICTION_METRICS==True`\n",
    "  1. Split the training data into a training and testing set\n",
    "  2. Re-create and train the classifier\n",
    "  3. Calculate and print the prediction metrics listed below for the currently selected supervised model:\n",
    "    * Confusion Matrix\n",
    "    * Accuracy\n",
    "    * Precision\n",
    "    * Recall\n",
    "    * F1 Score\n",
    "    * Log Loss\n",
    "  4. Calculate the above metrics for Supervised Classification models imported in the **Library Imports** Section and print the top performing model(s) with its/their performance metrics\n",
    "    * NOTE: Current functionality just prints the top performing model(s), it doesn't currently implement it/them\n",
    "\n",
    "The following code chunk is where any changes to the supervised model must be done. This would include changing the model employed in the final implementation, and modifying the code to automatically select which supervised model to use based on performance on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5_NLifCmgpS4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### ~~~~~ Starting - Step 4: Supervised Modeling and Predicting End-Use Labels ~~~~~ #######\n",
      "\t##### ~~~ Step 4: Displaying Prediction Metrics For the Current Model ~~~ #####\n",
      "[[6 0 0 0 0 0]\n",
      " [0 3 0 0 0 0]\n",
      " [0 0 5 0 0 0]\n",
      " [0 0 0 5 0 0]\n",
      " [0 0 0 0 2 0]\n",
      " [0 0 3 0 0 5]]\n",
      "accuracy: 0.896551724137931\n",
      "precision: 0.9353448275862069\n",
      "recall: 0.896551724137931\n",
      "f1: 0.896551724137931\n",
      "logloss: 0.28564799944349084\n",
      "\t##### ~~~ Step 4: Identifying the Best Supervised Model for the Current Data ~~~ #####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/connor/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/connor/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/connor/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         accuracy precision    recall  f1_score   logloss  num_best\n",
      "bagging  0.896552  0.935345  0.896552  0.896552  0.279866         5\n",
      "####### ~~~~~ Complete - Step 4: Supervised Modeling and Predicting End-Use Labels ~~~~~ #######\n"
     ]
    }
   ],
   "source": [
    "print(\"####### ~~~~~ Starting - Step 4: Supervised Modeling and Predicting End-Use Labels ~~~~~ #######\") # For tracking program progress\n",
    "###   a) Dataprep to get the step 3 data into an appropriate format for prediction\n",
    "training_data=step3_data\n",
    "# Manipulating dataset to be in the appropriate format for creating seperate predictor and response datasets\n",
    "cols = training_data.columns.tolist()\n",
    "cols.remove('endUseLabel')\n",
    "cols.append('endUseLabel')\n",
    "training_data = training_data[cols]\n",
    "# Creating End Use Label Dictionary\n",
    "end_use_labels = {}\n",
    "for label in training_data['endUseLabel'].unique():\n",
    "    end_use_labels[int(str(label)[0:2])] = label\n",
    "# Extracting just the number from the label\n",
    "training_data['endUseLabel'] = training_data['endUseLabel'].apply(lambda x: int(str(x)[0:2]))\n",
    "predicting_data = training_data[(training_data['endUseLabel']==99)]\n",
    "training_data=training_data[(training_data['endUseLabel']!=99)]\n",
    "# Storing Training and Prediction labels\n",
    "predicting_labels = predicting_data['uniqueId']\n",
    "predicting_labels = pd.concat([predicting_labels, predicting_data['endUseLabel']], axis=1)\n",
    "training_labels = training_data['uniqueId']\n",
    "training_labels = pd.concat([training_labels, training_data['endUseLabel']], axis=1)\n",
    "# Dropping uniqueid and filling na's with 0\n",
    "training_data = training_data.drop('uniqueId', axis=1)\n",
    "training_data = training_data.fillna(0)\n",
    "predicting_data = predicting_data.drop('uniqueId', axis=1)\n",
    "predicting_data = predicting_data.fillna(0)\n",
    "# Creating seperate predictor variable and response variable numpy arrays\n",
    "x_train = training_data.iloc[:, :-1].values\n",
    "y_train = training_data.iloc[:, -1].values\n",
    "x_pred = predicting_data.iloc[:, :-1].values\n",
    "\n",
    "###   b) Create and train the selected model\n",
    "# Creating and fitting the classifier\n",
    "classifier = BaggingClassifier(n_estimators = 100)\n",
    "classifier.fit(x_train, y_train)\n",
    "\n",
    "###   c) Predict the outputs for the new data\n",
    "# Predicting the outputs\n",
    "y_pred = classifier.predict(x_pred)\n",
    "\n",
    "###   d) Create dataframe of sensors and labels to be input for step 5\n",
    "predicting_labels['endUseLabel'] = y_pred\n",
    "sensor_labels = pd.concat([training_labels, predicting_labels])\n",
    "sensor_labels['endUseLabel'] = sensor_labels['endUseLabel'].apply(lambda x: end_use_labels[x])\n",
    "sensor_labels.to_csv(PREDICTED_SAVE_PATH, index=False)\n",
    "\n",
    "###   e) Display prediction metrics on a train-test split of the testing data if desired\n",
    "if DISPLAY_PREDICTION_METRICS:\n",
    "    print(\"\\t##### ~~~ Step 4: Displaying Prediction Metrics For the Current Model ~~~ #####\") # For tracking program progress\n",
    "    # Creating training and testing sets\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size = 0.2)\n",
    "    # Creating the classifier and predicting the output for the test set\n",
    "    classifier = BaggingClassifier(n_estimators = 100)\n",
    "    classifier.fit(x_train, y_train)\n",
    "    y_pred = classifier.predict(x_test)\n",
    "    \n",
    "    # Calculating and displaying the comparison metrics\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    try:\n",
    "        logloss = log_loss(y_true=y_test, y_pred=classifier.predict_proba(x_test))\n",
    "    except:\n",
    "        logloss = \"Unable to calculate logloss: Random train/test split did not provide at least 1 item from each class in both the trianing and testing set\"\n",
    "    print(\"accuracy: \"+str(accuracy))\n",
    "    print(\"precision: \"+str(precision))\n",
    "    print(\"recall: \"+str(recall))\n",
    "    print(\"f1: \"+str(f1))\n",
    "    print(\"logloss: \"+str(logloss))\n",
    "    \n",
    "    print(\"\\t##### ~~~ Step 4: Identifying the Best Supervised Model for the Current Data ~~~ #####\")      \n",
    "    # Initializing the dataframe displaying the results\n",
    "    models = ['decision_tree', 'knn', 'kernel_svm', 'logistic_regression', 'naive_bayse', 'random_forest', 'extremely_random_trees', 'bagging', 'adaboost', 'gradientboost']\n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'logloss']\n",
    "    results = pd.DataFrame(index=models, columns=metrics)\n",
    "    \n",
    "    # Creating a list of the classifier options\n",
    "    classifier_list = []\n",
    "    classifier_list.append(DecisionTreeClassifier(criterion = 'entropy', random_state = 0))\n",
    "    classifier_list.append(KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2))\n",
    "    classifier_list.append(SVC(kernel = 'rbf', random_state = 0, probability=True))\n",
    "    classifier_list.append(LogisticRegression(random_state = 0))\n",
    "    classifier_list.append(GaussianNB())\n",
    "    classifier_list.append(RandomForestClassifier(n_estimators = 100, criterion = 'gini', random_state = 0))\n",
    "    classifier_list.append(ExtraTreesClassifier(n_estimators = 100, criterion = 'gini', random_state = 0))\n",
    "    classifier_list.append(BaggingClassifier(n_estimators = 100, random_state = 0))\n",
    "    classifier_list.append(AdaBoostClassifier(n_estimators = 100, random_state = 0))\n",
    "    classifier_list.append(GradientBoostingClassifier(n_estimators = 100, loss='deviance', criterion='friedman_mse', random_state = 0))\n",
    "    \n",
    "    # For loop to go through each of the classifiers and calculate their comparison metrics on the training and testing data\n",
    "    i=0\n",
    "    for classifier in classifier_list:\n",
    "        classifier.fit(x_train, y_train)\n",
    "        y_pred = classifier.predict(x_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average='weighted')\n",
    "        recall = recall_score(y_test, y_pred, average='weighted')\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        logloss = log_loss(y_true=y_test, y_pred=classifier.predict_proba(x_test))\n",
    "        # Storing comparison metrics in dataframe\n",
    "        results.loc[models[i], 'accuracy'] = accuracy\n",
    "        results.loc[models[i], 'precision'] = precision\n",
    "        results.loc[models[i], 'recall'] = recall\n",
    "        results.loc[models[i], 'f1_score'] = f1\n",
    "        results.loc[models[i], 'logloss'] = logloss\n",
    "        i+=1\n",
    "    \n",
    "    # Identifying which classifier(s) performed best overall   \n",
    "    results['num_best'] = 0\n",
    "    for metric in metrics:\n",
    "        if metric != 'logloss':\n",
    "            cur_best = max(results.loc[:,metric])\n",
    "        else:\n",
    "            cur_best = min(results.loc[:,metric])\n",
    "        for model in models:\n",
    "            if results.loc[model, metric]==cur_best:\n",
    "                results.loc[model,'num_best'] += 1\n",
    "    # Printing the best classifier(s) and their associated comparison metrics\n",
    "    print(results[results['num_best']==results['num_best'].max()])\n",
    "\n",
    "print(\"####### ~~~~~ Complete - Step 4: Supervised Modeling and Predicting End-Use Labels ~~~~~ #######\") # For tracking program progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OxvxlMOQgwaL"
   },
   "source": [
    "***\n",
    "## Step 5 - Write results to InfluxDB\n",
    "The purpose of Step 5 is to write the predicted end-uses from Step 4 to the `END_USE` measurement in UDL's `SKYPSARK` InfluxDB instance.\n",
    "\n",
    "The activities required to accomplish the goals of this step are the following:\n",
    "1. Have the user verify that the file location for the data to be written is the correct path or have them update the path and store the correct path in `file_location`\n",
    "2. Prompt the user for the `username` and `password` to verify that they are authorized to write to the InfluxDB instance\n",
    "3. Connect to the InfluxDB client and store the connection in `client`\n",
    "4. Verify that the connection was successfull\n",
    "5. Load the end-use labels from the csv defined at the `file_location` path\n",
    "6. Create a constant `TIMESTAMP` for all measurements\n",
    "  * Required since InfluxDB is a time series database, therefore all observations must have a timestamp\n",
    "7. Write the end-uses to the InfluxDB measurement specified in `EU_MEASUREMENT`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wS4gs-HaWJ--"
   },
   "outputs": [],
   "source": [
    "if influxdb.__version__==\"5.3.0\":\n",
    "    print(\"There may be issues with this version of influxdb-python. If you encounter problems try downgrading to 5.2.3 or checking if there is a newer version than 5.3.0.\")\n",
    "\n",
    "file_location = MODEL_OUTPUT_FILE_LOCATION #Set default value for file location\n",
    "\n",
    "#Then check with user-input for different file location\n",
    "prompt = \"input path + filename or leave blank for default of \"+MODEL_OUTPUT_FILE_LOCATION\n",
    "filename_input = input(prompt)\n",
    "if len(filename_input)>0:\n",
    "    file_location=filename_input\n",
    "\n",
    "username=input(\"input username:\")\n",
    "password = input(\"input password:\")\n",
    "\n",
    "\n",
    "#Connect to localdb and test connection by seeing if any databases exist\n",
    "client = influxdb.DataFrameClient(host=HOST_ADDRESS,port=HOST_PORT, username=username,password=password,database=DATABASE_NAME)\n",
    "\n",
    "try:\n",
    "    client.ping()\n",
    "except:\n",
    "    raise Exception(\"Can not connect to InfluxDB. Is your network connection ok?\")\n",
    "\n",
    "#Load the output from the model into a dataframe\n",
    "output = pd.read_csv(file_location)\n",
    "if 'uniqueID' not in output.columns:\n",
    "    output.rename(columns={'uniqueId':'uniqueID'}, inplace=True)\n",
    "if len(output['uniqueID'].unique())<len(output):\n",
    "    raise Exception(\"There are duplicated uniqueID's in the data, correct the output file and run again.\")\n",
    "\n",
    "\n",
    "#Add a constant timestamp value and make it the index (influxdb-python expects a timestamp as the index)\n",
    "#By making it a constant value, it will be simple to overwrite the points in the influxdb \n",
    "#  when there is an update.\n",
    "\n",
    "TIMESTAMP=pd.to_datetime(\"2020-01-01\")\n",
    "output['time'] = TIMESTAMP\n",
    "output.set_index(['time'], inplace=True)\n",
    "\n",
    "#Write new points to the database using influxdb.DataframeClient().write_points()\n",
    "#Note that all the uniqueId is a tag and the endUseLabel is a field.\n",
    "#Since there can only be one point for every unique combo of timestamp+tag+field, if the\n",
    "# uniqueID already exists in the database, the endUseLabel field's value will simply be\n",
    "# overwritten with the new value. \n",
    "\n",
    "#NOTE: If things really get messed up you can delete the entire measurement from influx\n",
    "# and it will be recreated when you write new points to it.\n",
    "# But be careful as there is no undo so if you are storing quite a few end-use records\n",
    "# in Influx you will lose them all and have to re-write them.\n",
    "# here is the function you would use to do it:\n",
    "#--------------\n",
    "#client.delete_series(measurement=\"END_USE\")\n",
    "#-------------\n",
    "\n",
    "\n",
    "#Check that connection exists:\n",
    "prompt = \"Enter 'y' to write all \" + str(len(output))+\" points to the influx database.\"\n",
    "if input(prompt)=='y':\n",
    "    try:    \n",
    "        if (client.write_points(dataframe=output,measurement=EU_MEASUREMENT,protocol='line', \n",
    "                    tag_columns=[\"uniqueID\"], field_columns=['endUseLabel'])!=True):\n",
    "            print(\"There was some unknown with the write command problem\")\n",
    "        else:\n",
    "            print(\"Writes were successful\")\n",
    "    except influxdb.exceptions.InfluxDBClientError:\n",
    "        print(\"\\n**Authorization error*** No data written. Check username and password!\\n\")\n",
    "else:\n",
    "    print(\"User cancelled operation\")\n",
    "    exit()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "example_notebook.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
