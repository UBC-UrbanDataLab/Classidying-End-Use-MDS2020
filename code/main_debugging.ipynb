{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUNNING CONNOR'S PORTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### ~~~~~ Started - Step 1: Clustering Phase ~~~~~ #######\n",
      "\t##### ~~~ Started - Aggregation Phase 1 ~~~ #####\n",
      "\t\t1: 2020-03-16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t2: 2020-05-01\n",
      "\t\t### ~ Started - Agg Phase 1: Calculating Update Rates ~ ###\n",
      "\t##### ~~~ Started - Clustering Phase ~~~ #####\n",
      "Silhouette Score for meanshift clustering: 0.3940774047925365\n",
      "Silhouette Score for vbgm clustering: 0.5064490774153748\n",
      "\t##### ~~~ Started - Aggregation Phase 2 ~~~ #####\n",
      "\t\t1: 2020-03-16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t2: 2020-05-01\n",
      "\t\t### ~ Started - Agg Phase 2: Calculating Update Rates ~ ###\n"
     ]
    }
   ],
   "source": [
    "    #!/usr/bin/env python3\n",
    "    # -*- coding: utf-8 -*-\n",
    "    \"\"\"\n",
    "    Created on Wed May 27 16:27:50 2020\n",
    "\n",
    "    @author: connor\n",
    "    \"\"\"\n",
    "    ### ~ Library Imports ~ ###\n",
    "    # Data Formatting and Manipulation Imports\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    # Supervised Learning Imports\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.linear_model import Ridge, RidgeCV\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "\n",
    "    # Project Module Imports\n",
    "    import data_preparation\n",
    "    import aggregation\n",
    "    import clustering\n",
    "\n",
    "\n",
    "# def main():\n",
    "    #0) Set Constants (remember, constants are named in all caps with underscores between words)\n",
    "    #################\n",
    "\n",
    "    # TODO: write code to create a proper list of each day in the decided upon date-range store as DATELIST\n",
    "\n",
    "    DATELIST = [\"2020-03-16\",\"2020-05-01\"] # These dates are in the test_data folder so this is just here for testing purposes\n",
    "\n",
    "    SENSOR_ID_TAGS = [1,2,3,4,5,6] # order is [\"groupRef\",\"equipRef\",\"navName\",\"siteRef\",\"typeRef\",\"unit\"]\n",
    "    #NOTE: Including \"unit\" here means that we WILL have inconsistent units after aggregations unless we address\n",
    "    # them in the for loop BEFORE running agg_all, it's fine for now but this will need to be addressed\n",
    "    # Including \"unit\" causes issues when there are duplicate items with mixed units (need to run the code to fix the\n",
    "    # units during this for loop or ignore units in the clustering phase)\n",
    "\n",
    "    ##############################################################################################################\n",
    "    ############### TEMP: for testing from csvs to delete once actual querying code is implemented ###############\n",
    "    ###############       (gets list of file names in the given path)\n",
    "    from os import listdir\n",
    "    from os.path import isfile, join\n",
    "    mypath = 'test_data'\n",
    "    # DATELIST = [f.split(\".\")[0] for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "    DATELIST = [\"2020-03-16\",\"2020-05-01\"] # These dates are in the test_data folder so this is just here for testing purposes\n",
    "    ############### TEMP: for testing from csvs to delete once actual querying code is implemented ###############\n",
    "    ##############################################################################################################\n",
    "\n",
    "    SENSOR_ID_TAGS = [1,2,3,4,5,6] # order is [\"groupRef\",\"equipRef\",\"navName\",\"siteRef\",\"typeRef\",\"unit\"] #NOTE: Including \"unit\" here means that we WILL have inconsistent units after aggregations unless we address them in the for loop BEFORE running agg_all, it's fine for now but this will need to be addressed\n",
    "                                 # Contiued from above: including \"unit\" causes issues when there are duplicate items with mixed units (need to run the code to fix the units during this for loop or ignore units in the clustering phase)\n",
    "\n",
    "    # The planned update to the InfluxDB may change SENSOR_ID_TAGS to only [1] as in [\"uniqueID\"]\n",
    "\n",
    "    #1) Cluster NC data\n",
    "    ###################\n",
    "\n",
    "    print(\"####### ~~~~~ Started - Step 1: Clustering Phase ~~~~~ #######\") ############### TEMP: For Tracking test progress\n",
    "\n",
    "    # a) load+aggregate NC data (including weather), grouping by sensor ID fields [and 'unit'?]\n",
    "    # TODO: Determine what aggregation period to cluster for (Current is just overall values, could hour of the day, 6 hour increments, 12 hour increments, etc...) (I think every hour will likely be too computationally expensive if we need to calculate Gower's distance, and will be if we need MDS)\n",
    "    # TODO: Update to include weather data (should be as simple as expanding the query to include weather data, or having a seperate query for weather data and appending the weather dataframe to the bottom)\n",
    "    print(\"\\t##### ~~~ Started - Aggregation Phase 1 ~~~ #####\") ############### TEMP: For Tracking test progress\n",
    "    last_idx_as_cols = False\n",
    "    is_first_iter = True\n",
    "    cnt=1\n",
    "    for day in DATELIST:\n",
    "        print(\"\\t\\t\"+str(cnt)+\": \"+str(day)) ############### TEMP: For Tracking test progress\n",
    "        # Querying and preping data for aggregations\n",
    "        temp_df = data_preparation.query_csv(client=None, date=day, site=None)\n",
    "        col_names = ['datetime']\n",
    "        col_names.extend(temp_df.columns[1:])\n",
    "        temp_df.columns = col_names\n",
    "        if temp_df is None:\n",
    "            continue\n",
    "        temp_df = aggregation.split_datetime(temp_df)\n",
    "        if is_first_iter:\n",
    "            # Creating a low memory dataframe for the append_agg function before the structure is changed by agg_all\n",
    "            struct_df = temp_df.head(1)\n",
    "            # Aggregating the first date's data\n",
    "            nc_data = aggregation.agg_all(temp_df, how=\"all\", col_idx=SENSOR_ID_TAGS, last_idx_to_col=last_idx_as_cols)\n",
    "            is_first_iter = False\n",
    "        else:\n",
    "            # Aggregating the current date's data and aggregate it with the current running total\n",
    "            temp_df = aggregation.agg_all(temp_df, how=\"all\", col_idx=SENSOR_ID_TAGS, last_idx_to_col=last_idx_as_cols)\n",
    "            nc_data = aggregation.append_agg(df1=temp_df, df2=nc_data, struct_df=struct_df, col_idx=SENSOR_ID_TAGS, last_idx_to_col=last_idx_as_cols)\n",
    "        cnt += 1\n",
    "        if cnt == 15: ############### TEMP: For speeding up testing of updated code for main function delete once updates confirmed to work\n",
    "            break ############### TEMP: For speeding up testing of updated code for main function delete once updates confirmed to work\n",
    "\n",
    "    print(\"\\t\\t### ~ Started - Agg Phase 1: Calculating Update Rates ~ ###\") ############### TEMP: For Tracking test progress\n",
    "    # Freeing up some memory\n",
    "    temp_df = None\n",
    "    # Calculating the update rate\n",
    "    nc_data[\"update_rate\"] = nc_data[\"count\"] / cnt\n",
    "    nc_data.drop(\"count\", inplace=True, axis=1)\n",
    "\n",
    "\n",
    "    #nc_data.to_csv('aggregated_no_units_data.csv') ############### TEMP: Write aggregation output to file to provide sample data for testing step 2, remove for final model\n",
    "\n",
    "\n",
    "    # b) Encode and scale NC data\n",
    "    # TODO: Look up the correct function name for fixing units of measurement (getting added to the query function, can remove/update to DONE once confirmed complete)\n",
    "    # TODO: clean and correct units of measurement (getting added to the query function, can remove/update to DONE once confirmed complete)\n",
    "    # TODO: Test performance for categorical variables included vs excluded\n",
    "    # TODO: Scale continuous variables\n",
    "    # TODO: Test performance for continuous variables scaled vs not scaled\n",
    "\n",
    "\n",
    "    print(\"\\t##### ~~~ Started - Clustering Phase ~~~ #####\") ############### TEMP: For Tracking test progress\n",
    "\n",
    "\n",
    "    # Getting Indexes of the continuous columns\n",
    "    cont_cols = [i for i in range(len(SENSOR_ID_TAGS),len(nc_data.columns))]\n",
    "\n",
    "    # Scale Continuous\n",
    "    scaled_data = data_preparation.scale_continuous(nc_data, cont_cols)\n",
    "    nc_data = pd.concat([nc_data.iloc[:, 0:len(SENSOR_ID_TAGS)], pd.DataFrame(scaled_data, index=nc_data.index, columns=nc_data.columns[cont_cols].tolist())], axis=1)\n",
    "\n",
    "    # Encoding units\n",
    "    nc_data = data_preparation.encode_units(nc_data)\n",
    "\n",
    "    # c) cluster NC data to get df of sensor id fields + cluster group number\n",
    "    # TODO: Determine if the model we are using requires Gower's distance (if not including categorical variables then may not need it)\n",
    "    # TODO: Determine if the model we are using requires MDS\n",
    "    # TODO: Determine which clustering model to use\n",
    "\n",
    "    # DONE: Create dataframe that relates the unique sensors to the relevant cluster\n",
    "\n",
    "\n",
    "    # Calculating Gower's Distance, MDS, and clustering\n",
    "    gow_dist = clustering.calc_gowers(nc_data, cont_cols)\n",
    "    #pd.DataFrame(gow_dist).to_csv('gowers_scaled_no_units_data.csv') ############### TEMP: Write aggregation output to file to provide sample data for testing step 2, remove for final model\n",
    "    mds_data = clustering.multidim_scale(gow_dist, num_dim=2)\n",
    "\n",
    "    #clusters = clustering.cluster(gow_dist, 'hdbscan', continuous_columns = cont_cols, input_type='gowers')\n",
    "    clusters = clustering.cluster(mds_data, 'meanshift', continuous_columns = cont_cols, input_type='mds')\n",
    "\n",
    "    ##################################################################################\n",
    "    ###################################### TEST ######################################\n",
    "\n",
    "    # Just for testing purposes (shows how many observations belong to each cluster)\n",
    "    cluster_df = pd.DataFrame(clusters, columns=[\"cluster\"]) #################### Test\n",
    "    cluster_df['cluster'].value_counts().sort_index() ########################### Test\n",
    "\n",
    "    ###################################### TEST ######################################\n",
    "    ##################################################################################\n",
    "\n",
    "\n",
    "    #pd.DataFrame(mds_data).to_csv('mds_2d_scaled_no_units_data.csv') ############### TEMP: Write aggregation output to file to provide sample data for testing step 2, remove for final model\n",
    "    clusters = clustering.cluster(mds_data, 'vbgm', num_clusts=35, input_type='mds')\n",
    "\n",
    "    ###############################################################################\n",
    "    ############### TEMP: Included for testing the clustering model ###############\n",
    "    ###############       Shows how many observations belong to each cluster\n",
    "    ###############       Can delete once done testing clustering model\n",
    "    #cluster_df = pd.DataFrame(clusters, columns=[\"cluster\"])\n",
    "    #cluster_df['cluster'].value_counts().sort_index()\n",
    "    ############### TEMP: Included for testing the clustering model ###############\n",
    "    ###############################################################################\n",
    "\n",
    "\n",
    "    # Generating a list of the columns to keep when making the dataframe relating sensors to clusters(the unique identifiers for an NC sensor and cluster)\n",
    "    unique_cols_idx = [i for i in range(len(SENSOR_ID_TAGS))]\n",
    "    unique_cols = nc_data.columns[unique_cols_idx].values.tolist()\n",
    "    unique_cols.append(\"cluster\")\n",
    "    # Creating dataframe that identifies which unique sensors belong to which cluster\n",
    "    drop_cols = list(set(nc_data.columns.tolist())-set(unique_cols))\n",
    "    cluster_groups = pd.concat([nc_data, pd.DataFrame(clusters, columns=[\"cluster\"])], axis=1)\n",
    "    cluster_groups = cluster_groups.drop(drop_cols, axis=1)\n",
    "\n",
    "\n",
    "    print(\"\\t##### ~~~ Started - Aggregation Phase 2 ~~~ #####\") ############### TEMP: For Tracking test progress\n",
    "\n",
    "\n",
    "    # d) Reload NC data + join cluster group num + aggregate, this time grouping by date, time, and clust_group_num\n",
    "    last_idx_as_cols = True\n",
    "    is_first_iter = True\n",
    "    cnt=1\n",
    "    for day in DATELIST:\n",
    "        print(\"\\t\\t\"+str(cnt)+\": \"+str(day)) ############### TEMP: For Tracking test progress\n",
    "        # Querying and preping data for aggregations\n",
    "        temp_df = data_preparation.query_csv(client=None, date=day, site=None)\n",
    "        col_names = ['datetime']\n",
    "        col_names.extend(temp_df.columns[1:])\n",
    "        temp_df.columns = col_names\n",
    "        if temp_df is None:\n",
    "            continue\n",
    "        temp_df = aggregation.split_datetime(temp_df) # Added to create month and hour columns (must have at least hour for aggs)\n",
    "        temp_df = temp_df.merge(cluster_groups, how='left', on=cluster_groups.columns[:-1].tolist())\n",
    "        if is_first_iter:\n",
    "            # Calculating the count of sensor updates per hour per day per cluster for the first date\n",
    "            update_rates = temp_df.groupby(['hour','date','cluster']).agg({'value':'count'},axis=1)\n",
    "            # Creating a low memory dataframe for the append_agg function before the structure is changed by agg_all (different structure required than previous)\n",
    "            struct_df = temp_df.head(1)\n",
    "            # Identifying the indexes of the items being aggregated on (hour, date, and cluster)\n",
    "            cluster_id_tags = [temp_df.columns.tolist().index(\"hour\"), temp_df.columns.tolist().index(\"date\"), temp_df.columns.tolist().index(\"cluster\")]\n",
    "            # Aggregating the first date's data (Aggregations must be seperate b/c can't data gets too big during calculations if not)\n",
    "            temp_df_aggs = aggregation.agg_all(temp_df, how=\"mean\", col_idx=cluster_id_tags, last_idx_to_col=last_idx_as_cols)\n",
    "            if 'count_x' in temp_df_aggs.columns.tolist():\n",
    "                # Each aggregation type outputs a count, ensuring joins only result in 1 count (applies for all similar if statements)\n",
    "                temp_df_aggs = temp_df_aggs.drop('count_y', axis=1)\n",
    "                temp_df_aggs = temp_df_aggs.rename(columns={'count_x':'count'})\n",
    "            temp_df_aggs = temp_df_aggs.merge(aggregation.agg_all(temp_df, how=\"std\", col_idx=cluster_id_tags, last_idx_to_col=last_idx_as_cols), how='left', on=temp_df_aggs.columns.tolist()[:2])\n",
    "            if 'count_x' in temp_df_aggs.columns.tolist():\n",
    "                temp_df_aggs = temp_df_aggs.drop('count_y', axis=1)\n",
    "                temp_df_aggs = temp_df_aggs.rename(columns={'count_x':'count'})\n",
    "            temp_df_aggs = temp_df_aggs.merge(aggregation.agg_all(temp_df, how=\"max\", col_idx=cluster_id_tags, last_idx_to_col=last_idx_as_cols), how='left', on=temp_df_aggs.columns.tolist()[:2])\n",
    "            if 'count_x' in temp_df_aggs.columns.tolist():\n",
    "                temp_df_aggs = temp_df_aggs.drop('count_y', axis=1)\n",
    "                temp_df_aggs = temp_df_aggs.rename(columns={'count_x':'count'})\n",
    "            nc_data = temp_df_aggs.merge(aggregation.agg_all(temp_df, how=\"min\", col_idx=cluster_id_tags, last_idx_to_col=last_idx_as_cols), how='left', on=temp_df_aggs.columns.tolist()[:2])\n",
    "            if 'count_x' in nc_data.columns.tolist():\n",
    "                nc_data = nc_data.drop('count_y', axis=1)\n",
    "                nc_data = nc_data.rename(columns={'count_x':'count'})\n",
    "            is_first_iter = False\n",
    "        else:\n",
    "            # Calculating the count of sensor updates per hour per day per cluster for the current date\n",
    "            update_rates = pd.concat([update_rates, temp_df.groupby(['hour','date','cluster']).agg({'value':'count'},axis=1)])\n",
    "            # Aggregating the current date's data and aggregate it with the current running total\n",
    "            temp_df_aggs = aggregation.agg_all(temp_df, how=\"mean\", col_idx=cluster_id_tags, last_idx_to_col=last_idx_as_cols)\n",
    "            if 'count_x' in temp_df_aggs.columns.tolist():\n",
    "                temp_df_aggs = temp_df_aggs.drop('count_y', axis=1)\n",
    "                temp_df_aggs = temp_df_aggs.rename(columns={'count_x':'count'})\n",
    "            temp_df_aggs = temp_df_aggs.merge(aggregation.agg_all(temp_df, how=\"std\", col_idx=cluster_id_tags, last_idx_to_col=last_idx_as_cols), how='left', on=temp_df_aggs.columns.tolist()[:2])\n",
    "            if 'count_x' in temp_df_aggs.columns.tolist():\n",
    "                temp_df_aggs = temp_df_aggs.drop('count_y', axis=1)\n",
    "                temp_df_aggs = temp_df_aggs.rename(columns={'count_x':'count'})\n",
    "            temp_df_aggs = temp_df_aggs.merge(aggregation.agg_all(temp_df, how=\"max\", col_idx=cluster_id_tags, last_idx_to_col=last_idx_as_cols), how='left', on=temp_df_aggs.columns.tolist()[:2])\n",
    "            if 'count_x' in temp_df_aggs.columns.tolist():\n",
    "                temp_df_aggs = temp_df_aggs.drop('count_y', axis=1)\n",
    "                temp_df_aggs = temp_df_aggs.rename(columns={'count_x':'count'})\n",
    "            temp_df_aggs = temp_df_aggs.merge(aggregation.agg_all(temp_df, how=\"min\", col_idx=cluster_id_tags, last_idx_to_col=last_idx_as_cols), how='left', on=temp_df_aggs.columns.tolist()[:2])\n",
    "            if 'count_x' in temp_df_aggs.columns.tolist():\n",
    "                temp_df_aggs = temp_df_aggs.drop('count_y', axis=1)\n",
    "                temp_df_aggs = temp_df_aggs.rename(columns={'count_x':'count'})\n",
    "            nc_data = aggregation.append_agg(df1=temp_df_aggs, df2=nc_data, struct_df=struct_df, col_idx=cluster_id_tags, last_idx_to_col=last_idx_as_cols)\n",
    "        cnt += 1\n",
    "\n",
    "        if cnt == 15: ############### TEMP: For speeding up testing of updated code for main function delete once updates confirmed to work\n",
    "            break ############### TEMP: For speeding up testing of updated code for main function delete once updates confirmed to work\n",
    "\n",
    "    print(\"\\t\\t### ~ Started - Agg Phase 2: Calculating Update Rates ~ ###\") ############### TEMP: For Tracking test progress\n",
    "\n",
    "    # Re-format update rates so that clusters are columns\n",
    "    update_rates = update_rates.unstack()\n",
    "    update_rates.columns = update_rates.columns.droplevel(level=0)\n",
    "    update_rates = update_rates.fillna(0)\n",
    "\n",
    "    # Calculate the number of sensors per cluster\n",
    "    sensor_count_per_cluster = cluster_groups.groupby('cluster').agg({cluster_groups.columns.tolist()[0]:'count'})\n",
    "    sensor_count_per_cluster.columns = ['count']\n",
    "\n",
    "    # Calculate the average sensor update rate per hour per cluster\n",
    "    for cluster in cluster_groups['cluster'].unique():\n",
    "        update_rates.loc[:,cluster] = update_rates.loc[:,cluster]/sensor_count_per_cluster.loc[cluster][0]\n",
    "\n",
    "    # Rename the update rate columns and join them to the nc_data\n",
    "    update_rates = update_rates.add_prefix('urate_')\n",
    "    nc_data = nc_data.join(update_rates, how='left', on=['hour', 'date'])\n",
    "    nc_data = nc_data.drop('count', axis=1)\n",
    "\n",
    "\n",
    "    ##################################################################################\n",
    "    ###################################### TEMP ######################################\n",
    "    # Just here for writing sample data to csv for testing in other sections\n",
    "    nc_data.to_csv('sample_cluster_output2.csv')\n",
    "    ###################################### TEMP ######################################\n",
    "    ##################################################################################\n",
    "\n",
    "\n",
    "    print(\"####### ~~~~~ Complete - Step 1: NC Aggregation and Clustering Phase ~~~~~ #######\") ############### TEMP: For Tracking test progress\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     ### ~ Library Imports ~ ###\n",
    "#     # Data Formatting and Manipulation Imports\n",
    "#     import pandas as pd\n",
    "#     import numpy as np\n",
    "\n",
    "#     # Supervised Learning Imports\n",
    "#     from sklearn.ensemble import RandomForestClassifier\n",
    "#     from sklearn.model_selection import train_test_split\n",
    "#     from sklearn.linear_model import Ridge, RidgeCV\n",
    "#     from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#     # Project Module Imports\n",
    "#     import data_preparation\n",
    "#     import aggregation\n",
    "#     import clustering\n",
    "\n",
    "#     nr_can=pd.read_csv('~/data-599-capstone-ubc-urban-data-lab/data/ward_15_with_units_cluster_aggregations.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUNNING STEP 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### ~~~~~ Complete - Step 1: NC Aggregation and Clustering Phase ~~~~~ #######\n",
      "####### ~~~~~ Starting - Step 2: Model EC/NC Relationship ~~~~~ #######\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:2886: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  return runner(coro)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_mse: 0.004725741524691302\n",
      "####### ~~~~~ Complete - Step 2: Model EC/NC Relationship ~~~~~ #######\n"
     ]
    }
   ],
   "source": [
    "    print(\"####### ~~~~~ Starting - Step 2: Model EC/NC Relationship ~~~~~ #######\") ############### TEMP: For Tracking test progress\n",
    "\n",
    "    #2) Model EC/NC relationship\n",
    "    ############################\n",
    "    #    temp) Make a fake version of the output dataframe from step 1 so that step 2 can be (mostly) developed\n",
    "    #        without waiting for step 1 to be finished!\n",
    "\n",
    "    last_idx_as_cols = False\n",
    "    is_first_iter = True\n",
    "    cnt=1\n",
    "    for day in DATELIST:\n",
    "        # Querying and preping data for aggregations\n",
    "        temp_df2 = data_preparation.query_csv(client=None, date=day, site=None)\n",
    "        if temp_df2 is None:\n",
    "            continue\n",
    "        temp_df2 = aggregation.split_datetime(temp_df2)\n",
    "        # Filter for EC data, this step will be done in the query\n",
    "        temp_df2=temp_df2[temp_df2['unit']=='kWh']\n",
    "        # Creating uniqueId\n",
    "        temp_df2=data_preparation.create_unique_id(temp_df2)\n",
    "        # Filtering dataframe for only relevant fields\n",
    "        temp_df2=temp_df2[['uniqueId', 'date', 'hour', 'unit', 'value']]\n",
    "        if is_first_iter:\n",
    "            # Creating a low memory dataframe for the append_agg function before the structure is changed by agg_all\n",
    "            struct_df2 = temp_df2.head(1)\n",
    "            # Aggregating the first date's data\n",
    "            ec_data1=aggregation.agg_numeric_by_col(temp_df2, col_idx=[0,1,2,3], how='mean')\n",
    "    #    b) Also create second DF by aggregating further just using sensor ID fields (end result=1row per sensor)\n",
    "            ec_data2=aggregation.agg_numeric_by_col(temp_df2, col_idx=[0,3], how='all')\n",
    "            is_first_iter = False\n",
    "        else:\n",
    "            # Aggregating the current date's data and aggregate it with the current running total\n",
    "            temp_df2a=aggregation.agg_numeric_by_col(temp_df2, col_idx=[0,1,2,3], how='mean')\n",
    "            temp_df2b=aggregation.agg_numeric_by_col(temp_df2, col_idx=[0,3], how='all')\n",
    "            ec_data1=aggregation.append_agg(df1=temp_df2a, df2=ec_data1, struct_df=struct_df2, col_idx=[0,1,2,3])\n",
    "            ec_data2=aggregation.append_agg(df1=temp_df2b, df2=ec_data2, struct_df=struct_df2, col_idx=[0,3])\n",
    "        cnt += 1\n",
    "    # Freeing up some memory\n",
    "    temp_df2 = None\n",
    "    temp_df2a = None\n",
    "    temp_df2b = None\n",
    "    # Calculating the update rate\n",
    "    ec_data2[\"update_rate\"] = ec_data2[\"count\"] / (cnt*24)\n",
    "    ec_data2.drop(\"count\", inplace=True, axis=1)\n",
    "\n",
    "    # Resetting index columns\n",
    "    ec_data1=ec_data1.reset_index()\n",
    "    ec_data2=ec_data2.reset_index()\n",
    "\n",
    "    # Renaming column\n",
    "    ec_data1=ec_data1.rename(columns={\"mean\":\"EC_mean_value\"})\n",
    "\n",
    "    # Dataframe with unique sensor ids\n",
    "    uniqueSensors=ec_data2['uniqueId'].unique()\n",
    "\n",
    "    ### Scaling EC data\n",
    "    ec_data1['EC_mean_value']=data_preparation.scale_continuous(ec_data1, indexes=[4])\n",
    "\n",
    "\n",
    "    ### Scaling Cluster data\n",
    "    for i in range(6,len(nc_data.columns)):\n",
    "        nc_data.iloc[:,i]=data_preparation.scale_continuous(nc_data, indexes=[i])\n",
    "\n",
    "    #    c) For each unique EC sensorID (i.e. row in 2b_EC_data_df), create LASSO model using 2a_EC_data_df and\n",
    "    #       step1_output_NC_data_df. Model is basically: Y=EC response and Xn=NC data\n",
    "\n",
    "    ### Will store each ridge output into a list and append all the dataframes\n",
    "    coefficients_list=[]\n",
    "\n",
    "    ### total sum of mse from each ridge regression model (accumulative)\n",
    "    score=0\n",
    "\n",
    "    ### Creating individual data frames for each sensor and implementing lasso\n",
    "    for sensor in uniqueSensors:\n",
    "\n",
    "        ## Create data frame for only that relevant sensor\n",
    "        new_df=ec_data1[ec_data1['uniqueId']==sensor]\n",
    "        ######## Changing EC data types for merging later. Might not need depending on step 1 output types\n",
    "        nc_data = nc_data.astype({\"date\": str})\n",
    "        new_df = new_df.astype({\"date\": object, \"hour\": object})\n",
    "        new_df.loc[:,'date']=new_df['date'].apply(lambda x: str(x)[0:10])\n",
    "\n",
    "        ## Merge specific sensor to cluster data\n",
    "        new_merged=pd.merge(nc_data, new_df, how='inner', left_on=['date','hour'], right_on=['date','hour'])\n",
    "        ## Ridge does not allow NANs, seems like some sensors are not 'on' during specific hours\n",
    "        new_merged=new_merged.dropna()\n",
    "\n",
    "        ## All NC predictor variables\n",
    "        X=new_merged.iloc[:,2:22]\n",
    "\n",
    "        ## Mean value of EC data\n",
    "        Y=new_merged['EC_mean_value']\n",
    "        Y=Y.to_numpy().reshape(len(Y),1)\n",
    "\n",
    "        #Ridge CV to find optimal alpha value\n",
    "        alphas=[0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1e3, 1e4, 1e5, 1e6, 1e7, 1e8]\n",
    "        reg=RidgeCV(alphas=alphas, store_cv_values=True)\n",
    "        reg.fit(X, Y)\n",
    "        alpha_best=reg.alpha_\n",
    "\n",
    "      ## Ridge model using optimal alpha value found in step above\n",
    "        ridge_test=Ridge(alpha=alpha_best, tol=.01, max_iter=10e7,normalize=True)\n",
    "        ridge_test.fit(X,Y)\n",
    "        coef=ridge_test.coef_\n",
    "        mse=mean_squared_error(y_true=Y, y_pred=ridge_test.predict(X))\n",
    "        score=score+mse\n",
    "\n",
    "        ## Store coefficients into a dataframe\n",
    "        new=pd.DataFrame(data=coef.reshape(1,20))\n",
    "\n",
    "        ## Add uniqueId to the dataframe\n",
    "        new['uniqueId']=sensor\n",
    "\n",
    "        ## Store each sensorID's ridge coefficients into a list\n",
    "        coefficients_list.append(new)\n",
    "\n",
    "    ### Append all ridge coefficients for all sensors into a single dataframe\n",
    "    for df in uniqueSensors:\n",
    "        final_df = pd.concat(coefficients_list)\n",
    "\n",
    "    ### calculate the avarge mse across all ridge regression models\n",
    "    avg_mse=score/len(uniqueSensors)\n",
    "    print(\"avg_mse:\",avg_mse)\n",
    "\n",
    "    #    OUTPUT OF STEP2 = dataframe with EC sensor ID fields, mean response, and all n coeffecients from\n",
    "    #        that unique EC sensor's LASSO model\n",
    "    print(\"####### ~~~~~ Complete - Step 2: Model EC/NC Relationship ~~~~~ #######\") ############### TEMP: For Tracking test progress\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUNNING STEP 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### ~~~~~ Starting - Step 4: Prep EC Data for Classification Model ~~~~~ #######\n",
      "####### ~~~~~ Complete - Step 4: Prep EC Data for Classification Model ~~~~~ #######\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (2,54,81,118,133,140,149) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "    print(\"####### ~~~~~ Starting - Step 4: Prep EC Data for Classification Model ~~~~~ #######\") ############### TEMP: For Tracking test progress\n",
    "    #     4) Prep EC data for classification model\n",
    "    #     ########################################\n",
    "    #         a) Load metadata and join with 2b_EC_data_df\n",
    "    metadata=pd.read_csv('~/data-599-capstone-ubc-urban-data-lab/code/test_data/PharmacyQuery.csv')\n",
    "    # Make uniqueIDs\n",
    "    metadata=data_preparation.create_unique_id(metadata, metadata=True)\n",
    "    # Drop duplicates\n",
    "    metadata=metadata.sort_values('lastSynced').drop_duplicates('uniqueId',keep='last')\n",
    "    # Choose relevant fields\n",
    "    metadata=metadata[['uniqueId','kind', 'energy','power', 'sensor', 'unit', 'water']]\n",
    "    ### Changing boolean to easily identify during encoding process\n",
    "    metadata['energy']=metadata['energy'].apply(lambda x: 'yes_energy' if x=='✓' else 'no_energy')\n",
    "    metadata['power']=metadata['power'].apply(lambda x: 'yes_power' if x=='✓' else 'no_power')\n",
    "    metadata['sensor']=metadata['sensor'].apply(lambda x: 'yes_sensor' if x=='✓' else 'no_sensor')\n",
    "    metadata['water']=metadata['water'].apply(lambda x: 'yes_water' if x=='✓' else 'no_water')\n",
    "    metadata['unit']=metadata['unit'].apply(lambda x: 'omit' if x=='_' else x)\n",
    "    # inner join metadata and 2b_EC_data_df\n",
    "    merged_inner=pd.merge(ec_data2, metadata, left_on='uniqueId', right_on='uniqueId', how='inner')\n",
    "\n",
    "    #         b) Apply feature selection function(s) to the joined EC+metadata\n",
    "    # load NRCan classifications training data\n",
    "    nrcan_labels=pd.read_csv('~/data-599-capstone-ubc-urban-data-lab/data/FinalPharmacyECSensorList-WithLabels - PharmacyECSensorsWithLabels.csv')\n",
    "    # make uniqueId\n",
    "    nrcan_labels['siteRef']='Pharmacy'\n",
    "    nrcan_labels=data_preparation.create_unique_id(nrcan_labels)\n",
    "\n",
    "    # rename columns to fix unit of measurements\n",
    "    nrcan_labels.rename(columns={'UBC_EWS.firstValue':'value'}, inplace=True)\n",
    "    # run correct_df_units function\n",
    "    nrcan_labels=data_preparation.correct_df_units(nrcan_labels)\n",
    "\n",
    "    # TRAINING DATA CLEANING (maybe its own module with metadata?)\n",
    "    # can change ? to 0 since uom fixed\n",
    "    nrcan_labels=nrcan_labels.assign(isGas=nrcan_labels.isGas.apply(lambda x: '0' if x=='?' else x))\n",
    "    # changing boolean for more descriptive encoding\n",
    "    nrcan_labels=nrcan_labels.assign(isGas=nrcan_labels.isGas.apply(lambda x: 'no_gas' if x=='0' else 'yes_gas'))\n",
    "\n",
    "    # selecting relevant training data fields\n",
    "    nrcan_labels=nrcan_labels[['uniqueId', 'isGas', 'equipRef', 'groupRef', 'navName', 'endUseLabel']]\n",
    "    nrcan_labels=nrcan_labels.drop_duplicates()\n",
    "    merged_outer=pd.merge(left=merged_inner, right=nrcan_labels, how='outer', left_on='uniqueId', right_on='uniqueId')\n",
    "    # make equipRef and navName into smaller categories for feature engineering\n",
    "    merged_outer=merged_outer.assign(equipRef=merged_outer.equipRef.apply(lambda x: data_preparation.equip_label(str(x))))\n",
    "    merged_outer=merged_outer.assign(navName=merged_outer.navName.apply(lambda x: data_preparation.nav_label(str(x))))\n",
    "\n",
    "#             c) Encode and scale the EC+metadata\n",
    "#     encoding after feature selection\n",
    "    merged_outer=merged_outer.assign(energy_no_energy=merged_outer.energy.apply(lambda x: 1 if x=='no_energy' else 0))\n",
    "    merged_outer=merged_outer.assign(energy_yes_energy=merged_outer.energy.apply(lambda x: 1 if x=='yes_energy' else 0))\n",
    "    merged_outer=merged_outer.assign(sensor_no_sensor=merged_outer.sensor.apply(lambda x: 1 if x=='no_sensor' else 0))\n",
    "    merged_outer=merged_outer.assign(sensor_yes_sensor=merged_outer.sensor.apply(lambda x: 1 if x=='yes_sensor' else 0))\n",
    "    merged_outer=merged_outer.assign(equipRef_Air_Equip=merged_outer.equipRef.apply(lambda x: 1 if x=='Air_Equip' else 0))\n",
    "    merged_outer=merged_outer.assign(equipRef_Cooling=merged_outer.equipRef.apply(lambda x: 1 if x=='Cooling' else 0))\n",
    "    merged_outer=merged_outer.assign(equipRef_Heating=merged_outer.equipRef.apply(lambda x: 1 if x=='Heating' else 0))\n",
    "    merged_outer=merged_outer.assign(equipRef_LEED=merged_outer.equipRef.apply(lambda x: 1 if x=='LEED' else 0))\n",
    "#     scaling after feature selection\n",
    "    for i in range(1,6):\n",
    "        merged_outer.iloc[:,i]=data_preparation.scale_continuous(merged_outer, indexes=[i])\n",
    "#             d) Join the model coeffecients from step2 output to the EC+metadata\n",
    "    step4_data = pd.merge(merged_outer, final_df, left_on='uniqueId', right_on='uniqueId', how='outer')\n",
    "#     dropping unnessary columns to feed into classification\n",
    "    step4_data = step4_data.drop(['kind', 'energy', 'power', 'sensor', 'water', 'isGas', 'equipRef', 'groupRef', 'navName', 'unit'], axis=1)\n",
    "#             OUTPUT OF STEP = dataframe with EC sensor ID fields, selected EC features, model coeffecients\n",
    "\n",
    "    print(\"####### ~~~~~ Complete - Step 4: Prep EC Data for Classification Model ~~~~~ #######\") ############### TEMP: For Tracking test progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniqueId</th>\n",
       "      <th>min</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>max</th>\n",
       "      <th>update_rate</th>\n",
       "      <th>endUseLabel</th>\n",
       "      <th>energy_no_energy</th>\n",
       "      <th>energy_yes_energy</th>\n",
       "      <th>sensor_no_sensor</th>\n",
       "      <th>...</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AHU-01 SF Air Systems Energy AHU1_SF_VFD_PWR(kWh)</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.004027</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>1.0</td>\n",
       "      <td>02_HEATING_COOLING_COMBINED</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.964367e-10</td>\n",
       "      <td>-2.092877e-10</td>\n",
       "      <td>-5.341590e-10</td>\n",
       "      <td>1.625958e-10</td>\n",
       "      <td>1.614012e-10</td>\n",
       "      <td>2.083478e-10</td>\n",
       "      <td>-3.151098e-10</td>\n",
       "      <td>-2.244109e-10</td>\n",
       "      <td>1.296283e-09</td>\n",
       "      <td>6.172725e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AHU-02 SF Air Systems Energy AHU2_SF_VFD_PWR(kWh)</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.004096</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>1.0</td>\n",
       "      <td>02_HEATING_COOLING_COMBINED</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.964367e-10</td>\n",
       "      <td>-2.092877e-10</td>\n",
       "      <td>-5.341590e-10</td>\n",
       "      <td>1.625958e-10</td>\n",
       "      <td>1.614012e-10</td>\n",
       "      <td>2.083478e-10</td>\n",
       "      <td>-3.151098e-10</td>\n",
       "      <td>-2.244109e-10</td>\n",
       "      <td>1.296283e-09</td>\n",
       "      <td>6.172725e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AHU-03 SF Air Systems Energy AHU3_SF_VFD_PWR(kWh)</td>\n",
       "      <td>0.000704</td>\n",
       "      <td>0.000706</td>\n",
       "      <td>0.011131</td>\n",
       "      <td>0.000708</td>\n",
       "      <td>1.0</td>\n",
       "      <td>02_HEATING_COOLING_COMBINED</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.964367e-10</td>\n",
       "      <td>-2.092877e-10</td>\n",
       "      <td>-5.341590e-10</td>\n",
       "      <td>1.625958e-10</td>\n",
       "      <td>1.614012e-10</td>\n",
       "      <td>2.083478e-10</td>\n",
       "      <td>-3.151098e-10</td>\n",
       "      <td>-2.244109e-10</td>\n",
       "      <td>1.296283e-09</td>\n",
       "      <td>6.172725e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AHU-04 SF Air Systems Energy AHU4_SF_VFD_PWR(kWh)</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.000702</td>\n",
       "      <td>0.010972</td>\n",
       "      <td>0.000704</td>\n",
       "      <td>1.0</td>\n",
       "      <td>02_HEATING_COOLING_COMBINED</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.964367e-10</td>\n",
       "      <td>-2.092877e-10</td>\n",
       "      <td>-5.341590e-10</td>\n",
       "      <td>1.625958e-10</td>\n",
       "      <td>1.614012e-10</td>\n",
       "      <td>2.083478e-10</td>\n",
       "      <td>-3.151098e-10</td>\n",
       "      <td>-2.244109e-10</td>\n",
       "      <td>1.296283e-09</td>\n",
       "      <td>6.172725e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AHU-05 SF Air Systems Energy AHU5_SF_VFD_PWR(kWh)</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>02_HEATING_COOLING_COMBINED</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.964367e-10</td>\n",
       "      <td>-2.092877e-10</td>\n",
       "      <td>-5.341590e-10</td>\n",
       "      <td>1.625958e-10</td>\n",
       "      <td>1.614012e-10</td>\n",
       "      <td>2.083478e-10</td>\n",
       "      <td>-3.151098e-10</td>\n",
       "      <td>-2.244109e-10</td>\n",
       "      <td>1.296283e-09</td>\n",
       "      <td>6.172725e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>Elec Submeters PB Utilities Energy OPC Energy OPC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99_UNKNOWN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.865562e-02</td>\n",
       "      <td>9.082309e-04</td>\n",
       "      <td>4.606625e-02</td>\n",
       "      <td>2.438907e-02</td>\n",
       "      <td>2.434749e-02</td>\n",
       "      <td>-8.801561e-04</td>\n",
       "      <td>1.339121e-03</td>\n",
       "      <td>1.712560e-02</td>\n",
       "      <td>-6.565314e-02</td>\n",
       "      <td>1.667654e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>Elec Submeters DCB Utilities Energy OPC Copy E...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99_UNKNOWN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.865562e-02</td>\n",
       "      <td>9.082309e-04</td>\n",
       "      <td>4.606625e-02</td>\n",
       "      <td>2.438907e-02</td>\n",
       "      <td>2.434749e-02</td>\n",
       "      <td>-8.801561e-04</td>\n",
       "      <td>1.339121e-03</td>\n",
       "      <td>1.712560e-02</td>\n",
       "      <td>-6.565314e-02</td>\n",
       "      <td>1.667654e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>Cooling Plant CT-1A Hydronic Systems Energy CT...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>Cooling Plant CT-1B Hydronic Systems Energy CT...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>Cooling Plant CT-1C Hydronic Systems Energy CT...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>196 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              uniqueId       min      mean  \\\n",
       "0    AHU-01 SF Air Systems Energy AHU1_SF_VFD_PWR(kWh)  0.000176  0.000177   \n",
       "1    AHU-02 SF Air Systems Energy AHU2_SF_VFD_PWR(kWh)  0.000153  0.000154   \n",
       "2    AHU-03 SF Air Systems Energy AHU3_SF_VFD_PWR(kWh)  0.000704  0.000706   \n",
       "3    AHU-04 SF Air Systems Energy AHU4_SF_VFD_PWR(kWh)  0.000700  0.000702   \n",
       "4    AHU-05 SF Air Systems Energy AHU5_SF_VFD_PWR(kWh)  0.000000  0.000000   \n",
       "..                                                 ...       ...       ...   \n",
       "191  Elec Submeters PB Utilities Energy OPC Energy OPC       NaN       NaN   \n",
       "192  Elec Submeters DCB Utilities Energy OPC Copy E...       NaN       NaN   \n",
       "193  Cooling Plant CT-1A Hydronic Systems Energy CT...       NaN       NaN   \n",
       "194  Cooling Plant CT-1B Hydronic Systems Energy CT...       NaN       NaN   \n",
       "195  Cooling Plant CT-1C Hydronic Systems Energy CT...       NaN       NaN   \n",
       "\n",
       "          std       max  update_rate                  endUseLabel  \\\n",
       "0    0.004027  0.000177          1.0  02_HEATING_COOLING_COMBINED   \n",
       "1    0.004096  0.000155          1.0  02_HEATING_COOLING_COMBINED   \n",
       "2    0.011131  0.000708          1.0  02_HEATING_COOLING_COMBINED   \n",
       "3    0.010972  0.000704          1.0  02_HEATING_COOLING_COMBINED   \n",
       "4    0.000000  0.000000          1.0  02_HEATING_COOLING_COMBINED   \n",
       "..        ...       ...          ...                          ...   \n",
       "191       NaN       NaN          NaN                   99_UNKNOWN   \n",
       "192       NaN       NaN          NaN                   99_UNKNOWN   \n",
       "193       NaN       NaN          NaN                          NaN   \n",
       "194       NaN       NaN          NaN                          NaN   \n",
       "195       NaN       NaN          NaN                          NaN   \n",
       "\n",
       "     energy_no_energy  energy_yes_energy  sensor_no_sensor  ...            10  \\\n",
       "0                 0.0                1.0               0.0  ... -7.964367e-10   \n",
       "1                 0.0                1.0               0.0  ... -7.964367e-10   \n",
       "2                 0.0                1.0               0.0  ... -7.964367e-10   \n",
       "3                 0.0                1.0               0.0  ... -7.964367e-10   \n",
       "4                 0.0                1.0               0.0  ... -7.964367e-10   \n",
       "..                ...                ...               ...  ...           ...   \n",
       "191               0.0                0.0               0.0  ...  1.865562e-02   \n",
       "192               0.0                0.0               0.0  ...  1.865562e-02   \n",
       "193               NaN                NaN               NaN  ...  0.000000e+00   \n",
       "194               NaN                NaN               NaN  ...  0.000000e+00   \n",
       "195               NaN                NaN               NaN  ...  0.000000e+00   \n",
       "\n",
       "               11            12            13            14            15  \\\n",
       "0   -2.092877e-10 -5.341590e-10  1.625958e-10  1.614012e-10  2.083478e-10   \n",
       "1   -2.092877e-10 -5.341590e-10  1.625958e-10  1.614012e-10  2.083478e-10   \n",
       "2   -2.092877e-10 -5.341590e-10  1.625958e-10  1.614012e-10  2.083478e-10   \n",
       "3   -2.092877e-10 -5.341590e-10  1.625958e-10  1.614012e-10  2.083478e-10   \n",
       "4   -2.092877e-10 -5.341590e-10  1.625958e-10  1.614012e-10  2.083478e-10   \n",
       "..            ...           ...           ...           ...           ...   \n",
       "191  9.082309e-04  4.606625e-02  2.438907e-02  2.434749e-02 -8.801561e-04   \n",
       "192  9.082309e-04  4.606625e-02  2.438907e-02  2.434749e-02 -8.801561e-04   \n",
       "193  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "194  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "195  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "\n",
       "               16            17            18            19  \n",
       "0   -3.151098e-10 -2.244109e-10  1.296283e-09  6.172725e-10  \n",
       "1   -3.151098e-10 -2.244109e-10  1.296283e-09  6.172725e-10  \n",
       "2   -3.151098e-10 -2.244109e-10  1.296283e-09  6.172725e-10  \n",
       "3   -3.151098e-10 -2.244109e-10  1.296283e-09  6.172725e-10  \n",
       "4   -3.151098e-10 -2.244109e-10  1.296283e-09  6.172725e-10  \n",
       "..            ...           ...           ...           ...  \n",
       "191  1.339121e-03  1.712560e-02 -6.565314e-02  1.667654e-01  \n",
       "192  1.339121e-03  1.712560e-02 -6.565314e-02  1.667654e-01  \n",
       "193  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  \n",
       "194  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  \n",
       "195  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  \n",
       "\n",
       "[196 rows x 35 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step4_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUNNING STEP 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8 0 0 0 0 0]\n",
      " [0 3 0 0 0 0]\n",
      " [0 0 6 0 0 0]\n",
      " [0 0 0 8 0 0]\n",
      " [0 0 0 0 2 0]\n",
      " [0 0 1 0 0 5]]\n",
      "accuracy: 0.9696969696969697\n",
      "precision: 0.9740259740259739\n",
      "recall: 0.9696969696969697\n",
      "f1: 0.9694850603941513\n",
      "logloss: 0.22415006131462367\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fea10595950>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "    # 5) Classification model\n",
    "\n",
    "    #######################\n",
    "    #    a) Dataprep to get the step 4 data into an appropriate format for prediction\n",
    "    # TODO: Ensure that the step 4 output data is in the same format as the training_data dataframe (with the NRCan labels column as null for now)\n",
    "\n",
    "    #    b)\n",
    "    # TODO: Update read_csv() path to the correct location and file name for final product\n",
    "    # TODO: Remove the train_test_split() code for final product\n",
    "    # TODO: Ensure that the training_data dataframe is formated the same as the step 4 output (same as TODO for part a b/c it is that important)\n",
    "    training_data=step4_data\n",
    "#     training_data = pd.read_csv('~/data-599-capstone-ubc-urban-data-lab/code/step4_data.csv') # Reading in the training data file\n",
    "    # Manipulating dataset to be in the appropriate format for creating seperate predictor and response datasets\n",
    "    cols = training_data.columns.tolist()\n",
    "    cols.remove('endUseLabel')\n",
    "    cols.append('endUseLabel')\n",
    "    training_data = training_data[cols]\n",
    "    training_data = training_data.iloc[:,2:]\n",
    "    training_data = training_data.fillna(0)\n",
    "    # Extracting just the number from the label\n",
    "    training_data['endUseLabel'] = training_data['endUseLabel'].apply(lambda x: int(str(x)[0:2]))\n",
    "    training_data=training_data[(training_data['endUseLabel']!=99)]\n",
    "    \n",
    "    # Creating seperate predictor variable and response variable numpy arrays\n",
    "    x = training_data.iloc[:, :-1].values\n",
    "    y = training_data.iloc[:, -1].values\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0) ############### TEMP: For testing the model, remove for final product\n",
    "\n",
    "    #    b) Create and train the selected model\n",
    "    # Creating and fitting the classifier\n",
    "    # TODO: Test various models and select a final model\n",
    "    # TODO: Update model to the selected model (if not RandomForrestClassifier)\n",
    "    # TODO: Change x_train and y_train to x and y once final model has been selcted\n",
    "    classifier = RandomForestClassifier(n_estimators = 100, criterion = 'gini') # can include random_state=0 if we want to set the seed to be constant\n",
    "    classifier.fit(x_train, y_train) # Can change x_train and y_train to just x and y for final model\n",
    "\n",
    "    #    c) Predict the outputs for the new data\n",
    "    # Predicting the outputs\n",
    "    # TODO: Change x_test to whatever the numpy data to be predicted is (NOTE: Must have same format as the training_data dataframe)\n",
    "    y_pred = classifier.predict(x_test) # TODO: Change x_test to whatever the actual data to be predicted's variable for final model\n",
    "\n",
    "    ########################################################################################\n",
    "    ############### TEMP: Used to test the effectiveness of the chosen model ###############\n",
    "    ###############       To delete for the final model, just here so we can see some actual\n",
    "    ###############       output from step 5\n",
    "    from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, log_loss\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    logloss = log_loss(y_true=y_test, y_pred=classifier.predict_proba(x_test))\n",
    "    print(\"accuracy: \"+str(accuracy))\n",
    "    print(\"precision: \"+str(precision))\n",
    "    print(\"recall: \"+str(recall))\n",
    "    print(\"f1: \"+str(f1))\n",
    "    print(\"logloss: \"+str(logloss))\n",
    "    ############### TEMP: Used to test the effectiveness of the chosen model ###############\n",
    "    ########################################################################################\n",
    "\n",
    "    #    d) Join the predictions onto the  step 4 output dataframe\n",
    "    # TODO: Join the predictions onto the step 4 output dataframe (or populate the NRCan column with these values)\n",
    "\n",
    "    #    OUTPUT OF STEP = dataframe with EC sensor ID fields and end-use group\n",
    "\n",
    "########################################################################################\n",
    "############### TEMP: Used for calibrating dbscan and hdbscan clustering ###############\n",
    "###############       Can delete once final model selection and calibration complete\n",
    "###############                                     OR\n",
    "###############       once dbscan AND hdbscan dismissed as possible models\n",
    "from matplotlib import pyplot as plt\n",
    "sorted_vals = gow_dist[0]\n",
    "sorted_vals = sorted_vals[np.argsort(-gow_dist[0])]\n",
    "plt.plot(sorted_vals)\n",
    "\n",
    "testGow = pd.DataFrame(sorted_vals, columns=[\"values\"])\n",
    "testGow['values'].value_counts()#.sort_index()\n",
    "############### TEMP: Used for calibrating dbscan and hdbscan clustering ###############\n",
    "########################################################################################\n",
    "\n",
    "##################################################################\n",
    "############### TEMP: For visualizing the clusters ###############\n",
    "###############       Can delete after moved to a seperate file\n",
    "###############                             OR\n",
    "###############       once desired plots have been obtained for\n",
    "###############       the final report\n",
    "from matplotlib import pyplot as plt\n",
    "# Make Data into a Dataframe\n",
    "data_2d_df = pd.DataFrame(data=mds_data, columns = ['x','y'])\n",
    "data_2d_df['cluster'] = cluster_groups['cluster']\n",
    "\n",
    "# Plotting the dbscan clusters (Fit pre-MDS)\n",
    "plt.scatter(data_2d_df['x']*1000,data_2d_df['y']*1000, c=data_2d_df['cluster'])\n",
    "############### TEMP: For visualizing the clusters ###############\n",
    "##################################################################\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
