{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURE SELECTION\n",
    "The purpose of this jupyter notebook is to exemplify how feature selection was accomplished.   \n",
    "\n",
    "Feature selection for categorical fields used the Mutual Information method and feature selection for continuous fields used the ANOVA method. Cross-validation was used to identify the most optimal value for k in both methods.   \n",
    "\n",
    "The first part of the notebook runs a few processes from main.py in order to create the data needed for feature selection. Parts of Step 2 is used to create the `ec_data2` dataframe and parts of Step 3 is used to merge `ec_data2` with metadata csv and training set csv to create the `merged_outer` dataframe.\n",
    "\n",
    "**Limitations**\n",
    "* The process of feature selection is not dynamic in main.py. The reason is we had to implement two separate feature selection techniques for categorical and continuous data. We had to remove unique sensor ID for categorical feature selection and for that reason we were unable to join back continous data. \n",
    "\n",
    "**Future Work**  \n",
    "To apply feature selection for future buildings, it is recommended to:\n",
    "* Update QUERY_SITE variable, metadata csv, and training set csv if applicable\n",
    "* Modify `data_preparation.equip_label()` and `data_preparation.nav_label()` functions to capture additional categorical levels if \"NEED_TO_LABEL\" is outputted as a recommended categorical feature "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ~ Library Imports ~ ###\n",
    "# General Imports\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from datetime import datetime, date, timedelta\n",
    "# Data Formatting and Manipulation Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Feature Selection Imports\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import RFECV, SelectKBest, f_classif, mutual_info_classif\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Clustering Step Imports\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "# Regression Step Imports\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# Modules Developed for this Project Imports\n",
    "import data_preparation\n",
    "import aggregation\n",
    "import clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Parts of Steps 2 and Step 3 from main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################################\n",
    "################################## SETTTING CONSTANTS #######################################\n",
    "#############################################################################################\n",
    "\n",
    "### ~ Required for base functionality ~ ###\n",
    "# String defining which site to run the model for\n",
    "QUERY_SITE = 'Pharmacy'\n",
    "# String defining which measurement from the database to query\n",
    "MEASUREMENT='UBC_EWS'\n",
    "# String defining the path to the metadata csv for the given building\n",
    "METADATA_CSV_PATH = '../data/PharmacyQuery.csv'\n",
    "# String defining the path to the training dataset\n",
    "TRAINING_SET_PATH = '../data/FinalPharmacyECSensorList-WithLabels - PharmacyECSensorsWithLabels.csv'\n",
    "# List of indices that can be combined to uniquely identify a sensor (used to group on each sensors)\n",
    "SENSOR_ID_TAGS = [1,2,3,4,5,6] # order is [\"groupRef\",\"equipRef\",\"navName\",\"siteRef\",\"typeRef\",\"unit\"]\n",
    "                               # The planned update to the InfluxDB may change SENSOR_ID_TAGS to only [1] as in [\"uniqueID\"]\n",
    "### ~ Alows customization of outputs ~ ###\n",
    "# Boolean defining if the output dataframes from each step should be saved (save if True, else False)\n",
    "SAVE_STEP_OUTPUTS = True\n",
    "# Strings defining the location to save the dataframe csv's\n",
    "STEP1_SAVE_PATH = '../data/csv_outputs/step1_clustering_phase_output.csv'\n",
    "STEP2_SAVE_PATH_FINAL_DF = '../data/csv_outputs/step2_ridge_regression_output.csv'\n",
    "STEP2_SAVE_PATH_EC_DATA2 = '../data/csv_outputs/step2_aggregated_ec_data_output.csv'\n",
    "# Boolean defining if the model should query from the database or pull from csv's (from database if True, else False)\n",
    "QUERY_FROM_DB = False\n",
    "# Strings defining the start and end date fo the date range to query (if QUERY_FROM_DB==True)\n",
    "START_DATE = '2020-03-16'\n",
    "END_DATE = '2020-03-17'\n",
    "# Strings containing the paths to the folders that contains the csv's to pull data from if QUERY_FROM_DB==False\n",
    "# All file names within the folders must be formatted as \"YYYY-MM-DD.csv\"\n",
    "QUERY_CSV_PATH = '../data/sensor_data/'\n",
    "QUERY_WEATHER_CSV_PATH = '../data/weather_data/'\n",
    "\n",
    "if QUERY_FROM_DB:\n",
    "    # Generating a list of the dates within the daterange to query (if QUERY_FROM_DB==True)\n",
    "    DELTA = (datetime.strptime(END_DATE, \"%Y-%m-%d\")-datetime.strptime(START_DATE, \"%Y-%m-%d\")).days\n",
    "    DATELIST =  [(datetime.strptime(END_DATE, \"%Y-%m-%d\") - timedelta(days=x)).strftime('%Y-%m-%d') for x in range(0,DELTA+1)]\n",
    "else:\n",
    "    # Getting the list of files stored in the path provided in QUERY_CSV_PATH\n",
    "    # All files names must be formatted as \"YYYY-MM-DD.csv\"\n",
    "    DATELIST = [f.split(\".\")[0] for f in listdir(QUERY_CSV_PATH) if isfile(join(QUERY_CSV_PATH, f))]\n",
    "\n",
    "# Connecting to influxDB\n",
    "if QUERY_FROM_DB:\n",
    "    client = data_preparation.connect_to_db()\n",
    "\n",
    "\n",
    "#############################################################################################\n",
    "############################### STEP 2) Model EC/NC  ########################################\n",
    "#############################################################################################\n",
    "\n",
    "print(\"####### ~~~~~ Starting - Step 2: Model EC/NC Relationship ~~~~~ #######\") # For tracking program progress\n",
    "###   a) Aggregate EC sensors to be response variables for the regression model\n",
    "print(\"\\t##### ~~~ Started - Step 2 a): Aggregation ~~~ #####\") # For tracking program progress\n",
    "last_idx_as_cols = False\n",
    "is_first_iter = True\n",
    "cnt=1\n",
    "for day in DATELIST:\n",
    "    print(\"\\t\\t\"+str(cnt)+\": \"+str(day)) # For tracking program progress\n",
    "    # Querying and preping data for aggregations\n",
    "    if QUERY_FROM_DB:\n",
    "        temp_df2 = data_preparation.query_db_ec(client, day, measurement=MEASUREMENT, num_days=1, site=QUERY_SITE)\n",
    "        if temp_df2 is not None:\n",
    "            # Making the datetime index into a column so that date and hour can be extracted later\n",
    "            temp_df2.reset_index(level=0, inplace=True)\n",
    "    else:\n",
    "        temp_df2 = data_preparation.query_csv(client=QUERY_CSV_PATH, date=day, site=None)\n",
    "        # Filter for EC data, this step will be done in the query\n",
    "        if temp_df2 is not None:\n",
    "            temp_df2=temp_df2[(temp_df2['unit']=='kWh') | (temp_df2['unit']=='mÂ³')]\n",
    "    if temp_df2 is None:\n",
    "        continue\n",
    "    col_names = ['datetime']\n",
    "    col_names.extend(temp_df2.columns[1:])\n",
    "    temp_df2.columns = col_names\n",
    "    temp_df2 = aggregation.split_datetime(temp_df2)\n",
    "    # Creating uniqueId\n",
    "    temp_df2=data_preparation.create_unique_id(temp_df2)\n",
    "    # Filtering dataframe for only relevant fields\n",
    "    temp_df2=temp_df2[['uniqueId', 'date', 'hour', 'unit', 'value']]\n",
    "    if is_first_iter:\n",
    "        # Creating a low memory dataframe for the append_agg function before the structure is changed by agg_all\n",
    "        struct_df2 = temp_df2.head(1)\n",
    "        # Aggregating the first date's data\n",
    "        ec_data1=aggregation.agg_numeric_by_col(temp_df2, col_idx=[0,1,2,3], how='mean')\n",
    "        # Also create second DF by aggregating further just using sensor ID fields (end result=1row per sensor)\n",
    "        ec_data2=aggregation.agg_numeric_by_col(temp_df2, col_idx=[0,3], how='all')\n",
    "        is_first_iter = False\n",
    "    else:\n",
    "        # Aggregating the current date's data and aggregate it with the current running total\n",
    "        temp_df2a=aggregation.agg_numeric_by_col(temp_df2, col_idx=[0,1,2,3], how='mean')\n",
    "        temp_df2b=aggregation.agg_numeric_by_col(temp_df2, col_idx=[0,3], how='all')\n",
    "        ec_data1=aggregation.append_agg(df1=temp_df2a, df2=ec_data1, struct_df=struct_df2, col_idx=[0,1,2,3])\n",
    "        ec_data2=aggregation.append_agg(df1=temp_df2b, df2=ec_data2, struct_df=struct_df2, col_idx=[0,3])\n",
    "    cnt += 1\n",
    "# Freeing up some memory\n",
    "temp_df2 = None\n",
    "temp_df2a = None\n",
    "temp_df2b = None\n",
    "# Calculating the update rate\n",
    "ec_data2[\"update_rate\"] = ec_data2[\"count\"] / (cnt*24)\n",
    "ec_data2.drop(\"count\", inplace=True, axis=1)\n",
    "\n",
    "# Resetting index columns\n",
    "ec_data2=ec_data2.reset_index()\n",
    "\n",
    "if SAVE_STEP_OUTPUTS:\n",
    "    ec_data2.to_csv(STEP2_SAVE_PATH_EC_DATA2)\n",
    "\n",
    "print(\"####### ~~~~~ Complete - Step 2: Model EC/NC Relationship ~~~~~ #######\") # For tracking program progress\n",
    "\n",
    "    \n",
    "#############################################################################################\n",
    "#################### STEP 3) Prep EC data for classification model  #########################\n",
    "#############################################################################################\n",
    "\n",
    "print(\"####### ~~~~~ Starting - Step 3: Prep EC Data for Classification Model ~~~~~ #######\") # For tracking program progress\n",
    "###   a) Load metadata and join with 2b_EC_data_df\n",
    "metadata=pd.read_csv(METADATA_CSV_PATH, dtype=object)\n",
    "# Make uniqueIDs\n",
    "metadata=data_preparation.create_unique_id(metadata, metadata=True)\n",
    "# Drop duplicates\n",
    "metadata=metadata.sort_values('lastSynced').drop_duplicates('uniqueId',keep='last')\n",
    "# Choose relevant fields\n",
    "metadata=metadata[['uniqueId','kind', 'energy','power', 'sensor', 'unit', 'water']]\n",
    "# Changing boolean to easily identify during encoding process\n",
    "metadata['energy']=metadata['energy'].apply(lambda x: 'yes_energy' if x=='â' else 'no_energy')\n",
    "metadata['power']=metadata['power'].apply(lambda x: 'yes_power' if x=='â' else 'no_power')\n",
    "metadata['sensor']=metadata['sensor'].apply(lambda x: 'yes_sensor' if x=='â' else 'no_sensor')\n",
    "metadata['water']=metadata['water'].apply(lambda x: 'yes_water' if x=='â' else 'no_water')\n",
    "metadata['unit']=metadata['unit'].apply(lambda x: 'omit' if x=='_' else x)\n",
    "# left join metadata and 2b_EC_data_df\n",
    "merged_left=pd.merge(ec_data2, metadata, how='left', left_on='uniqueId', right_on='uniqueId')\n",
    "\n",
    "###   b) Apply feature selection function(s) to the joined EC+metadata\n",
    "# load end_use classifications training data\n",
    "end_use_labels=pd.read_csv(TRAINING_SET_PATH)\n",
    "# make uniqueId\n",
    "end_use_labels['siteRef']=QUERY_SITE\n",
    "end_use_labels=data_preparation.create_unique_id(end_use_labels)\n",
    "\n",
    "# rename columns to fix unit of measurements\n",
    "end_use_labels.rename(columns={'UBC_EWS.firstValue':'value'}, inplace=True)\n",
    "# run correct_df_units function\n",
    "end_use_labels=data_preparation.correct_df_units(end_use_labels)\n",
    "\n",
    "# TRAINING DATA CLEANING\n",
    "# Change ? to 0 since uom fixed\n",
    "end_use_labels=end_use_labels.assign(isGas=end_use_labels.isGas.apply(lambda x: '0' if x=='?' else x))\n",
    "# changing boolean for more descriptive encoding\n",
    "end_use_labels=end_use_labels.assign(isGas=end_use_labels.isGas.apply(lambda x: 'no_gas' if x=='0' else 'yes_gas'))\n",
    "\n",
    "# selecting relevant training data fields\n",
    "end_use_labels=end_use_labels[['uniqueId', 'isGas', 'equipRef', 'groupRef', 'navName', 'endUseLabel']]\n",
    "end_use_labels=end_use_labels.drop_duplicates()\n",
    "merged_outer=pd.merge(left=merged_left, right=end_use_labels, how='left', left_on='uniqueId', right_on='uniqueId')\n",
    "# make equipRef and navName into smaller categories for feature engineering\n",
    "merged_outer=merged_outer.assign(equipRef=merged_outer.equipRef.apply(lambda x: data_preparation.equip_label(str(x))))\n",
    "merged_outer=merged_outer.assign(navName=merged_outer.navName.apply(lambda x: data_preparation.nav_label(str(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CATEGORICAL FEATURE SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################\n",
    "####################### CATEGORICAL FEATURE SELECTION ####################\n",
    "##########################################################################\n",
    "categorical=merged_outer[['energy', 'power', 'sensor', 'unit', 'water', 'isGas', 'equipRef', 'navName', 'endUseLabel']].copy()\n",
    "categorical=categorical[(categorical['unit']=='kWh') | (categorical['unit']=='mÂ³')]\n",
    "categorical=categorical.drop(['unit'], axis=1)\n",
    "\n",
    "##########################################################################\n",
    "###################### Preprocessing of Data #############################\n",
    "##########################################################################\n",
    "#### Split data into training and test data\n",
    "dataset = categorical.values\n",
    "X = dataset[:, :-1]\n",
    "y = dataset[:,-1]\n",
    "X=X.astype(str)\n",
    "y=y.astype(str)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.42, random_state=1)\n",
    "\n",
    "#### Encode categorical data\n",
    "oe = OneHotEncoder(handle_unknown='ignore')\n",
    "oe.fit(X_train)\n",
    "X_train_enc = oe.transform(X_train)\n",
    "X_test_enc = oe.transform(X_test)\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(y_train)\n",
    "y_train_enc = le.transform(y_train)\n",
    "y_test_enc = le.transform(y_test)\n",
    "##########################################################################\n",
    "################### Done with Preprocessing of Data ######################\n",
    "##########################################################################\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "############# Choosing K=max_feature through Cross-Validation #############\n",
    "##########################################################################\n",
    "\n",
    "### Create the RFE object and compute a cross-validated score.\n",
    "svc = SVC(kernel=\"linear\")\n",
    "### The \"accuracy\" scoring is proportional to the number of correct classifications\n",
    "rfecv = RFECV(estimator=svc, step=1, cv=StratifiedKFold(2),\n",
    "              scoring='accuracy')\n",
    "rfecv.fit(X_train_enc, y_train_enc)\n",
    "\n",
    "### Create dataframe with scroes and number of features\n",
    "Scores=pd.DataFrame({'scores':rfecv.grid_scores_,'num_features':range(1, len(rfecv.grid_scores_) + 1)})\n",
    "\n",
    "### Choosing max number of features depending on how many features are recommended\n",
    "max_feature=rfecv.n_features_\n",
    "\n",
    "print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "\n",
    "##### Plot Cross-Validation Scores and Number of Features\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "plt.title(\"Cross-Validation Plot to Decide K (Categorical Data)\")\n",
    "plt.show()    \n",
    "##########################################################################\n",
    "###################### Done with Cross-Validation ########################\n",
    "##########################################################################\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "###########  Mutual Information Technique using K=max_feature #############\n",
    "##########################################################################\n",
    "fs = SelectKBest(score_func=mutual_info_classif, k=int(max_feature))\n",
    "fs.fit(X_train_enc, y_train_enc)\n",
    "X_train_fs = fs.transform(X_train_enc)\n",
    "X_test_fs = fs.transform(X_test_enc)\n",
    "\n",
    "### Storing all feature labels\n",
    "all_feature_names=oe.get_feature_names()\n",
    "all_features_count=len(all_feature_names)\n",
    "\n",
    "### Storing recommended feature labels\n",
    "fs.get_support(indices=True)\n",
    "feature_names = [all_feature_names[i] for i in fs.get_support(indices=True)]\n",
    "if feature_names:\n",
    "    feature_names = np.asarray(feature_names)\n",
    "\n",
    "### Modifying feature labels to include original field names\n",
    "new_feature_names=[]\n",
    "for i in range(len(categorical.columns)+1):\n",
    "    if feature_names[i][1:2]==str(0):\n",
    "        new_feature_names.append(feature_names[i].replace(feature_names[i][:2], categorical.columns[0]))\n",
    "    elif feature_names[i][1:2]==str(1):\n",
    "        new_feature_names.append(feature_names[i].replace(feature_names[i][:2], categorical.columns[1]))\n",
    "    elif feature_names[i][1:2]==str(2):\n",
    "        new_feature_names.append(feature_names[i].replace(feature_names[i][:2], categorical.columns[2]))\n",
    "    elif feature_names[i][1:2]==str(3):\n",
    "        new_feature_names.append(feature_names[i].replace(feature_names[i][:2], categorical.columns[3]))\n",
    "    elif feature_names[i][1:2]==str(4):\n",
    "        new_feature_names.append(feature_names[i].replace(feature_names[i][:2], categorical.columns[4]))\n",
    "    elif feature_names[i][1:2]==str(5):\n",
    "        new_feature_names.append(feature_names[i].replace(feature_names[i][:2], categorical.columns[5]))\n",
    "    elif feature_names[i][1:2]==str(6):\n",
    "        new_feature_names.append(feature_names[i].replace(feature_names[i][:2], categorical.columns[6]))\n",
    "    elif feature_names[i][1:2]==str(7):\n",
    "        new_feature_names.append(feature_names[i].replace(feature_names[i][:2], categorical.columns[7]))\n",
    "\n",
    "print('Original number of features: {:.0f}'.format(all_features_count))\n",
    "print('')\n",
    "print('These are the {:.0f} recommended categorical features:'.format(max_feature))\n",
    "print('')\n",
    "print(new_feature_names)\n",
    "print('')\n",
    "\n",
    "##########################################################################\n",
    "##################### Done with Mutual Information #######################\n",
    "##########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONTINUOUS FEATURE SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################\n",
    "######################## NUMERICAL FEATURE SELECTION #####################\n",
    "##########################################################################\n",
    "\n",
    "##########################################################################\n",
    "###################### Preprocessing of Data #############################\n",
    "##########################################################################\n",
    "numerical=merged_outer[['mean','std','max','min','update_rate','endUseLabel']]\n",
    "numerical=numerical.dropna(axis=0) # drop rows if they contain nan for enduselabel, because not part of training data\n",
    "dataset = numerical.values\n",
    "X = pd.DataFrame(dataset[:, :-1], columns=numerical.columns[0:5])\n",
    "y = dataset[:,-1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "##########################################################################\n",
    "################### Done with Preprocessing of Data ######################\n",
    "##########################################################################\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "################# Choosing K through Cross-Validation ####################\n",
    "##########################################################################\n",
    "rfc = RandomForestClassifier(random_state=101)\n",
    "rfecv = RFECV(estimator=rfc, step=1, cv=StratifiedKFold(2), scoring='accuracy')\n",
    "rfecv.fit(X_train, y_train)\n",
    "\n",
    "print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "\n",
    "# Plot number of features VS. cross-validation scores\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "plt.title(\"Cross-Validation Plot to Decide K (Numerical Data)\")\n",
    "plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "plt.show()\n",
    "##########################################################################\n",
    "###################### Done with Cross-Validation ########################\n",
    "##########################################################################\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "#################  ANOVA F-Score Technique using K #######################\n",
    "##########################################################################\n",
    "fvalue_selector = SelectKBest(f_classif, k=rfecv.n_features_)\n",
    "# Apply the SelectKBest object to the features and target\n",
    "X_kbest = fvalue_selector.fit_transform(X, y)\n",
    "# Create and fit selector\n",
    "cols = fvalue_selector.get_support(indices=True)\n",
    "numerical_features=X.iloc[:,cols].columns.tolist()\n",
    "# Show results\n",
    "print('Original number of features:', X.shape[1])\n",
    "print('')\n",
    "print('These are the {:.0f} recommended numerical features:'.format(X.shape[1]))\n",
    "print('')\n",
    "print(numerical_features)\n",
    "##########################################################################\n",
    "############################ Done with ANOVA #############################\n",
    "##########################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
