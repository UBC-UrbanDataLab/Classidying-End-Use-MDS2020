{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### ~~~~~ Started - Step 1: Clustering Phase ~~~~~ #######\n",
      "\t##### ~~~ Started - Step 1 a): Aggregation Phase 1 ~~~ #####\n",
      "\t\t1: 2020-03-16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t2: 2020-05-01\n",
      "\t\t### ~ Started - Step 1 a): Agg Phase 1: Calculating Update Rates ~ ###\n",
      "\t##### ~~~ Step 1 b): Started - Clustering Phase ~~~ #####\n",
      "\t\t### ~ Started - Step 1 c): Clust Phase 1: Calculating Gower's Distance ~ ###\n",
      "\t\t### ~ Started - Step 1 c): Clust Phase 3: Calculating Clusters ~ ###\n",
      "\t##### ~~~ Started - Step 1 d): Aggregation Phase 2 ~~~ #####\n",
      "\t\t1: 2020-03-16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t2: 2020-05-01\n",
      "\t\t### ~ Started - Step 1 d): Agg Phase 2: Calculating Update Rates ~ ###\n",
      "####### ~~~~~ Complete - Step 1: NC Aggregation and Clustering Phase ~~~~~ #######\n",
      "####### ~~~~~ Starting - Step 2: Model EC/NC Relationship ~~~~~ #######\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/async_helpers.py:68: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  coro.send(None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_mse: 8.203498865179609e-11\n",
      "####### ~~~~~ Complete - Step 2: Model EC/NC Relationship ~~~~~ #######\n",
      "####### ~~~~~ Starting - Step 3: Prep EC Data for Classification Model ~~~~~ #######\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3254: DtypeWarning: Columns (2,54,81,118,133,140,149) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-6f189ee2faad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    593\u001b[0m     \u001b[0;31m##########################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-56-6f189ee2faad>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;31m##########################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[0mcategorical\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmerged_outer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'energy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'power'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sensor'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'unit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'water'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'isGas'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'equipRef'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'navName'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'endUseLabel'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m     \u001b[0mcategorical\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategorical\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'unit'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'kWh'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcategorical\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'unit'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'mÂ³'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m     \u001b[0mcategorical\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'unit'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__nonzero__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1477\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1478\u001b[0m         raise ValueError(\n\u001b[0;32m-> 1479\u001b[0;31m             \u001b[0;34mf\"The truth value of a {type(self).__name__} is ambiguous. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1480\u001b[0m             \u001b[0;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "### ~ Library Imports ~ ###\n",
    "# General Imports\n",
    "from datetime import datetime, date, timedelta\n",
    "# Data Formatting and Manipulation Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Feature Selection Imports\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import RFECV, SelectKBest, f_classif, mutual_info_classif\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Clustering Step Imports\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "# Regression Step Imports\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# Supervised Classification Step Imports\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "# Project Module Imports\n",
    "import data_preparation\n",
    "import aggregation\n",
    "import clustering\n",
    "    \n",
    "\n",
    "\n",
    "def main():\n",
    "    #0) Set Constants (remember, constants are named in all caps with underscores between words)\n",
    "    display_prediction_metrics = True # Set True to display prediciton metrics (confusion matrix, accuracy, precission, recall, f1 score, logloss), else set False\n",
    "    # Getting a list of the last 90 dates\n",
    "    DATELIST =  [(date.today() + timedelta(days=x)).strftime('%Y-%m-%d') for x in range(91)] # NOTE: To update with the number of days desired to pull data for (currently has 91)\n",
    "    DATELIST.sort(key=lambda date: datetime.strptime(date, \"%Y-%m-%d\"))\n",
    "\n",
    "    ##############################################################################################################\n",
    "    ############### TEMP: for testing from csvs to delete once actual querying code is implemented ###############\n",
    "    ###############       (gets list of file names in the given path)\n",
    "    from os import listdir\n",
    "    from os.path import isfile, join\n",
    "    mypath = 'test_data'\n",
    "    DATELIST = [f.split(\".\")[0] for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "    DATELIST = DATELIST[1:3]\n",
    "    ############### TEMP: for testing from csvs to delete once actual querying code is implemented ###############\n",
    "    ##############################################################################################################\n",
    "\n",
    "    SENSOR_ID_TAGS = [1,2,3,4,5,6] # order is [\"groupRef\",\"equipRef\",\"navName\",\"siteRef\",\"typeRef\",\"unit\"]\n",
    "\n",
    "    # The planned update to the InfluxDB may change SENSOR_ID_TAGS to only [1] as in [\"uniqueID\"]\n",
    "\n",
    "    #1) Cluster NC data\n",
    "    ###################\n",
    "    print(\"####### ~~~~~ Started - Step 1: Clustering Phase ~~~~~ #######\") ############### TEMP: For Tracking test progress\n",
    "    # a) load+aggregate NC data (including weather), grouping by sensor ID fields [and 'unit'?]\n",
    "    print(\"\\t##### ~~~ Started - Step 1 a): Aggregation Phase 1 ~~~ #####\") ############### TEMP: For Tracking test progress\n",
    "    last_idx_as_cols = False\n",
    "    is_first_iter = True\n",
    "    cnt=1\n",
    "    for day in DATELIST:\n",
    "        print(\"\\t\\t\"+str(cnt)+\": \"+str(day)) ############### TEMP: For Tracking test progress\n",
    "        # Querying and preping data for aggregations\n",
    "        temp_df = data_preparation.query_csv(client=None, date=day, site=None) ############### TEMP: To be replaced by actual query functions in final product\n",
    "        weather_df = data_preparation.query_weather_csv(client=None, date=day, site=None) ############### TEMP: To be replaced by actual query functions in final product\n",
    "        if weather_df is None: ############### TEMP: To be replaced by actual query functions in final product\n",
    "            pass\n",
    "        else:\n",
    "            temp_df = pd.concat([temp_df, weather_df]) ############### TEMP: To be replaced by actual query functions in final product\n",
    "            temp_df = temp_df.fillna('empty') ############### TEMP: To be replaced by actual query functions in final product # Aggregation doesn't work with nan's, used empty as an obvious flag for value being nan\n",
    "        col_names = ['datetime']\n",
    "        col_names.extend(temp_df.columns[1:])\n",
    "        temp_df.columns = col_names\n",
    "        if temp_df is None:\n",
    "            continue\n",
    "        temp_df = aggregation.split_datetime(temp_df)\n",
    "        if is_first_iter:\n",
    "            # Creating a low memory dataframe for the append_agg function before the structure is changed by agg_all\n",
    "            struct_df = temp_df.head(1)\n",
    "            # Aggregating the first date's data\n",
    "            nc_data = aggregation.agg_all(temp_df, how=\"all\", col_idx=SENSOR_ID_TAGS, last_idx_to_col=last_idx_as_cols)\n",
    "            is_first_iter = False\n",
    "        else:\n",
    "            # Aggregating the current date's data and aggregate it with the current running total\n",
    "            temp_df = aggregation.agg_all(temp_df, how=\"all\", col_idx=SENSOR_ID_TAGS, last_idx_to_col=last_idx_as_cols)\n",
    "            nc_data = aggregation.append_agg(df1=temp_df, df2=nc_data, struct_df=struct_df, col_idx=SENSOR_ID_TAGS, last_idx_to_col=last_idx_as_cols)\n",
    "        cnt += 1\n",
    "        #if cnt == 15: ############### TEMP: For speeding up testing of updated code for main function delete once updates confirmed to work\n",
    "        #    break ############### TEMP: For speeding up testing of updated code for main function delete once updates confirmed to work\n",
    "\n",
    "    print(\"\\t\\t### ~ Started - Step 1 a): Agg Phase 1: Calculating Update Rates ~ ###\") ############### TEMP: For Tracking test progress\n",
    "    # Freeing up some memory\n",
    "    temp_df = None\n",
    "    weather_df = None\n",
    "    # Calculating the update rate\n",
    "    nc_data[\"update_rate\"] = nc_data[\"count\"] / cnt\n",
    "    nc_data.drop(\"count\", inplace=True, axis=1)\n",
    "\n",
    "    #nc_data.to_csv('aggregated_no_units_data.csv') ############### TEMP: Write aggregation output to file to provide sample data for testing step 2, remove for final model\n",
    "\n",
    "    # b) Encode and scale NC data\n",
    "    # TODO: Look up the correct function name for fixing units of measurement (getting added to the query function, can remove/update to DONE once confirmed complete)\n",
    "    # TODO: clean and correct units of measurement (getting added to the query function, can remove/update to DONE once confirmed complete)\n",
    "    print(\"\\t##### ~~~ Step 1 b): Started - Clustering Phase ~~~ #####\") ############### TEMP: For Tracking test progress\n",
    "\n",
    "    # Getting Indexes of the continuous columns\n",
    "    cont_cols = [i for i in range(len(SENSOR_ID_TAGS),len(nc_data.columns))]\n",
    "\n",
    "    # Scale Continuous\n",
    "    scaled_data = data_preparation.scale_continuous(nc_data, cont_cols)\n",
    "    nc_data = pd.concat([nc_data.iloc[:, 0:len(SENSOR_ID_TAGS)], pd.DataFrame(scaled_data, index=nc_data.index, columns=nc_data.columns[cont_cols].tolist())], axis=1)\n",
    "    scaled_data = None\n",
    "    \n",
    "    # Encoding units\n",
    "    nc_data = data_preparation.encode_units(nc_data)\n",
    "\n",
    "    # c) cluster NC data to get df of sensor id fields + cluster group number\n",
    "    # Calculating Gower's Distance, MDS, and clustering\n",
    "    print(\"\\t\\t### ~ Started - Step 1 c): Clust Phase 1: Calculating Gower's Distance ~ ###\") ############### TEMP: For Tracking test progress\n",
    "    gow_dist = clustering.calc_gowers(nc_data, cont_cols)\n",
    "    \n",
    "    ###################################################################\n",
    "    ############### NOTE: Doesn't look like we need MDS ###############\n",
    "    ##########    keeping it here for now just incase the scaled up model\n",
    "    ##########    performs worse without, but I don't see why it should\n",
    "    #print(\"\\t\\t### ~ Started - Step 1 c): Clust Phase 2: Calculating MDS ~ ###\") ############### TEMP: For Tracking test progress\n",
    "    #mds_data = clustering.multidim_scale(gow_dist, num_dim=2)\n",
    "    #clusters = AgglomerativeClustering(linkage = 'single', n_clusters=20).fit_predict(mds_data)\n",
    "    ############### NOTE: Doesn't look like we need MDS ###############\n",
    "    ###################################################################\n",
    "    \n",
    "    print(\"\\t\\t### ~ Started - Step 1 c): Clust Phase 3: Calculating Clusters ~ ###\") ############### TEMP: For Tracking test progress\n",
    "    clusters = AgglomerativeClustering(linkage = 'single', affinity='precomputed', n_clusters=20).fit_predict(gow_dist)\n",
    "\n",
    "    # Generating a list of the columns to keep when making the dataframe relating sensors to clusters(the unique identifiers for an NC sensor and cluster)\n",
    "    unique_cols_idx = [i for i in range(len(SENSOR_ID_TAGS))]\n",
    "    unique_cols = nc_data.columns[unique_cols_idx].values.tolist()\n",
    "    unique_cols.append(\"cluster\")\n",
    "    # Creating dataframe that identifies which unique sensors belong to which cluster\n",
    "    drop_cols = list(set(nc_data.columns.tolist())-set(unique_cols))\n",
    "    cluster_groups = pd.concat([nc_data, pd.DataFrame(clusters, columns=[\"cluster\"])], axis=1)\n",
    "    cluster_groups = cluster_groups.drop(drop_cols, axis=1)\n",
    "\n",
    "    # d) Reload NC data + join cluster group num + aggregate, this time grouping by date, time, and clust_group_num\n",
    "    print(\"\\t##### ~~~ Started - Step 1 d): Aggregation Phase 2 ~~~ #####\") ############### TEMP: For Tracking test progress\n",
    "    last_idx_as_cols = True\n",
    "    is_first_iter = True\n",
    "    cnt=1\n",
    "    for day in DATELIST:\n",
    "        print(\"\\t\\t\"+str(cnt)+\": \"+str(day)) ############### TEMP: For Tracking test progress\n",
    "        # Querying and preping data for aggregations\n",
    "        temp_df = data_preparation.query_csv(client=None, date=day, site=None)\n",
    "        weather_df = data_preparation.query_weather_csv(client=None, date=day, site=None) ############### TEMP: To be replaced by actual query functions in final product\n",
    "        if weather_df is None: ############### TEMP: To be replaced by actual query functions in final product\n",
    "            pass\n",
    "        else:\n",
    "            temp_df = pd.concat([temp_df, weather_df]) ############### TEMP: To be replaced by actual query functions in final product\n",
    "            temp_df = temp_df.fillna('empty') ############### TEMP: To be replaced by actual query functions in final product # Aggregation doesn't work with nan's, used empty as an obvious flag for value being nan\n",
    "        \n",
    "        col_names = ['datetime']\n",
    "        col_names.extend(temp_df.columns[1:])\n",
    "        temp_df.columns = col_names\n",
    "        if temp_df is None:\n",
    "            continue\n",
    "        temp_df = aggregation.split_datetime(temp_df) # Added to create month and hour columns (must have at least hour for aggs)\n",
    "        temp_df = temp_df.merge(cluster_groups, how='left', on=cluster_groups.columns[:-1].tolist())\n",
    "        if is_first_iter:\n",
    "            # Calculating the count of sensor updates per hour per day per cluster for the first date\n",
    "            update_rates = temp_df.groupby(['hour','date','cluster']).agg({'value':'count'},axis=1)\n",
    "            # Creating a low memory dataframe for the append_agg function before the structure is changed by agg_all (different structure required than previous)\n",
    "            struct_df = temp_df.head(1)\n",
    "            # Identifying the indexes of the items being aggregated on (hour, date, and cluster)\n",
    "            cluster_id_tags = [temp_df.columns.tolist().index(\"hour\"), temp_df.columns.tolist().index(\"date\"), temp_df.columns.tolist().index(\"cluster\")]\n",
    "            # Aggregating the first date's data (Aggregations must be seperate b/c can't data gets too big during calculations if not)\n",
    "            temp_df_aggs = aggregation.agg_all(temp_df, how=\"mean\", col_idx=cluster_id_tags, last_idx_to_col=last_idx_as_cols)\n",
    "            if 'count_x' in temp_df_aggs.columns.tolist():\n",
    "                # Each aggregation type outputs a count, ensuring joins only result in 1 count (applies for all similar if statements)\n",
    "                temp_df_aggs = temp_df_aggs.drop('count_y', axis=1)\n",
    "                temp_df_aggs = temp_df_aggs.rename(columns={'count_x':'count'})\n",
    "            temp_df_aggs = temp_df_aggs.merge(aggregation.agg_all(temp_df, how=\"std\", col_idx=cluster_id_tags, last_idx_to_col=last_idx_as_cols), how='left', on=temp_df_aggs.columns.tolist()[:2])\n",
    "            if 'count_x' in temp_df_aggs.columns.tolist():\n",
    "                temp_df_aggs = temp_df_aggs.drop('count_y', axis=1)\n",
    "                temp_df_aggs = temp_df_aggs.rename(columns={'count_x':'count'})\n",
    "            temp_df_aggs = temp_df_aggs.merge(aggregation.agg_all(temp_df, how=\"max\", col_idx=cluster_id_tags, last_idx_to_col=last_idx_as_cols), how='left', on=temp_df_aggs.columns.tolist()[:2])\n",
    "            if 'count_x' in temp_df_aggs.columns.tolist():\n",
    "                temp_df_aggs = temp_df_aggs.drop('count_y', axis=1)\n",
    "                temp_df_aggs = temp_df_aggs.rename(columns={'count_x':'count'})\n",
    "            nc_data = temp_df_aggs.merge(aggregation.agg_all(temp_df, how=\"min\", col_idx=cluster_id_tags, last_idx_to_col=last_idx_as_cols), how='left', on=temp_df_aggs.columns.tolist()[:2])\n",
    "            if 'count_x' in nc_data.columns.tolist():\n",
    "                nc_data = nc_data.drop('count_y', axis=1)\n",
    "                nc_data = nc_data.rename(columns={'count_x':'count'})\n",
    "            is_first_iter = False\n",
    "        else:\n",
    "            # Calculating the count of sensor updates per hour per day per cluster for the current date\n",
    "            update_rates = pd.concat([update_rates, temp_df.groupby(['hour','date','cluster']).agg({'value':'count'},axis=1)])\n",
    "            # Aggregating the current date's data and aggregate it with the current running total\n",
    "            temp_df_aggs = aggregation.agg_all(temp_df, how=\"mean\", col_idx=cluster_id_tags, last_idx_to_col=last_idx_as_cols)\n",
    "            if 'count_x' in temp_df_aggs.columns.tolist():\n",
    "                temp_df_aggs = temp_df_aggs.drop('count_y', axis=1)\n",
    "                temp_df_aggs = temp_df_aggs.rename(columns={'count_x':'count'})\n",
    "            temp_df_aggs = temp_df_aggs.merge(aggregation.agg_all(temp_df, how=\"std\", col_idx=cluster_id_tags, last_idx_to_col=last_idx_as_cols), how='left', on=temp_df_aggs.columns.tolist()[:2])\n",
    "            if 'count_x' in temp_df_aggs.columns.tolist():\n",
    "                temp_df_aggs = temp_df_aggs.drop('count_y', axis=1)\n",
    "                temp_df_aggs = temp_df_aggs.rename(columns={'count_x':'count'})\n",
    "            temp_df_aggs = temp_df_aggs.merge(aggregation.agg_all(temp_df, how=\"max\", col_idx=cluster_id_tags, last_idx_to_col=last_idx_as_cols), how='left', on=temp_df_aggs.columns.tolist()[:2])\n",
    "            if 'count_x' in temp_df_aggs.columns.tolist():\n",
    "                temp_df_aggs = temp_df_aggs.drop('count_y', axis=1)\n",
    "                temp_df_aggs = temp_df_aggs.rename(columns={'count_x':'count'})\n",
    "            temp_df_aggs = temp_df_aggs.merge(aggregation.agg_all(temp_df, how=\"min\", col_idx=cluster_id_tags, last_idx_to_col=last_idx_as_cols), how='left', on=temp_df_aggs.columns.tolist()[:2])\n",
    "            if 'count_x' in temp_df_aggs.columns.tolist():\n",
    "                temp_df_aggs = temp_df_aggs.drop('count_y', axis=1)\n",
    "                temp_df_aggs = temp_df_aggs.rename(columns={'count_x':'count'})\n",
    "            nc_data = aggregation.append_agg(df1=temp_df_aggs, df2=nc_data, struct_df=struct_df, col_idx=cluster_id_tags, last_idx_to_col=last_idx_as_cols)\n",
    "        cnt += 1\n",
    "        #if cnt == 15: ############### TEMP: For speeding up testing of updated code for main function delete once updates confirmed to work\n",
    "        #    break ############### TEMP: For speeding up testing of updated code for main function delete once updates confirmed to work\n",
    "\n",
    "    print(\"\\t\\t### ~ Started - Step 1 d): Agg Phase 2: Calculating Update Rates ~ ###\") ############### TEMP: For Tracking test progress\n",
    "    # Re-format update rates so that clusters are columns\n",
    "    update_rates = update_rates.unstack()\n",
    "    update_rates.columns = update_rates.columns.droplevel(level=0)\n",
    "    update_rates = update_rates.fillna(0)\n",
    "\n",
    "    # Calculate the number of sensors per cluster\n",
    "    sensor_count_per_cluster = cluster_groups.groupby('cluster').agg({cluster_groups.columns.tolist()[0]:'count'})\n",
    "    sensor_count_per_cluster.columns = ['count']\n",
    "\n",
    "    # Calculate the average sensor update rate per hour per cluster\n",
    "    for cluster in cluster_groups['cluster'].unique():\n",
    "        update_rates.loc[:,cluster] = update_rates.loc[:,cluster]/sensor_count_per_cluster.loc[cluster][0]\n",
    "\n",
    "    # Rename the update rate columns and join them to the nc_data\n",
    "    update_rates = update_rates.add_prefix('urate_')\n",
    "    nc_data = nc_data.join(update_rates, how='left', on=['hour', 'date'])\n",
    "    nc_data = nc_data.drop('count', axis=1)\n",
    "\n",
    "    print(\"####### ~~~~~ Complete - Step 1: NC Aggregation and Clustering Phase ~~~~~ #######\") ############### TEMP: For Tracking test progress\n",
    "\n",
    "    #2) Model EC/NC relationship\n",
    "    ############################\n",
    "    print(\"####### ~~~~~ Starting - Step 2: Model EC/NC Relationship ~~~~~ #######\") ############### TEMP: For Tracking test progress\n",
    "    last_idx_as_cols = False\n",
    "    is_first_iter = True\n",
    "    cnt=1\n",
    "    for day in DATELIST:\n",
    "        # Querying and preping data for aggregations\n",
    "        temp_df2 = data_preparation.query_csv(client=None, date=day, site=None)\n",
    "        if temp_df2 is None:\n",
    "            continue\n",
    "        col_names = ['datetime']\n",
    "        col_names.extend(temp_df2.columns[1:])\n",
    "        temp_df2.columns = col_names\n",
    "        temp_df2 = aggregation.split_datetime(temp_df2)\n",
    "        # Filter for EC data, this step will be done in the query\n",
    "        temp_df2=temp_df2[(temp_df2['unit']=='kWh') | (temp_df2['unit']=='mÂ³')]\n",
    "        # Creating uniqueId\n",
    "        temp_df2=data_preparation.create_unique_id(temp_df2)\n",
    "        # Filtering dataframe for only relevant fields\n",
    "        temp_df2=temp_df2[['uniqueId', 'date', 'hour', 'unit', 'value']]\n",
    "        if is_first_iter:\n",
    "            # Creating a low memory dataframe for the append_agg function before the structure is changed by agg_all\n",
    "            struct_df2 = temp_df2.head(1)\n",
    "            # Aggregating the first date's data\n",
    "            ec_data1=aggregation.agg_numeric_by_col(temp_df2, col_idx=[0,1,2,3], how='mean')\n",
    "     ##### b) Also create second DF by aggregating further just using sensor ID fields (end result=1row per sensor)\n",
    "            ec_data2=aggregation.agg_numeric_by_col(temp_df2, col_idx=[0,3], how='all')\n",
    "            is_first_iter = False\n",
    "        else:\n",
    "            # Aggregating the current date's data and aggregate it with the current running total\n",
    "            temp_df2a=aggregation.agg_numeric_by_col(temp_df2, col_idx=[0,1,2,3], how='mean')\n",
    "            temp_df2b=aggregation.agg_numeric_by_col(temp_df2, col_idx=[0,3], how='all')\n",
    "            ec_data1=aggregation.append_agg(df1=temp_df2a, df2=ec_data1, struct_df=struct_df2, col_idx=[0,1,2,3])\n",
    "            ec_data2=aggregation.append_agg(df1=temp_df2b, df2=ec_data2, struct_df=struct_df2, col_idx=[0,3])\n",
    "        cnt += 1\n",
    "    # Freeing up some memory\n",
    "    temp_df2 = None\n",
    "    temp_df2a = None\n",
    "    temp_df2b = None\n",
    "    # Calculating the update rate\n",
    "    ec_data2[\"update_rate\"] = ec_data2[\"count\"] / (cnt*24)\n",
    "    ec_data2.drop(\"count\", inplace=True, axis=1)\n",
    "\n",
    "    # Resetting index columns\n",
    "    ec_data1=ec_data1.reset_index()\n",
    "    ec_data2=ec_data2.reset_index()\n",
    "\n",
    "    # Renaming column\n",
    "    ec_data1=ec_data1.rename(columns={\"mean\":\"EC_mean_value\"})\n",
    "\n",
    "    # Dataframe with unique sensor ids\n",
    "    uniqueSensors=ec_data2['uniqueId'].unique()\n",
    "\n",
    "    ### Scaling EC data\n",
    "    ec_data1['EC_mean_value']=data_preparation.scale_continuous(ec_data1, indexes=[4])\n",
    "\n",
    "\n",
    "    ### Scaling Cluster data\n",
    "    for i in range(6,len(nc_data.columns)):\n",
    "        nc_data.iloc[:,i]=data_preparation.scale_continuous(nc_data, indexes=[i])\n",
    "\n",
    "    #    c) For each unique EC sensorID (i.e. row in 2b_EC_data_df), create Ridge Regression model using 2a_EC_data_df and\n",
    "    #       step1_output_NC_data_df. Model is basically: Y=EC response and Xn=NC data\n",
    "\n",
    "    ### Will store each ridge output into a list and append all the dataframes\n",
    "    coefficients_list=[]\n",
    "\n",
    "    ### total sum of mse from each ridge regression model (accumulative)\n",
    "    score=0\n",
    "\n",
    "    ### Creating individual data frames for each sensor and implementing lasso\n",
    "    for sensor in uniqueSensors:\n",
    "\n",
    "        ## Create data frame for only that relevant sensor\n",
    "        new_df=ec_data1[ec_data1['uniqueId']==sensor]\n",
    "        ######## Changing EC data types for merging later. Might not need depending on step 1 output types\n",
    "        nc_data = nc_data.astype({\"date\": str})\n",
    "        new_df = new_df.astype({\"date\": object, \"hour\": object})\n",
    "        new_df.loc[:,'date']=new_df['date'].apply(lambda x: str(x)[0:10])\n",
    "\n",
    "        ## Merge specific sensor to cluster data\n",
    "        new_merged=pd.merge(nc_data, new_df, how='inner', left_on=['date','hour'], right_on=['date','hour'])\n",
    "        ## Ridge does not allow NANs, seems like some sensors are not 'on' during specific hours\n",
    "        new_merged=new_merged.fillna(0)\n",
    "\n",
    "        ## All NC predictor variables\n",
    "        X=new_merged.iloc[:,2:(len(new_merged.columns)-3)]\n",
    "\n",
    "        ## Mean value of EC data\n",
    "        Y=new_merged['EC_mean_value']\n",
    "        Y=Y.to_numpy().reshape(len(Y),1)\n",
    "\n",
    "        #Ridge CV to find optimal alpha value\n",
    "        alphas=[0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1e3, 1e4, 1e5, 1e6, 1e7, 1e8]\n",
    "        reg=RidgeCV(alphas=alphas, store_cv_values=True)\n",
    "        reg.fit(X, Y)\n",
    "        alpha_best=reg.alpha_\n",
    "\n",
    "      ### Ridge model using optimal alpha value found in step above\n",
    "        ridge_test=Ridge(alpha=alpha_best, tol=.01, max_iter=10e7,normalize=True)\n",
    "        ridge_test.fit(X,Y)\n",
    "        coef=ridge_test.coef_\n",
    "        mse=mean_squared_error(y_true=Y, y_pred=ridge_test.predict(X))\n",
    "        score=score+mse\n",
    "\n",
    "        ## Store coefficients into a dataframe\n",
    "        new=pd.DataFrame(data=coef.reshape(1,((len(new_merged.columns)-3)-2)))\n",
    "\n",
    "        ## Add uniqueId to the dataframe\n",
    "        new['uniqueId']=sensor\n",
    "\n",
    "        ## Store each sensorID's ridge coefficients into a list\n",
    "        coefficients_list.append(new)\n",
    "\n",
    "    ### Append all ridge coefficients for all sensors into a single dataframe\n",
    "    for df in uniqueSensors:\n",
    "        final_df = pd.concat(coefficients_list)\n",
    "\n",
    "    ### calculate the avarge mse across all ridge regression models\n",
    "    avg_mse=score/len(uniqueSensors)\n",
    "    print(\"avg_mse:\",avg_mse)\n",
    "\n",
    "    #    OUTPUT OF STEP2 = dataframe with EC sensor ID fields, mean response, and all n coeffecients from\n",
    "    #        that unique EC sensor's Ridge model\n",
    "    print(\"####### ~~~~~ Complete - Step 2: Model EC/NC Relationship ~~~~~ #######\") ############### TEMP: For Tracking test progress\n",
    "\n",
    "    #### 3) Prep EC data for classification model\n",
    "    ################################################\n",
    "    print(\"####### ~~~~~ Starting - Step 3: Prep EC Data for Classification Model ~~~~~ #######\") ############### TEMP: For Tracking test progress\n",
    "    #### a) Load metadata and join with 2b_EC_data_df\n",
    "    metadata=pd.read_csv('../data/PharmacyQuery.csv')\n",
    "    # Make uniqueIDs\n",
    "    metadata=data_preparation.create_unique_id(metadata, metadata=True)\n",
    "    # Drop duplicates\n",
    "    metadata=metadata.sort_values('lastSynced').drop_duplicates('uniqueId',keep='last')\n",
    "    # Choose relevant fields\n",
    "    metadata=metadata[['uniqueId','kind', 'energy','power', 'sensor', 'unit', 'water']]\n",
    "    ### Changing boolean to easily identify during encoding process\n",
    "    metadata['energy']=metadata['energy'].apply(lambda x: 'yes_energy' if x=='â' else 'no_energy')\n",
    "    metadata['power']=metadata['power'].apply(lambda x: 'yes_power' if x=='â' else 'no_power')\n",
    "    metadata['sensor']=metadata['sensor'].apply(lambda x: 'yes_sensor' if x=='â' else 'no_sensor')\n",
    "    metadata['water']=metadata['water'].apply(lambda x: 'yes_water' if x=='â' else 'no_water')\n",
    "    metadata['unit']=metadata['unit'].apply(lambda x: 'omit' if x=='_' else x)\n",
    "    # left join metadata and 2b_EC_data_df\n",
    "    merged_left=pd.merge(ec_data2, metadata, how='left', left_on='uniqueId', right_on='uniqueId')\n",
    "\n",
    "    #### b) Apply feature selection function(s) to the joined EC+metadata\n",
    "    # load NRCan classifications training data\n",
    "    nrcan_labels=pd.read_csv('../data/FinalPharmacyECSensorList-WithLabels - PharmacyECSensorsWithLabels.csv')\n",
    "    # make uniqueId\n",
    "    nrcan_labels['siteRef']='Pharmacy'\n",
    "    nrcan_labels=data_preparation.create_unique_id(nrcan_labels)\n",
    "\n",
    "    # rename columns to fix unit of measurements\n",
    "    nrcan_labels.rename(columns={'UBC_EWS.firstValue':'value'}, inplace=True)\n",
    "    # run correct_df_units function\n",
    "    nrcan_labels=data_preparation.correct_df_units(nrcan_labels)\n",
    "\n",
    "    # TRAINING DATA CLEANING (maybe its own module with metadata?)\n",
    "    # can change ? to 0 since uom fixed\n",
    "    nrcan_labels=nrcan_labels.assign(isGas=nrcan_labels.isGas.apply(lambda x: '0' if x=='?' else x))\n",
    "    # changing boolean for more descriptive encoding\n",
    "    nrcan_labels=nrcan_labels.assign(isGas=nrcan_labels.isGas.apply(lambda x: 'no_gas' if x=='0' else 'yes_gas'))\n",
    "\n",
    "    # selecting relevant training data fields\n",
    "    nrcan_labels=nrcan_labels[['uniqueId', 'isGas', 'equipRef', 'groupRef', 'navName', 'endUseLabel']]\n",
    "    nrcan_labels=nrcan_labels.drop_duplicates()\n",
    "    merged_outer=pd.merge(left=merged_left, right=nrcan_labels, how='outer', left_on='uniqueId', right_on='uniqueId')\n",
    "    # make equipRef and navName into smaller categories for feature engineering\n",
    "    merged_outer=merged_outer.assign(equipRef=merged_outer.equipRef.apply(lambda x: data_preparation.equip_label(str(x))))\n",
    "    merged_outer=merged_outer.assign(navName=merged_outer.navName.apply(lambda x: data_preparation.nav_label(str(x))))\n",
    "    \n",
    "    ##########################################################################\n",
    "    ####################### CATEGORICAL FEATURE SELECTION ####################\n",
    "    ##########################################################################\n",
    "    categorical=merged_outer[['energy', 'power', 'sensor', 'unit', 'water', 'isGas', 'equipRef', 'navName', 'endUseLabel']].copy()\n",
    "    categorical=categorical[(categorical['unit']=='kWh') or (categorical['unit']=='mÂ³')]\n",
    "    categorical=categorical.drop(['unit'], axis=1)\n",
    "\n",
    "    ##########################################################################\n",
    "    ###################### Preprocessing of Data #############################\n",
    "    ##########################################################################\n",
    "    #### Split data into training and test data\n",
    "    dataset = categorical.values\n",
    "    X = dataset[:, :-1]\n",
    "    y = dataset[:,-1]\n",
    "    X=X.astype(str)\n",
    "    y=y.astype(str)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.42, random_state=1)\n",
    "\n",
    "    #### Encode categorical data\n",
    "    oe = OneHotEncoder(handle_unknown='ignore')\n",
    "    oe.fit(X_train)\n",
    "    X_train_enc = oe.transform(X_train)\n",
    "    X_test_enc = oe.transform(X_test)\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    le.fit(y_train)\n",
    "    y_train_enc = le.transform(y_train)\n",
    "    y_test_enc = le.transform(y_test)\n",
    "    ##########################################################################\n",
    "    ################### Done with Preprocessing of Data ######################\n",
    "    ##########################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ##########################################################################\n",
    "    ############# Choosing K=maxFeature through Cross-Validation #############\n",
    "    ##########################################################################\n",
    "\n",
    "    ### Create the RFE object and compute a cross-validated score.\n",
    "    svc = SVC(kernel=\"linear\")\n",
    "    ### The \"accuracy\" scoring is proportional to the number of correct classifications\n",
    "    rfecv = RFECV(estimator=svc, step=1, cv=StratifiedKFold(2),\n",
    "                  scoring='accuracy')\n",
    "    rfecv.fit(X_train_enc, y_train_enc)\n",
    "\n",
    "    ### Create dataframe with scroes and number of features\n",
    "    Scores=pd.DataFrame({'scores':rfecv.grid_scores_,'num_features':range(1, len(rfecv.grid_scores_) + 1)})\n",
    "\n",
    "    ### Choosing max number of features depending on how many features are recommended\n",
    "    ### If recommended amount if less than half of the number of possible features\n",
    "    if rfecv.n_features_< (len(Scores)/2):\n",
    "    ### If the number of features with the 5th largest score is less than half of the number of possible features\n",
    "        if Scores.sort_values(by='scores', ascending=False)[0:5].max()['num_features']<(len(Scores)/2):\n",
    "    ### Make maxFeatures equal to the max number of possible features\n",
    "            maxFeature=Scores.num_features.max()\n",
    "        else: \n",
    "    ### Else make maxFeatures to number of features with the 5th largest score\n",
    "            maxFeature=Scores.sort_values(by='scores', ascending=False)[0:5].max()['num_features']   \n",
    "    ### Else make maxFeatures to the recommended amount \n",
    "    else:\n",
    "        maxFeature=rfecv.n_features_\n",
    "\n",
    "    print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "\n",
    "    ##### Plot Cross-Validation Scores and Number of Features\n",
    "    plt.figure()\n",
    "    plt.xlabel(\"Number of features selected\")\n",
    "    plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "    plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "    plt.title(\"Cross-Validation Plot to Decide K (Categorical Data)\")\n",
    "    plt.show()    \n",
    "    ##########################################################################\n",
    "    ###################### Done with Cross-Validation ########################\n",
    "    ##########################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ##########################################################################\n",
    "    ###########  Mutual Information Technique using K=maxFeature #############\n",
    "    ##########################################################################\n",
    "    fs = SelectKBest(score_func=mutual_info_classif, k=int(maxFeature))\n",
    "    fs.fit(X_train_enc, y_train_enc)\n",
    "    X_train_fs = fs.transform(X_train_enc)\n",
    "    X_test_fs = fs.transform(X_test_enc)\n",
    "\n",
    "    ### Storing all feature labels\n",
    "    all_feature_names=oe.get_feature_names()\n",
    "    all_features_count=len(all_feature_names)\n",
    "\n",
    "    ### Storing recommended feature labels\n",
    "    fs.get_support(indices=True)\n",
    "    feature_names = [all_feature_names[i] for i in fs.get_support(indices=True)]\n",
    "    if feature_names:\n",
    "        feature_names = np.asarray(feature_names)\n",
    "\n",
    "    ### Modifying feature labels to include original field names\n",
    "    new_feature_names=[]\n",
    "    for i in range(len(categorical.columns)):\n",
    "        if feature_names[i][1:2]==str(0):\n",
    "            new_feature_names.append(feature_names[i].replace(feature_names[i][:2], categorical.columns[0]))\n",
    "        elif feature_names[i][1:2]==str(1):\n",
    "            new_feature_names.append(feature_names[i].replace(feature_names[i][:2], categorical.columns[1]))\n",
    "        elif feature_names[i][1:2]==str(2):\n",
    "            new_feature_names.append(feature_names[i].replace(feature_names[i][:2], categorical.columns[2]))\n",
    "        elif feature_names[i][1:2]==str(3):\n",
    "            new_feature_names.append(feature_names[i].replace(feature_names[i][:2], categorical.columns[3]))\n",
    "        elif feature_names[i][1:2]==str(4):\n",
    "            new_feature_names.append(feature_names[i].replace(feature_names[i][:2], categorical.columns[4]))\n",
    "        elif feature_names[i][1:2]==str(5):\n",
    "            new_feature_names.append(feature_names[i].replace(feature_names[i][:2], categorical.columns[5]))\n",
    "        elif feature_names[i][1:2]==str(6):\n",
    "            new_feature_names.append(feature_names[i].replace(feature_names[i][:2], categorical.columns[6]))\n",
    "        elif feature_names[i][1:2]==str(7):\n",
    "            new_feature_names.append(feature_names[i].replace(feature_names[i][:2], categorical.columns[7]))\n",
    "\n",
    "    print('Original number of features: {:.0f}'.format(all_features_count))\n",
    "    print('')\n",
    "    print('These are the {:.0f} recommended categorical features:'.format(maxFeature))\n",
    "    print('')\n",
    "    print(new_feature_names)\n",
    "    print('')\n",
    "\n",
    "    ##########################################################################\n",
    "    ##################### Done with Mutual Information #######################\n",
    "    ##########################################################################\n",
    "    \n",
    "     ##########################################################################\n",
    "    ######################## NUMERICAL FEATURE SELECTION #####################\n",
    "    ##########################################################################\n",
    "\n",
    "    ##########################################################################\n",
    "    ###################### Preprocessing of Data #############################\n",
    "    ##########################################################################\n",
    "    numerical=merged_outer[['mean','std','max','min','update_rate','endUseLabel']]\n",
    "    numerical=numerical.dropna(axis=0) # drop rows if they contain nan for enduselabel, because not part of training data\n",
    "    dataset = numerical.values\n",
    "    X = pd.DataFrame(dataset[:, :-1], columns=numerical.columns[0:5])\n",
    "    y = dataset[:,-1]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "    ##########################################################################\n",
    "    ################### Done with Preprocessing of Data ######################\n",
    "    ##########################################################################\n",
    "\n",
    "\n",
    "    ##########################################################################\n",
    "    ################# Choosing K through Cross-Validation ####################\n",
    "    ##########################################################################\n",
    "    rfc = RandomForestClassifier(random_state=101)\n",
    "    rfecv = RFECV(estimator=rfc, step=1, cv=StratifiedKFold(2), scoring='accuracy')\n",
    "    rfecv.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "\n",
    "    # Plot number of features VS. cross-validation scores\n",
    "    plt.figure()\n",
    "    plt.xlabel(\"Number of features selected\")\n",
    "    plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "    plt.title(\"Cross-Validation Plot to Decide K (Numerical Data)\")\n",
    "    plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "    plt.show()\n",
    "    ##########################################################################\n",
    "    ###################### Done with Cross-Validation ########################\n",
    "    ##########################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ##########################################################################\n",
    "    #################  ANOVA F-Score Technique using K #######################\n",
    "    ##########################################################################\n",
    "    fvalue_selector = SelectKBest(f_classif, k=rfecv.n_features_)\n",
    "    # Apply the SelectKBest object to the features and target\n",
    "    X_kbest = fvalue_selector.fit_transform(X, y)\n",
    "    # Create and fit selector\n",
    "    cols = fvalue_selector.get_support(indices=True)\n",
    "    numerical_features=X.iloc[:,cols].columns.tolist()\n",
    "    # Show results\n",
    "    print('Original number of features:', X.shape[1])\n",
    "    print('')\n",
    "    print('These are the {:.0f} recommended numerical features:'.format(X.shape[1]))\n",
    "    print('')\n",
    "    print(numerical_features)\n",
    "    ##########################################################################\n",
    "    ############################ Done with ANOVA #############################\n",
    "    ##########################################################################\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
